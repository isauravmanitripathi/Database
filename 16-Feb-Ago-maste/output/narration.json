[
  {
    "create-folder": "backtest"
  },
  {
    "create-file": "backtest/run.py"
  },
  {
    "open-file": "backtest/run.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 11,
        "character": 0
      }
    },
    "code": "import datetime\nimport os.path\nimport argparse\nimport importlib\nimport dateutil.parser\nimport pandas as pd\nimport numpy as np\nimport backtrader as bt\nfrom backtrader import TimeFrame\nfrom .util import commission, observers, analyzers\nfrom .util import universe as universe_util",
    "narration": "The imports set up the runtime pieces that let run orchestrate a backtest end-to-end: standard library datetime and os.path provide timestamp handling and filesystem checks used when saving results and locating data files; argparse and importlib enable the runner to accept command-line options and to dynamically load a strategy module at runtime; dateutil.parser is used to turn human-friendly date strings into concrete date objects for the backtest start/end boundaries; pandas and numpy supply the tabular and numerical manipulation needed to prepare input series, compute performance aggregates like Sortino ratios, and reshape results for reporting; backtrader and its TimeFrame class bring in the execution engine and timeframe semantics so the runner can construct Cerebro, add data feeds, and control intraday vs daily behavior; the project-specific helpers commission, observers, and analyzers supply the custom commission model, display/logging observers, and performance analyzers that the runner wires into Cerebro; and universe is imported as universe_util to access ticker sanitation and universe selection utilities (the tail logic mentioned in the file role) so the runner can produce a clean list of tickers before requesting data or executing a strategy. This set of imports mirrors similar patterns elsewhere in the project where backtrader is the core dependency and the package-local helpers are reused, with the only stylistic difference here being that universe is aliased to universe_util to clarify its purpose in the orchestration code."
  },
  {
    "open-file": "backtest/run.py",
    "range": {
      "start": {
        "line": 12,
        "character": 0
      },
      "end": {
        "line": 22,
        "character": 22
      }
    },
    "code": "def clean_tickers(tickers, start, end):\n    data_path = os.path.join(os.path.dirname(__file__), '../data/price/')\n    out_tickers = []\n    for ticker in tickers:\n        d = pd.read_csv(data_path + ticker + '.csv', index_col=0, parse_dates=True)\n        if not (d.tail(1).index[0] < start or\n                d.head(1).index[0] > end):\n            out_tickers.append(ticker)\n        else:\n            print('Data out of date range:', ticker)\n    return out_tickers",
    "narration": "clean_tickers filters the requested universe down to only those symbols that have local historical price files covering the requested backtest window so the orchestrator doesn't attempt a run on assets with no usable data. It builds a path to the local price CSV directory relative to the module, then iterates each ticker, using pandas.read_csv with the file's first column as the datetime index to load the series. For each loaded DataFrame it inspects the earliest and latest timestamps via the DataFrame head(1) and tail(1) calls and compares them to the start and end datetimes provided by run_strategy (run_strategy already parsed the input strings into datetimes before calling clean_tickers). If the file's latest date is before the requested start or its earliest date is after the requested end the ticker is considered out of range and a message is printed; otherwise the ticker is kept and appended to the output list. The function returns the list of tickers whose CSV files overlap the requested interval, which then flows back into run_strategy so the backtest only executes on assets with valid local data."
  },
  {
    "create-folder": "tools"
  },
  {
    "create-folder": "tools/log"
  },
  {
    "create-file": "tools/log/log.py"
  },
  {
    "open-file": "tools/log/log.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import os\nfrom datetime import datetime",
    "narration": "The module pulls in the standard os module and the datetime class to provide the small set of runtime services the logging utilities need: os supplies filesystem and path operations used when creating, locating, or validating the run log file referenced by LOG_FILE_PATH, and datetime supplies the current timestamp values the log function uses to prefix entries so readers and tools can order events. These imports are intentionally minimal because the log utilities are standalone helpers used by tooling like clean_tickers and by convenience readers such as last() and tail() to inspect recent activity; other parts of the codebase follow the same lightweight pattern, importing the log module where needed (and sometimes combining it with utilities like fin_calc) rather than bundling heavier dependencies into the logging layer."
  },
  {
    "open-file": "tools/log/log.py",
    "range": {
      "start": {
        "line": 16,
        "character": 0
      },
      "end": {
        "line": 38,
        "character": 23
      }
    },
    "code": "def tail(count=5):\n    with open(LOG_FILE_PATH, 'r') as f:\n        total_lines_wanted = count\n        BLOCK_SIZE = 1024\n        f.seek(0, 2)\n        block_end_byte = f.tell()\n        lines_to_go = total_lines_wanted\n        block_number = -1\n        blocks = []  \n        while lines_to_go > 0 and block_end_byte > 0:\n            if (block_end_byte - BLOCK_SIZE > 0):\n                f.seek(block_number*BLOCK_SIZE, 2)\n                blocks.append(f.read(BLOCK_SIZE))\n            else:\n                f.seek(0, 0)\n                blocks.append(f.read(block_end_byte))\n            lines_found = blocks[-1].count('\\n')\n            lines_to_go -= lines_found\n            block_end_byte -= BLOCK_SIZE\n            block_number -= 1\n        all_read_text = ''.join(reversed(blocks))\n        for line in all_read_text.splitlines()[-total_lines_wanted:]:\n            print(line)",
    "narration": "The tail function reads the run log from LOG_FILE_PATH and prints the most recent count lines to the console by scanning the file from the end in fixed-size chunks rather than loading the whole file into memory. It opens the log in text mode, seeks to the end to determine the file length, then enters a loop that reads backwards one BLOCK_SIZE at a time: if more than a block remains it seeks relative to the end and reads a block, otherwise it seeks to the start and reads the remaining bytes. After each block read it counts newline characters to see how many of the requested lines have been collected, decrementing a lines-to-go counter and moving the backward window until either enough lines are gathered or the file start is reached. When the loop finishes it reverses the accumulated blocks to restore forward order, splits into lines, slices the last requested lines and prints each line. This strategy is chosen for efficiency on large log files and the branches protect against seeking before the file start and handle files shorter than the requested tail size. tail is a companion to last and get_last_date: last retrieves only the single final line using a backward byte-wise seek in binary mode, and get_last_date uses a similar single-line extraction to parse a date; tail extends those ideas to return multiple recent lines for quick inspection during helper runs like clean_tickers, and its only side effects are file I/O and console output."
  },
  {
    "open-file": "backtest/run.py",
    "range": {
      "start": {
        "line": 23,
        "character": 0
      },
      "end": {
        "line": 90,
        "character": 22
      }
    },
    "code": "def run_strategy(strategy, tickers=None, start='1900-01-01', end='2100-01-01', cash=100000.0,\n                 verbose=False, plot=False, plotreturns=False, universe=None, exclude=[],\n                 kwargs=None):\n    start_date = dateutil.parser.isoparse(start)\n    end_date = dateutil.parser.isoparse(end)\n    tickers = tickers if (tickers or universe) else ['SPY']\n    if universe:\n        u = universe_util.get(universe)()\n        tickers = [a for a in u.assets if a not in exclude]\n    tickers = clean_tickers(tickers, start_date, end_date)\n    module_path = f'.algos.{strategy}'\n    module = importlib.import_module(module_path, 'backtest')\n    strategy = getattr(module, strategy)\n    cerebro = bt.Cerebro(\n        stdstats=not plotreturns,\n        cheat_on_open=strategy.params.cheat_on_open\n    )\n    cerebro.addstrategy(strategy, verbose=verbose)\n    for ticker in tickers:\n        datapath = os.path.join(os.path.dirname(__file__), f'../data/price/{ticker}.csv')\n        data = bt.feeds.YahooFinanceCSVData(\n            dataname=datapath,\n            fromdate=start_date,\n            todate=end_date,\n            reverse=False,\n            adjclose=False,\n            plot=not plotreturns)\n        cerebro.adddata(data)\n    cerebro.broker.setcash(cash)\n    if plotreturns:\n        cerebro.addobserver(observers.Value)\n    cerebro.addanalyzer(bt.analyzers.SharpeRatio,\n                        riskfreerate=strategy.params.riskfreerate,\n                        timeframe=TimeFrame.Days,\n                        annualize=True)\n    cerebro.addanalyzer(analyzers.Sortino,\n                        riskfreerate=strategy.params.riskfreerate,\n                        timeframe=TimeFrame.Days,\n                        annualize=True)\n    cerebro.addanalyzer(bt.analyzers.Returns)\n    cerebro.addanalyzer(bt.analyzers.DrawDown)\n    cerebro.addanalyzer(bt.analyzers.PositionsValue)\n    cerebro.addanalyzer(bt.analyzers.GrossLeverage)\n    results = cerebro.run(preload=False)\n    start_value = cash\n    end_value = cerebro.broker.getvalue()\n    print('Starting Portfolio Value:\\t{:.2f}'.format(cash))\n    print('Final Portfolio Value:\\t\\t{:.2f}'.format(end_value))\n    drawdown = results[0].analyzers.drawdown.get_analysis()['max']['drawdown']\n    cagr = results[0].analyzers.returns.get_analysis()['rnorm100']\n    sharpe = results[0].analyzers.sharperatio.get_analysis()['sharperatio']\n    sortino = results[0].analyzers.sortino.get_analysis()['sortino']\n    positions = results[0].analyzers.positionsvalue.get_analysis()\n    avg_positions = np.mean([sum(d != 0.0 for d in i) for i in positions.values()])\n    leverage = results[0].analyzers.grossleverage.get_analysis()\n    avg_leverage = np.mean([abs(i) for i in leverage.values()])\n    sharpe = 'None' if sharpe is None else round(sharpe, 5)\n    print('ROI:\\t\\t{:.2f}%'.format(100.0 * ((end_value / start_value) - 1.0)))\n    analyzer_results = []\n    analyzer_results.append('Max Drawdown:\\t{:.2f}'.format(drawdown))\n    analyzer_results.append('CAGR:\\t\\t{:.2f}'.format(cagr))\n    analyzer_results.append('Sharpe:\\t\\t{}'.format(sharpe))\n    analyzer_results.append('Sortino:\\t{:.5f}'.format(sortino))\n    analyzer_results.append('Positions:\\t{:.5f}'.format(avg_positions))\n    analyzer_results.append('Leverage:\\t{:.5f}'.format(avg_leverage))\n    print('\\n'.join(analyzer_results))\n    if plot:\n        cerebro.plot()",
    "narration": "run_strategy is the entry point that orchestrates a full Backtrader backtest run: it first parses the start and end strings"
  },
  {
    "create-file": "tools/plot.py"
  },
  {
    "open-file": "tools/plot.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 7,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nfrom matplotlib import pyplot as plt",
    "narration": "These imports set up the plotting utility to take time-series results produced by the engine and turn them into files and visualizations for reporting. The os module is used for filesystem work such as creating output directories and writing figure files; argparse is present so the plotting routines can respect command-line options passed in from run_strategy (for example toggling interactive display or output paths). datetime.datetime is pulled in to produce human-friendly timestamps and annotate or name plots consistently. Numpy is used for the numeric heavy lifting — array math and computing log returns and other numerical summaries — while pandas provides the time-indexed Series and DataFrame operations that the strategy logging produces and that the plots consume. The pandas plotting helper register_matplotlib_converters is registered so matplotlib and pandas datetime indexes interoperate cleanly when rendering time axes, avoiding conversion issues or deprecation warnings. Finally, matplotlib.pyplot is the plotting backend that actually draws and saves the figures. These choices mirror the common pattern in other modules that import os, argparse, pandas, and numpy for CLI-driven data processing, but here the file adds matplotlib and the pandas–matplotlib converter because its responsibility is visualization, whereas other modules either use date instead of full datetime for lighter date handling or import external data/optimization libraries that are irrelevant to rendering."
  },
  {
    "open-file": "tools/plot.py",
    "range": {
      "start": {
        "line": 8,
        "character": 0
      },
      "end": {
        "line": 17,
        "character": 14
      }
    },
    "code": "def plot(data, plot_returns=False):\n    if plot_returns:\n        data = _log_returns(data)\n    plt.clf()\n    for y in data:\n        print(y.name)\n        plt.plot(y, label=y.name)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()",
    "narration": "In the reporting layer of the backtesting framework, plot is the simple renderer that run_strategy invokes to visualize the time series produced by the data pipeline and strategy. It expects an iterable of time-indexed"
  },
  {
    "open-file": "tools/plot.py",
    "range": {
      "start": {
        "line": 18,
        "character": 0
      },
      "end": {
        "line": 25,
        "character": 16
      }
    },
    "code": "def _log_returns(data):\n    log_d = []\n    for d in data:\n        r = np.log(d).diff()\n        r.iloc[0] = 0.0\n        r = np.cumprod(r + 1) - 1\n        log_d.append(r)\n    return log_d",
    "narration": "Within the plotting utilities, _log_returns converts each incoming price series into a cumulative return series that the plot routine can draw as a growth curve. It iterates over the provided data collection (typically pandas Series coming from the backtest), takes the natural log of the price series and differences it to produce period log returns, replaces the initial missing period with zero so every series is anchored at the start, then turns those period returns into a running cumulative return by treating each period as a one-plus-return multiplier and composing them through cumulative product before subtracting one. Each resulting cumulative-return series is collected into a list and returned. plot calls _log_returns when plot_returns is requested so the chart shows cumulative performance lines rather than raw prices; by contrast, the log_returns helper used elsewhere returns only the period log returns (dropping the initial NaN) rather than the cumulative series used here."
  },
  {
    "create-folder": "backtest/algos"
  },
  {
    "create-file": "backtest/algos/BaseStrategy.py"
  },
  {
    "open-file": "backtest/algos/BaseStrategy.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "import backtrader as bt",
    "narration": "Importing the backtrader library under the alias bt brings the framework’s core primitives into the module so BaseStrategy can subclass Backtrader’s Strategy base and use its orders, indicators, data feeds, and callback hooks to implement the standardized initialization, logging, and order-notification behavior that BuyAndHold, CrossOver, and EqualVolatility rely on. Other modules in the project typically import BaseStrategy as base when they extend or reference that shared behavior, and some modules import BaseStrategy together with concrete strategies such as BuyAndHold, CrossOver, MeanReversion, LeveragedEtfPair, WeightedHold, and NCAV to expose or register available strategies; here, the file depends directly on Backtrader itself while sibling files consume the BaseStrategy abstraction to construct the higher-level strategy variants used by the backtesting pipeline."
  },
  {
    "open-file": "backtest/algos/BaseStrategy.py",
    "range": {
      "start": {
        "line": 15,
        "character": 4
      },
      "end": {
        "line": 18,
        "character": 57
      }
    },
    "code": "    def log(self, txt, date=None):\n        if self.verbose:\n            date = date or self.data.datetime.date(0)\n            print('{}, {}'.format(date.isoformat(), txt))",
    "narration": "Strategy.log provides a single, consistent way for all concrete strategies to emit timestamped console messages during a backtest: when called it first checks the instance flag that Strategy.__init__ copies from the strategy parameters to determine whether verbose output is enabled, and only proceeds if that flag is true. If no explicit date is supplied, it derives the date from the active Backtrader data feed for the current bar so the message is tied to the exact simulation timestamp; it then formats that date in ISO form and prints it together with the given text to stdout. Because notify_order, CrossOver.next, EqualVolatility.rebalance and many other strategy routines call Strategy.log, this method centralizes readable, time-aligned logging for order lifecycle events, rebalances and other diagnostics, unlike the separate file-based logger elsewhere in the project which writes structured entries to disk."
  },
  {
    "create-file": "tools/vix_term.py"
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "import pandas as pd",
    "narration": "pandas, imported as pd, is the core tabular and time-series library this module uses to represent, slice, convert and persist the VIX and futures dataset that VixTermStructure manages. VixTermStructure.download produces a pandas DataFrame that the constructor then slices into the term-structure and contango-ready columns using label- and position-based accessors; VixTermStructure.get relies on DataFrame row/column selection and numeric casting to return either a single float or a Series of floats, and VixTermStructure.contango performs arithmetic on those DataFrame/Series columns to produce the market-regime metric. Because the broader framework’s plotting utilities and the _log_returns transformer expect clean, indexed time-series, pandas supplies the consistent, in-memory representation that allows this helper to download, cache, update, and hand off values to run_strategy and the reporters."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 27,
        "character": 4
      },
      "end": {
        "line": 33,
        "character": 24
      }
    },
    "code": "    def get(self, month, month2=None):\n        if month2 is None:\n            return float(self._term_structure.iloc[0, month-1])\n        else:\n            terms = self._term_structure.iloc[0, month-1: month2-1]\n            terms = terms.astype(float)\n            return terms",
    "narration": "VixTermStructure.get is the simple accessor that reads the most recent snapshot of the VIX futures strip that VixTermStructure.__init__ built from download and exposes either a single contract value or a slice of contracts for downstream signal logic. It treats the month and month2 parameters as 1‑based positions into the F1..F12 columns that were stored on self._term_structure; when month2 is omitted it returns the single numeric value for the requested contract (casting the underlying scalar to a plain float so callers can do arithmetic directly), and when month2 is provided it returns a positional slice across the requested range of contracts from the first row and coerces that slice to floats so callers receive a numeric series. The branching is a simple guard: one path for a scalar lookup and one path for a multi‑month series, which is what VixTermStructure.contango and strategy code consume to compute contango or to inspect the term structure for regime signals."
  },
  {
    "create-folder": "backtest/util"
  },
  {
    "create-file": "backtest/util/analyzers.py"
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 4,
        "character": 0
      }
    },
    "code": "import math\nimport numpy as np\nfrom backtrader import Analyzer, TimeFrame\nfrom backtrader.analyzers import TimeReturn",
    "narration": "The file brings in two standard numeric toolkits and three Backtrader primitives that the Sortino analyzer needs to both compute statistics and plug into the engine. math supplies low-level scalar utilities used for exact arithmetic and edge-case checks during the Sortino computation (for example square roots and NaN/finite checks), while numpy is used for efficient vectorized operations over return series (converting lists of periodic returns into arrays, computing means, masking downside returns, and similar array-level math). From Backtrader the Analyzer base class is imported so the Sortino implementation can register with the framework lifecycle and expose init/stop/get_analysis hooks; TimeFrame provides the timeframe identifiers that the analyzer uses to interpret and annualize return streams; and TimeReturn (from backtrader.analyzers) is pulled in to produce the periodized return series that Sortino wraps rather than reimplementing return aggregation itself — as you saw in Sortino.__init__ where a TimeReturn instance is created with the analyzer’s timeframe and compression. Compared with the project’s internal package imports that wire together local modules, these imports connect the analyzer to external numeric libraries and Backtrader’s analyzer composition pattern so it can calculate downside risk and surface the metric through the framework’s reporting pipeline."
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 50,
        "character": 4
      },
      "end": {
        "line": 51,
        "character": 39
      }
    },
    "code": "    def get_analysis(self):\n        return dict(sortino=self.ratio)",
    "narration": "Sortino.get_analysis acts as the analyzer’s public output for the reporting layer: it reads the Sortino ratio stored on the analyzer instance and returns it wrapped in a simple dictionary under the key \"sortino\". The ratio value originates from the analyzer lifecycle—Sortino.__init__ creates the TimeReturn collector and initializes the ratio, and Sortino.stop performs the actual computation from the collected returns and writes the resulting value into the instance attribute—so get_analysis performs no calculation or side effects itself, it merely exposes that precomputed metric in the standard dict form that run_strategy and the rest of the reporting pipeline expect from Backtrader analyzers."
  },
  {
    "open-file": "backtest/run.py",
    "range": {
      "start": {
        "line": 91,
        "character": 0
      },
      "end": {
        "line": 117,
        "character": 33
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('strategy', nargs=1)\n    PARSER.add_argument('-t', '--tickers', nargs='+')\n    PARSER.add_argument('-u', '--universe', nargs=1)\n    PARSER.add_argument('-x', '--exclude', nargs='+')\n    PARSER.add_argument('-s', '--start', nargs=1)\n    PARSER.add_argument('-e', '--end', nargs=1)\n    PARSER.add_argument('--cash', nargs=1, type=int)\n    PARSER.add_argument('-v', '--verbose', action='store_true')\n    PARSER.add_argument('-p', '--plot', action='store_true')\n    PARSER.add_argument('--plotreturns', action='store_true')\n    PARSER.add_argument('-k', '--kwargs', nargs='+')\n    ARGS = PARSER.parse_args()\n    ARG_ITEMS = vars(ARGS)\n    TICKERS = ARG_ITEMS['tickers']\n    KWARGS = ARG_ITEMS['kwargs']\n    EXCLUDE = ARG_ITEMS['exclude']\n    del ARG_ITEMS['tickers']\n    del ARG_ITEMS['kwargs']\n    del ARG_ITEMS['exclude']\n    STRATEGY_ARGS = {k: (v[0] if isinstance(v, list) else v) for k, v in ARG_ITEMS.items() if v}\n    STRATEGY_ARGS['tickers'] = TICKERS\n    STRATEGY_ARGS['kwargs'] = KWARGS\n    if EXCLUDE:\n        STRATEGY_ARGS['exclude'] = [EXCLUDE] if len(EXCLUDE) == 1 else EXCLUDE\n    run_strategy(**STRATEGY_ARGS)\n",
    "narration": "When the module is executed as a script it wires a command-line interface so an operator can configure and launch a backtest without touching code: argparse is used to declare the available options (strategy name, tickers or universe, exclusions, start/end dates, cash, verbose/plot flags, and an extra kwargs list), then parse_args populates ARGS and vars turns that namespace into a plain dictionary called ARG_ITEMS for easier manipulation. The code pulls out the raw tickers list, the raw kwargs list, and the exclude list into TICKERS, KWARGS, and EXCLUDE respectively and removes those keys from ARG_ITEMS so they can be handled specially rather than treated as single-value parameters. STRATEGY_ARGS is then constructed from the remaining ARG_ITEMS by collapsing any list-valued arguments into single scalar values when appropriate and skipping empty entries, producing the core keyword-argument mapping that run_strategy expects; after that the earlier-extracted TICKERS and KWARGS are reattached to STRATEGY_ARGS in their original list form, and EXCLUDE is normalized so a single exclude value becomes a one-element list while leaving multi-item excludes unchanged. Finally, the prepared STRATEGY_ARGS map is passed into run_strategy to perform the full orchestration of data cleaning, VIX context, strategy execution, and Sortino-based aggregation."
  },
  {
    "open-file": "tools/log/log.py",
    "range": {
      "start": {
        "line": 3,
        "character": 0
      },
      "end": {
        "line": 3,
        "character": 64
      }
    },
    "code": "LOG_FILE_PATH = os.path.join(os.path.dirname(__file__), './log')",
    "narration": "LOG_FILE_PATH is a module-level constant that resolves to a stable file location for the tooling layer’s run log by taking the directory that contains this module and joining it with a local log filename, producing a platform-safe path independent of the process working directory. Because the tooling utilities need a single, predictable place to append and inspect messages during data acquisition and cleanup runs, functions like log use LOG_FILE_PATH to open that file in append mode and last()/tail() read from the same path to return recent entries; helpers such as clean_tickers then consult those entries to quickly review recent activity. This ties the lightweight logging utilities into the broader data pipeline by colocating run metadata with the tooling code, and it relies on the previously covered imports (os and datetime) so the module can compute the path and timestamp entries consistently."
  },
  {
    "open-file": "tools/log/log.py",
    "range": {
      "start": {
        "line": 4,
        "character": 0
      },
      "end": {
        "line": 8,
        "character": 20
      }
    },
    "code": "def log(log_type, message):\n    time = datetime.now()\n    out = '{} -- {}: {}\\n'.format(time, log_type, message)\n    with open(LOG_FILE_PATH, 'a') as f:\n        f.write(out)",
    "narration": "log is the lightweight append-only logger the tooling layer uses to persist runtime messages so cleanup and acquisition helpers have a durable run record. When called with the parameters log_type and message it captures the current timestamp, composes a single line that combines the timestamp, the log type, and the message, and then opens the file at LOG_FILE_PATH in append mode to write that line with a trailing newline. The function performs no branching or return value; its sole side effect is file I/O to record the event for later inspection. In the project flow, tools like clean_tickers call log to record what they did and why, and inspection helpers such as last read the same LOG_FILE_PATH to retrieve recent entries (last approaches the file from the end to print the most recent line). Whereas Strategy.log is a runtime, console-oriented helper for backtests, log provides the persistent, file-backed trace that the tooling layer uses for post-run diagnosis."
  },
  {
    "open-file": "tools/log/log.py",
    "range": {
      "start": {
        "line": 9,
        "character": 0
      },
      "end": {
        "line": 15,
        "character": 24
      }
    },
    "code": "def last():\n    with open(LOG_FILE_PATH, 'rb') as f:\n        f.seek(-2, os.SEEK_END)\n        while f.read(1) != b'\\n':\n            f.seek(-2, os.SEEK_CUR)\n        last_line = f.readline().decode()\n        print(last_line)",
    "narration": "last opens the run log at LOG_FILE_PATH in binary mode and locates the final log entry by seeking from the end of the file and walking backwards one byte at a time until it encounters the last newline; once that boundary is found it reads and decodes the bytes that follow as the final line and prints that decoded line to the console. Within the tooling layer of the project this provides a very lightweight inspector so helpers like clean_tickers can quickly view the most recent activity without loading the whole log; its behavior is a single-line analogue to tail and is implemented with low-level file seeks rather than block-chunking. Compared to get_last_date, which uses the same backward-seek-and-read pattern but parses and returns a date from the last line, last simply emits the full last log entry to stdout and produces no return value, and compared to tail it targets only the last line and operates in binary byte-wise mode rather than reading multiple blocks to assemble several trailing lines. The function’s observable effects are file I/O and console output; it does not return the retrieved line."
  },
  {
    "open-file": "tools/log/log.py",
    "range": {
      "start": {
        "line": 39,
        "character": 0
      },
      "end": {
        "line": 40,
        "character": 25
      }
    },
    "code": "if __name__ == '__main__':\n    print(\"log imported\")\n",
    "narration": "The file ends with the standard Python module entrypoint guard that, when the module is executed as a standalone script, emits a brief console confirmation that the log module loaded; this acts as a tiny smoke test so a developer can run the logging utilities directly and see that the module can be imported and executed without invoking any of its log-append or inspection functions. Because the guard only activates on direct execution, normal consumers such as clean_tickers or other tooling that import the log module will not see that console message, keeping importing modules quiet during automated runs; this is why the print differs from the import statements found elsewhere in the project that bring log into other modules. The message is therefore purely for quick manual verification and does not interact with the run-log appenders, last(), tail(), or the logging logic used by the rest of the data-acquisition tooling."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 4,
        "character": 0
      },
      "end": {
        "line": 38,
        "character": 35
      }
    },
    "code": "class VixTermStructure:\n    def __init__(self, days=1):\n        ...\n\n    def download(self, days=1):\n        ...\n\n    def get(self, month, month2=None):\n        ...\n\n    def contango(self, months=(1, 2)):\n        ...",
    "narration": "VixTermStructure encapsulates the logic for fetching, caching, and exposing the VIX futures term structure so strategies can ask for point-in-time term values or a simple contango signal. On construction the class validates that the requested lookback is positive and immediately populates its internal cache by invoking the download method; the downloaded dataset is split into a fast-access table of front-month through twelfth-month futures and a small derived slice used for contango-style diagnostics. The download method performs the network requests to pull VIX and futures price rows for the requested window, merges and normalizes them into a time-indexed table, and returns that dataframe for the instance to store — this is the only place the class talks to external providers. The get accessor supports two paths: when asked for a single month it returns the numeric value for that front-month index, and when given a second month it returns the slice of term values between those month indices as numeric types so callers can compute aggregates or spreads; indexing is implemented so callers think in month numbers rather than raw column offsets. The contango helper computes a market-regime metric by pulling the front and back month values via get and returning the percentage spread of back relative to front (back over front minus one), which is the signal strategies consult to decide regime-dependent behavior. The design follows the project’s data-layer pattern of downloading and caching a canonical dataset and exposing small, deterministic accessors for strategy code to call at runtime; it is similar to earlier variants that also split the term columns and compute contango, but here the download method and the explicit contango_data slice make the update/cache responsibilities and the contango-facing interface explicit for run-time queries."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 5,
        "character": 4
      },
      "end": {
        "line": 9,
        "character": 53
      }
    },
    "code": "    def __init__(self, days=1):\n        assert days > 0\n        self._data = self.download(days)\n        self._term_structure = self._data.loc[:, 'F1':'F12']\n        self._contango_data = self._data.iloc[:, -3:]",
    "narration": "VixTermStructure.__init__ validates that the caller asked for a positive lookback in days, then immediately populates the instance by calling VixTermStructure.download and storing its returned DataFrame on the instance as _data. It then extracts and caches two focused views of that downloaded table so subsequent calls are cheap: _term_structure selects the monthly futures columns labeled F1 through F12 (the first-row values from which VixTermStructure.get reads front and back month prices), and _contango_data captures the last three columns of the table (kept for the contango-related metrics and any other short-window diagnostics). By doing this work during construction the class implements an eager-fetch-and-cache pattern so run-time callers such as run_strategy and strategy methods like EqualVolatility.rebalance or MeanReversion.add_rank can call VixTermStructure.get or VixTermStructure.contango repeatedly without re-downloading. The initialization therefore has the side effects of issuing the download/network call (and its console output via download) and writing the three instance attributes: _data, _term_structure, and _contango_data."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 11,
        "character": 4
      },
      "end": {
        "line": 25,
        "character": 19
      }
    },
    "code": "    def download(self, days=1):\n        print('Downloading VIX Term-Structure...')\n\n        url = f'http://vixcentral.com/historical/?days={days}'\n\n        data = pd.read_html(url)[0]\n\n        header = data.iloc[0]\n        data = data[1:-1]\n        data = data.set_index(0)\n        del data.index.name\n        data.columns = header[1:]\n\n        print(\"Term-Structure downloaded.\")\n        return data",
    "narration": "Within the backtesting framework's data layer, VixTermStructure.download is responsible for fetching the raw VIX futures term-structure table from an external provider and turning it into a normalized pandas DataFrame that the rest of VixTermStructure can use. It begins by emitting a console message, then builds an HTTP request URL that encodes the days parameter so the provider returns the requested history. It uses pandas' HTML table parser to retrieve the first table on the page and treats the very first row of that table as the column header; the code then drops the original first row and the final row (the latter is treated as a trailing footer or summary row from the source), sets the first column as the DataFrame index (removing the index name), and replaces the column labels with the extracted header entries starting from the second header cell. The method prints a completion message and returns the cleaned DataFrame. The normalization steps are intentional so VixTermStructure.__init__ can slice columns named F1 through F12 into _term_structure and pick the trailing columns into _contango_data, and so VixTermStructure.get can index the front-month row and convert term values to floats; in other words, download supplies the structured, indexed table that downstream accessors expect. The function performs a network GET and prints progress to stdout and does not perform additional validation or error handling."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 35,
        "character": 4
      },
      "end": {
        "line": 38,
        "character": 35
      }
    },
    "code": "    def contango(self, months=(1, 2)):\n        front = self.get(months[0])\n        back = self.get(months[1])\n        return (back / front - 1.0)",
    "narration": "VixTermStructure.contango computes a simple market‑regime signal by taking two month indices from its months parameter (defaults to the front and next month), retrieving the corresponding term values via VixTermStructure.get, and returning the proportional spread between them as a single floating‑point value; concretely it asks for the front month value and the back month value, then computes the back divided by the front minus one to produce a percentage difference (positive implies contango, negative implies backwardation). The function itself performs no state mutation — it relies on get to read the prepared term structure that VixTermStructure.__init__ populated from download — and it does not perform input validation, so it depends on the same indexing convention used by get (month numbers are 1‑based). Strategies call contango to obtain an on‑demand scalar used for regime decisions (for example to adjust leverage or hedging) so the returned ratio flows directly into decision logic in run_strategy and the strategy rebalance/add_rank paths."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 41,
        "character": 0
      },
      "end": {
        "line": 43,
        "character": 30
      }
    },
    "code": "if __name__ == '__main__':\n    vts = VixTermStructure()\n    print(vts._term_structure)\n",
    "narration": "When the module is run as a script the entry-point guard creates a VixTermStructure instance, which invokes VixTermStructure.__init__ and therefore runs the download-and-cache path to populate the in-memory datasets and the term-structure and contango attributes. Immediately after construction the code emits the VixTermStructure._term_structure attribute to standard output so a developer gets a direct, human-readable snapshot of the monthly futures table that the class exposes for strategies. Compared with the simple import smoke test you saw earlier for the logging utilities, this run-as-script path actually exercises the data acquisition and initialization logic and lets you confirm the concrete term-structure that VixTermStructure.get or VixTermStructure.contango would later provide to run_strategy."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 45,
        "character": 0
      },
      "end": {
        "line": 47,
        "character": 11
      }
    },
    "code": "    print('F1:', vts.get(1))\n    print('F2:', vts.get(2))\n    print()\n",
    "narration": "As a simple runtime demonstration of the VixTermStructure helper, the two print calls exercise the get accessor to show the front-month and the second-month future values: they invoke VixTermStructure.get with the first-month index and then with the second-month index so the helper pulls the corresponding F1 and F2 points from the cached term-structure row and writes those numeric values to the console. That output provides an immediate sanity check that the download-and-cache step completed and that callers can retrieve raw term points on demand; it exercises the single-month lookup path described by VixTermStructure.get (which returns a scalar float for a single month and would return a Series when given a month range), and it complements the contango calculation which uses the same accessors to compute relative spreads. In short, these prints are a lightweight, human-readable smoke test showing concrete term-structure values so a developer or strategy can verify the helper is returning expected F1/F2 readings at runtime."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 49,
        "character": 0
      },
      "end": {
        "line": 50,
        "character": 11
      }
    },
    "code": "    print(vts._contango_data)\n    print()\n",
    "narration": "When the module is run interactively as a tiny smoke test, the code prints the VixTermStructure instance’s internal contango snapshot to the console so a developer can immediately see the cached contango-related columns that were carved out during initialization from the downloaded dataset. That output is the three-column, point-in-time slice stored on the instance as _contango_data (the same attribute that __init__ populated from the tail of the downloaded frame), and the following blank print just inserts a visual line break to separate that snapshot from any subsequent console text. This is a diagnostic, human-facing output used for quick verification when running the helper directly; it differs from the contango method, which programmatically returns a scalar contango metric for a given pair of months."
  },
  {
    "open-file": "tools/vix_term.py",
    "range": {
      "start": {
        "line": 52,
        "character": 0
      },
      "end": {
        "line": 54,
        "character": 50
      }
    },
    "code": "    print('Contango (1/2):', vts.contango((1, 2)))\n    print('Contango (3/5):', vts.contango((3, 5)))\n    print('Contango (4/7):', vts.contango((4, 7)))\n",
    "narration": "The three print lines are simple runtime checks that ask the VixTermStructure instance vts for contango values for three different front/back month pairs and write them to the console with human-readable labels. Each call invokes VixTermStructure.contango, which in turn uses VixTermStructure.get to fetch the front and back month points from the cached term-structure and returns the fractional difference (i.e., how much the back month trades above or below the front). The specific tuples passed represent a near-term pair, a mid-term pair, and a longer-dated pair so a developer can quickly inspect how the term structure's slope changes across maturities; positive values indicate contango, negative values indicate backwardation. These prints are therefore a small, ad-hoc demonstration/sanity check showing the metric that strategies (for example run_strategy) would consume programmatically during signal generation."
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 51,
        "character": 39
      }
    },
    "code": "class Sortino(Analyzer):\n    params = (\n        ('timeframe', TimeFrame.Years),\n        ('compression', 1),\n        ('riskfreerate', 0.01),\n        ('factor', None),\n        ('convertrate', True),\n        ('annualize', False),\n    )\n    RATEFACTORS = {\n        TimeFrame.Days: 252,\n        TimeFrame.Weeks: 52,\n        TimeFrame.Months: 12,\n        TimeFrame.Years: 1,\n    }\n    def __init__(self):\n        ...\n    def stop(self):\n        ...\n    def get_analysis(self):\n        ...",
    "narration": "Sortino is an analyzer that plugs into the Backtrader lifecycle to produce a downside-risk-adjusted performance metric for reporting; it is instantiated during strategy setup and runs its computation when the backtest finishes so run_strategy and the universe initializers can collect the metric alongside other analyzers. The analyzer declares parameters controlling the aggregation timeframe and compression, a nominal risk-free rate, an optional multiplier or conversion behavior, and whether to annualize; a small RATEFACTORS mapping translates common TimeFrame values into period counts used for rate and annualization math. In __init__ Sortino wires a TimeReturn analyzer configured to the chosen timeframe and compression so that periodic returns are collected as the engine runs, and it initializes an internal slot for the resulting ratio. In stop it retrieves the series of period returns from TimeReturn, derives a period factor from TIMEFACTORS when available, and then applies either a per-period conversion of the configured risk-free rate or a transformation of the collected returns depending on the convertrate flag; it computes average excess return relative to the adjusted risk-free rate and calculates the downside deviation by squaring and averaging only the negative shortfalls below that rate and taking the square root. The code then divides the excess mean by the downside deviation to get the Sortino ratio, applies an optional annualization multiplicative factor when requested, and protects against invalid math by catching numeric errors and emitting a None when the ratio cannot be computed. get_analysis simply exposes the final numeric result as a keyed value so the reporting pipeline can ingest it. The overall pattern mirrors other performance analyzers in the project—using TimeReturn to gather period returns, computing a scalar metric in stop, and exposing it via get_analysis—but focuses specifically on downside risk rather than symmetric volatility."
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 6,
        "character": 4
      },
      "end": {
        "line": 13,
        "character": 5
      }
    },
    "code": "    params = (\n        ('timeframe', TimeFrame.Years),\n        ('compression', 1),\n        ('riskfreerate', 0.01),\n        ('factor', None),\n        ('convertrate', True),\n        ('annualize', False),\n    )",
    "narration": "params defines the Sortino analyzer's configurable defaults that Backtrader reads when the analyzer is attached so the framework can collect downside-risk–adjusted performance in a consistent, user-configurable way. It declares a default timeframe of Years and a compression of one to establish the temporal granularity the analyzer will treat returns at, provides a default riskfreerate of 0.01 to be used when computing excess returns, and exposes an optional factor that callers can supply to scale or reinterpret the raw returns if the strategy reports in nonstandard units. The convertrate flag tells the analyzer to convert the risk-free rate from its annual form into the analyzer's per-period equivalent (using the RATEFACTORS mapping defined nearby), and the annualize flag controls whether the final Sortino output is expressed as an annualized metric or left at the analyzer's native period basis. Together these parameters implement the standard Backtrader Analyzer configuration pattern so run_strategy and universe initializers can attach Sortino with sensible defaults yet override behavior to match different data timeframes and reporting conventions."
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 14,
        "character": 4
      },
      "end": {
        "line": 19,
        "character": 5
      }
    },
    "code": "    RATEFACTORS = {\n        TimeFrame.Days: 252,\n        TimeFrame.Weeks: 52,\n        TimeFrame.Months: 12,\n        TimeFrame.Years: 1,\n    }",
    "narration": "RATEFACTORS is a small, explicit lookup that tells the Sortino analyzer how many of each Backtrader TimeFrame unit are treated as a year for the purposes of scaling rates and returns. The mapping pairs Backtrader TimeFrame members for days, weeks, months, and years with the conventional annualization denominators (using the common trading convention for days), so when the Sortino analyzer sees a timeframe in its params tuple it uses RATEFACTORS to convert period-level downside deviation and the risk‑free rate into an annualized basis if convertrate or annualize are enabled. Because TimeFrame is the Backtrader enum imported earlier, these keys align directly with the analyzer’s timeframe/compression parameters and ensure the downside-risk-adjusted metric the analyzer emits is comparable across strategies and universes, matching the project’s goal of producing consistent, reportable performance metrics."
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 20,
        "character": 4
      },
      "end": {
        "line": 25,
        "character": 24
      }
    },
    "code": "    def __init__(self):\n        super(Sortino, self).__init__()\n        self.ret = TimeReturn(\n            timeframe=self.p.timeframe,\n            compression=self.p.compression)\n        self.ratio = 0.0",
    "narration": "The constructor first calls the Analyzer superclass initialization so the Sortino instance is properly registered in Backtrader’s lifecycle, then creates and stores a TimeReturn analyzer configured with the analyzer’s timeframe and compression parameters so that the exact per-period returns needed for downside-risk calculations are captured during the run; finally it sets a predictable placeholder for the output by initializing the ratio attribute to zero. Remember Sortino is attached by universe initializers and run_strategy and its stop method later pulls the period returns out of the TimeReturn analyzer to compute the downside-risk–adjusted metric and write the final value back into the ratio attribute, which get_analysis then exposes for reporting."
  },
  {
    "open-file": "backtest/util/analyzers.py",
    "range": {
      "start": {
        "line": 26,
        "character": 4
      },
      "end": {
        "line": 49,
        "character": 26
      }
    },
    "code": "    def stop(self):\n        returns = list(self.ret.get_analysis().values())\n        rate = self.p.riskfreerate\n        factor = None\n        if self.p.timeframe in self.RATEFACTORS:\n            factor = self.RATEFACTORS[self.p.timeframe]\n        if factor is not None:\n            if self.p.convertrate:\n                rate = pow(1.0 + rate, 1.0 / factor) - 1.0\n            else:\n                returns = [pow(1.0 + x, factor) - 1.0 for x in returns]\n        if len(returns):\n            ret_free_avg = np.mean(returns) - rate\n            tdd = math.sqrt(np.mean([min(0, r - rate)**2 for r in returns]))\n            try:\n                ratio = ret_free_avg / tdd\n                if factor is not None and \\\n                        self.p.convertrate and self.p.annualize:\n                    ratio = math.sqrt(factor) * ratio\n            except (ValueError, TypeError, ZeroDivisionError):\n                ratio = None\n        else:\n            ratio = None\n        self.ratio = ratio",
    "narration": "Stop is the lifecycle hook that runs when Backtrader finishes the run and turns the per-period returns collected by TimeReturn in Sortino.__init__ into the downside-risk–adjusted metric the rest of the reporting pipeline consumes. It first extracts the list of returns from the TimeReturn analyzer attached at initialization, then picks up the configured riskfree rate and tries to map the analyzer timeframe to a scaling factor using RATEFACTORS (the TIMEFRAME-to-periods-per-year lookup already exposed in params). If a factor exists, Stop has two mutually exclusive ways to align compounding: when convertrate is true it converts the configured annual riskfree rate into the per-period rate to match the returns; otherwise it annualizes each observed return into an annualized equivalent so both series share the same period basis. With returns and rate on the same footing, Stop computes the mean excess return by subtracting the rate from the average return, and computes the target downside deviation as the square root of the mean squared negative deviations of returns below the rate (i.e., the downside-only volatility). It then tries to form the Sortino ratio by dividing the excess mean by the downside deviation; if the timeframe had a factor and both convertrate and annualize are true, it scales the ratio by the square root of the factor to present an annualized ratio. Any numerical problems during that division (invalid types, zero division, or value errors) cause the ratio to become None, and if there are no returns at all the ratio is also set to None. Finally, Stop writes the computed ratio back to the analyzer instance so get_analysis can expose it to run_strategy and the universe initializers for reporting."
  },
  {
    "open-file": "tools/plot.py",
    "range": {
      "start": {
        "line": 26,
        "character": 0
      },
      "end": {
        "line": 44,
        "character": 41
      }
    },
    "code": "if __name__ == '__main__':\n    register_matplotlib_converters()  \n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('tickers', nargs='+')\n    PARSER.add_argument('-r', '--returns', action=\"store_true\")\n    PARSER.add_argument('-s', '--start', nargs=1, type=int)\n    PARSER.add_argument('-e', '--end', nargs=1, type=int)\n    ARGS = PARSER.parse_args()\n    TICKERS = ARGS.tickers\n    START = ARGS.start or [1900]\n    END = ARGS.end or [2100]\n    START_DATE = datetime(START[0], 1, 1)\n    END_DATE = datetime(END[0], 1, 1)\n    DATA = []\n    for ticker in TICKERS:\n        datapath = os.path.join(os.path.dirname(__file__), f'../data/price/{ticker}.csv')\n        ticker_data = pd.read_csv(datapath, index_col='Date', parse_dates=True)['Adj Close'].rename(ticker)\n        DATA.append(ticker_data.loc[START_DATE: END_DATE])\n    plot(DATA, plot_returns=ARGS.returns)\n",
    "narration": "When the module is executed as a script, it runs a small interactive entry point that prepares and renders price (or return) plots from local CSV files so a developer can quickly visualize series outside of the Backtrader run. It first registers pandas' datetime-to-matplotlib converters so time-indexed series plot cleanly. It then builds a command-line interface using argparse that requires one or more tickers and accepts a boolean flag that requests plotting log returns plus optional start and end year arguments; because start and end are parsed with single-item argument semantics they are normalized into values and then converted into January 1 datetimes for slicing. The code iterates over each requested ticker, composes a path into the project's data/price directory relative to the module file, reads the CSV, selects the adjusted close column and renames the resulting pandas Series to the ticker symbol, and slices it to the requested date range before appending it to a list called DATA. Finally it calls the module-level plot function, passing the assembled list and the returns flag; plot will convert the series to log returns when the flag is set by invoking the module's _log_returns helper (that helper is documented elsewhere) and then clear the current figure, draw each series with labels, lay out the axes, and show the figure. This entry point mirrors the earlier interactive smoke test for VixTermStructure in that it provides a quick developer-facing probe of cached/local datasets, but here the probe exercises the plotting utility and local price CSVs rather than contango lookups."
  },
  {
    "create-file": "backtest/algos/EqualVolatility.py"
  },
  {
    "open-file": "backtest/algos/EqualVolatility.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 3,
        "character": 0
      }
    },
    "code": "import numpy as np\nimport pandas as pd\nfrom . import BaseStrategy as base",
    "narration": "The file brings in numpy and pandas under the conventional aliases so the EqualVolatility strategy can perform vectorized math and time-series manipulations — numpy supplies fast numeric ops for weighting and scaling while pandas provides Series/DataFrame utilities for rolling-window standard deviation, alignment with timestamped market data, and easy slicing of lookback windows that the rebalance logic relies on. It also imports the framework's BaseStrategy module as base so EqualVolatility can inherit the common lifecycle hooks, utilities, and initialization behavior that all strategies share; as you saw in EqualVolatility.__init__, it delegates to base.Strategy.__init__ to hook into the framework. This mirrors the project's typical import pattern (most strategies alias numpy and import the base strategy), with the only difference here being the explicit use of pandas because EqualVolatility needs higher-level time-series operations for computing rolling vol and aligning those signals with the data layer and VixTermStructure checks discussed earlier (see the interactive smoke-test prints and the attached analyzers like Sortino whose params and RATEFACTORS were covered)."
  },
  {
    "open-file": "backtest/algos/EqualVolatility.py",
    "range": {
      "start": {
        "line": 4,
        "character": 0
      },
      "end": {
        "line": 38,
        "character": 39
      }
    },
    "code": "class EqualVolatility(base.Strategy):\n    params = {\n        'rebalance_days': 21,\n        'target_percent': 0.95,\n        'lookback': 21\n    }\n    def __init__(self):\n        ...\n    def rebalance(self):\n        ...\n    def next(self):\n        ...",
    "narration": "EqualVolatility is a concrete strategy implementation that plugs into the backtesting framework’s Strategy base and implements a periodic, risk-equalizing rebalancing policy for the universe of data feeds. In EqualVolatility.__init__ it sets up the internal scheduling and state the strategy needs at runtime: it stores the configuration from params (rebalance_days, target_percent, lookback), keeps a simple counter/timestamp to know when to trigger rebalances, and obtains the regime context helper from the VixTermStructure so subsequent decisions can be logged alongside contango/regime information. Because it inherits from base.Strategy, it uses the framework’s logging and order-notification plumbing rather than reimplementing those concerns. EqualVolatility.rebalance is where the portfolio allocation logic lives: for each data feed it pulls a close-price history of length lookback, converts that to log returns, computes the rolling standard deviation as the asset’s volatility, then constructs inverse-volatility weights by taking the reciprocal of each volatility and normalizing so the weights sum to one. Those normalized weights are scaled by target_percent to cap the total equity exposure, and for each instrument the method computes a desired position value from the current broker portfolio value and the weight, compares that to the instrument’s current position value (obtained via getposition), and turns those differences into order targets. The method then sequences the actual orders by sorting the required changes and issuing order_target_percent calls so the broker executes the smallest-to-largest adjustments in a deterministic order that accounts for cash/position interactions during a multi-instrument rebalancing. EqualVolatility.next advances the strategy on every bar, increments its internal counter, consults VixTermStructure for any regime note to include in logs, and invokes rebalance whenever the configured rebalance_days cadence is reached and there is sufficient lookback history; it writes human-readable decision logs via the base Strategy’s log method so the backtest output records both the allocation choices and the VIX-term-structure context. Conceptually, EqualVolatility follows the same lifecycle pattern other strategies in the codebase use (a setup phase, per-bar next, and a rebalance/send-orders"
  },
  {
    "open-file": "backtest/algos/EqualVolatility.py",
    "range": {
      "start": {
        "line": 5,
        "character": 4
      },
      "end": {
        "line": 9,
        "character": 5
      }
    },
    "code": "    params = {\n        'rebalance_days': 21,\n        'target_percent': 0.95,\n        'lookback': 21\n    }",
    "narration": "The params dictionary on EqualVolatility declares three default tuning knobs that Backtrader will read when the strategy is instantiated: rebalance_days controls the cadence (measured in bars) at which the strategy runs its rebalance routine, target_percent sets the fraction of portfolio equity the strategy aims to put to work after risk-equalized weights are computed, and lookback determines the length of the rolling window used to estimate each asset’s volatility (the standard deviation) that drives the equal-volatility weighting. These defaults directly steer the data and control flow: the lookback window feeds the volatility estimator used during next, rebalance_days gates when the rebalance method recomputes and applies weights, and target_percent scales the resulting position sizes before orders are issued; Backtrader reads these parameters at instantiation so runs and experiments can vary behavior without changing code. This follows the same params pattern used elsewhere in the codebase (for example, the Sortino analyzer’s params and other strategy variants); compared with the similar parameter sets, this instance uses a relatively short lookback of twenty-one bars, whereas a sibling variant uses sixty, and another variant exposes leverage and an inverted target_percent for short or leveraged behavior."
  },
  {
    "open-file": "backtest/algos/EqualVolatility.py",
    "range": {
      "start": {
        "line": 10,
        "character": 4
      },
      "end": {
        "line": 11,
        "character": 36
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)",
    "narration": "EqualVolatility.__init__ performs a single, crucial bootstrapping step: it invokes the framework’s Strategy base initializer so the EqualVolatility instance inherits the standard Backtrader wiring and the framework’s common bookkeeping and logging behavior before any strategy-specific logic runs. By calling Strategy.__init__, the strategy gets the underlying Backtrader strategy initialization and the base attributes and flags that Strategy.__init__ establishes (order tracking, executed price/commission placeholders, the order_rejected flag, and the verbose switch that Strategy.log uses to control console output). That setup guarantees notify_order and log will behave predictably when EqualVolatility later executes its rebalancing and next logic (and it aligns with the lightweight smoke-test prints you saw earlier that exercise the VixTermStructure). The call mirrors the same initialization pattern used by other strategies such as BuyAndHold, CrossOver, and WeightedHold, with those strategies adding their own indicators or attributes after the base initializer; EqualVolatility, by contrast, defers any additional per-strategy state to its other lifecycle methods and relies on the base initializer to provide the shared infrastructure it needs for risk-equalized rebalancing and runtime reporting."
  },
  {
    "open-file": "backtest/algos/BaseStrategy.py",
    "range": {
      "start": {
        "line": 8,
        "character": 4
      },
      "end": {
        "line": 14,
        "character": 42
      }
    },
    "code": "    def __init__(self, kwargs=None):\n        bt.Strategy.__init__(self)\n        self.order = None\n        self.buyprice = None\n        self.buycomm = None\n        self.order_rejected = False\n        self.verbose = self.params.verbose",
    "narration": "Strategy.__init__ performs the minimal, common setup every concrete strategy needs so the Backtrader lifecycle and the framework’s shared utilities behave consistently. It first delegates to Backtrader by invoking the parent strategy initializer to hook into the engine’s lifecycle and analyzers. It then creates and zeroes the small set of instance fields that coordinate order lifecycle and reporting: a placeholder for the currently tracked order, containers for the last executed buy price and commission, and a boolean flag used to mark a rejected order. Finally it copies the verbosity toggle out of the strategy parameters so Strategy.log can decide whether to print runtime messages. The result is a standardized starting state that concrete strategies such as BuyAndHold, CrossOver, EqualVolatility, LeveragedEtfPair, MeanReversion, NCAV, PairSwitching, and WeightedHold inherit — for example, CrossOver.next uses the order placeholder to avoid duplicate submissions and checks the order_rejected flag to allow retries, while notify_order updates the order, buyprice and buycomm used for logging. This small initializer implements the template-style pattern of centralizing common state so individual strategy subclasses can concentrate on their trading logic."
  },
  {
    "open-file": "backtest/algos/EqualVolatility.py",
    "range": {
      "start": {
        "line": 12,
        "character": 4
      },
      "end": {
        "line": 30,
        "character": 72
      }
    },
    "code": "    def rebalance(self):\n        vols = []\n        for d in self.datas:\n            returns = pd.Series(d.close.get(size=self.params.lookback))\n            returns = np.log(returns).diff().iloc[1:]\n            vol = returns.std()\n            vols.append(vol)\n        vols = np.array(vols)\n        order_sort = []\n        weights = []\n        for v, d in zip(vols, self.datas):\n            weight = (1.0 / v) / sum(1.0 / vols)\n            weights.append(weight)\n            position = self.getposition(d)\n            position_value = position.size * position.price\n            order_target = self.params.target_percent * weight * self.broker.get_value()\n            order_sort.append(order_target - position_value)\n        for s, d, w in sorted(zip(order_sort, self.datas, weights), key=lambda pair: pair[0]):\n            self.order_target_percent(d, self.params.target_percent * w)",
    "narration": "EqualVolatility.rebalance walks through the universe of Backtrader datas attached to the strategy, measures each asset’s recent realized volatility, converts those volatilities into inverse-volatility weights,"
  },
  {
    "create-file": "tools/std.py"
  },
  {
    "open-file": "tools/std.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 4,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nimport pandas as pd\nimport numpy as np",
    "narration": "The module pulls in four fundamentals: os to let the utilities interact with the filesystem and environment when they need to inspect files, build paths, or gate behavior during interactive runs; argparse so the file can expose a tiny command-line surface for quick smoke tests and developer checks (the same lightweight CLI surface that the earlier interactive prints use when querying VixTermStructure contango pairs); pandas to provide the DataFrame and Series abstractions, time-series indexing, and missing-data/rolling utilities that std() and gap_L13_21 operate on; and numpy to supply the vectorized numerical primitives and array math used in the standardized/volatility calculations and gap detection. This set mirrors the common pattern across the codebase of relying on pandas and numpy under their familiar aliases, but is deliberately slimmer than other modules that also import datetime, plotting helpers, or SciPy because this file’s role is a compact, reusable stats helper consumed by strategies and analyzers (similar in spirit to the Sortino analyzer’s use of params and RATEFACTORS for scaling) rather than a plotting or heavyweight statistical component."
  },
  {
    "open-file": "tools/std.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 12,
        "character": 55
      }
    },
    "code": "def std(ticker, length=250, usereturns=False):\n    path = os.path.join(os.path.dirname(__file__), f'../data/price/{ticker}.csv')\n    price = pd.read_csv(path, parse_dates=True, index_col='Date')['Adj Close'].rename('Price')\n    s = price\n    if usereturns:\n        s = np.log(price).diff().iloc[1:]\n    print(f'{ticker} ${price.iloc[-1]} ({length})' + (' [Using Returns]' if usereturns else ''))\n    print('std:\\t\\t', round(s.iloc[-length:].std(), 5))",
    "narration": "std is a small, mid-level utility that gives a quick volatility diagnostic for a single ticker so strategies and developer tooling get a consistent volatility number from the same local price dataset. It takes a ticker symbol, a lookback length, and a flag that toggles whether to compute volatility on raw adjusted prices or on log returns. The function builds a filesystem path to the ticker’s CSV in the project's price data area and reads it with pandas.read_csv while parsing dates and using the Date column as the index; it then extracts the adjusted close series and labels it Price. If usereturns is true it converts that price series into log returns by taking the logarithm, differencing, and dropping the initial NA; otherwise it keeps the price series itself. There is a single control branch for that choice. Finally it computes the sample standard deviation over the trailing window defined by length, rounds the result to five decimal places, and prints two diagnostic lines: one showing the ticker, the most recent price and the window length (annotated when returns were used), and a second showing the computed standard deviation. The function performs file I/O and emits console output; unlike the framework’s Strategy.log helper used throughout strategies for structured per-bar logging, std prints directly for quick ad-hoc inspection. Compared with get_returns, which prints multiple return metrics and also returns values, std is narrowly focused on producing a single volatility summary (with an option to use returns) so different"
  },
  {
    "open-file": "backtest/algos/EqualVolatility.py",
    "range": {
      "start": {
        "line": 31,
        "character": 4
      },
      "end": {
        "line": 38,
        "character": 39
      }
    },
    "code": "    def next(self):\n        if self.order:\n            return\n        if len(self) % self.params.rebalance_days == 0:\n            self.rebalance()\n        elif self.order_rejected:\n            self.rebalance()\n            self.order_rejected = False",
    "narration": "EqualVolatility.next is the per-bar lifecycle hook that decides when the strategy actually runs its equal-volatility reweighting routine; because EqualVolatility is the entry point for the strategy, next governs when rebalance gets invoked and guards against concurrent or failed order states. On every incoming bar it first checks the pending-order flag that Strategy.__init__ and the framework manage—if there is an outstanding order it returns immediately so no new trading actions are started while an order may still be filling. If there is no pending order it uses Backtrader’s bar counter (len(self)) together with the rebalance_days parameter to enforce the periodic cadence: when the bar count modulo rebalance_days equals zero it calls EqualVolatility.rebalance, which you already saw computes rolling volatilities across the attached datas, consults VixTermStructure for regime context when needed, and logs actions via Strategy.log while producing the portfolio side effects. The final branch handles a recent order rejection: if an order was previously rejected it retries the rebalancing immediately and clears the order_rejected flag so the retry only happens once. This control flow implements a simple scheduling and retry pattern that other strategies in the codebase follow (for example LeveragedEtfPair and NCAV use the same pending-order guard and periodic trigger; NCAV also runs an extra filter step before rebalance, and PairSwitching offsets the cadence slightly), ensuring the EqualVolatility strategy runs on a predictable cadence while handling in-flight and failed order conditions."
  },
  {
    "create-file": "backtest/algos/MeanReversion.py"
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 3,
        "character": 0
      }
    },
    "code": "import pandas as pd\nimport numpy as np\nfrom . import BaseStrategy as base",
    "narration": "The file brings in pandas and numpy to support the numeric and time-series work the mean-reversion strategy performs, and it pulls in the framework’s BaseStrategy under the alias base so the concrete strategy can use the shared Strategy behavior. pandas supplies the DataFrame/Series abstractions, rolling windows, indexing, and missing-data semantics the strategy needs to compute rolling volatility, z-scores and gap-based signals, while numpy provides the vectorized arithmetic and array operations used for standardizing returns, computing equal-volatility/Kelly weights, and any fast elementwise math. Importing BaseStrategy as base gives the class access to the framework’s common lifecycle, bookkeeping, and order-placement helpers that EqualVolatility relies on (recall EqualVolatility.__init__ delegates to the shared Strategy initializer), so the signal math done with pandas and numpy feeds directly into the execution methods defined on the base. This mirrors the project’s common pattern where modules pair the numeric libraries with the local Strategy base; other files follow the same import combination, though some also include filesystem or CLI helpers when they need them."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 4,
        "character": 0
      },
      "end": {
        "line": 125,
        "character": 23
      }
    },
    "code": "class MeanReversion(base.Strategy):\n    params = {\n        'target_percent': 0.95,\n        'riskfreerate': 0,\n        'quantile': 0.10,\n        'npositions': 25,\n        'quantile_std': 0.10,\n        'quantile_vol': 1.0,\n        'lookback': 6,\n        'offset': 1,\n        'order_frequency': 5,\n        'cheat_on_open': False\n    }\n    def __init__(self):\n        ...\n    def add_rank(self):\n        ...\n    def add_filter(self):\n        ...\n    def process(self):\n        ...\n    def send_orders(self):\n        ...\n    def set_kelly_weights(self):\n        ...\n    def close_positions(self):\n        ...\n    def next(self):\n        ...\n    def prenext(self):\n        ...",
    "narration": "MeanReversion is the strategy that implements the framework’s mean-reversion trading logic: after inheriting the shared Backtrader wiring and bookkeeping from Strategy.__init__ (the same bootstrapping pattern EqualVolatility.__init__ used), its constructor initializes counters, an empty ranking series, filter containers and placeholders for the top/bottom groups, long/short target lists, and order bookkeeping so the strategy can maintain state across bars. The params dictionary you saw earlier supplies the cadence, sizing and lookback knobs that govern how often process runs and how aggressive sizing will be, while add_rank and add_filter encapsulate the signal-generation and candidate-pruning stages: add_rank turns the per-asset statistical signals (volatility and gap/std signals computed with pandas and numpy) together with VIX-term structure context into a scalar score per asset, and add_filter applies the screening rules that remove illiquid or regime-inappropriate names. process is the orchestration layer that runs ranking and filtering, selects the top and bottom quantiles or fixed npositions as the long and short candidates, and prepares the trade intentions; it also interprets VIX regime information to switch behavior between normal mean-reversion mode and risk-off handling. set_kelly_weights computes portfolio weights from expected return and variance information using a Kelly-derived sizing approach scaled by the target_percent param so position sizes are risk-calibrated rather than equal-volatility weighted; send_orders takes those target weights and translates them into concrete Backtrader orders via the Strategy order plumbing and relies on Strategy.notify_order/log for lifecycle and auditing. close_positions contains the logic to unwind names that fail filters or when regime or ranking changes require exits. Control flow is driven by prenext and next: prenext handles the bootstrap period before lookback is satisfied so the engine does not act on incomplete statistics, and next advances a counter and triggers process/send_orders/close_positions on the configured order cadence and offsets so rebalances occur predictably. Conceptually, MeanReversion follows the same Strategy base pattern you saw in BuyAndHold, CrossOver and EqualVolatility but replaces simple buy/hold or volatility-equal rules with a rank-then-filter pipeline plus Kelly sizing, making it the engine’s dedicated statistical mean-reversion implementation that centralizes per-bar decisioning and order management."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 5,
        "character": 4
      },
      "end": {
        "line": 16,
        "character": 5
      }
    },
    "code": "    params = {\n        'target_percent': 0.95,\n        'riskfreerate': 0,\n        'quantile': 0.10,\n        'npositions': 25,\n        'quantile_std': 0.10,\n        'quantile_vol': 1.0,\n        'lookback': 6,\n        'offset': 1,\n        'order_frequency': 5,\n        'cheat_on_open': False\n    }",
    "narration": "The params dictionary on MeanReversion enumerates the strategy’s tunable knobs that Backtrader will read at instantiation and that steer every stage of the mean‑reversion pipeline: portfolio sizing, signal construction, filtering, and order cadence. target_percent is the same high‑level lever you saw on EqualVolatility that controls the fraction of equity the strategy aims to put to work after weights are computed; riskfreerate is exposed so the Kelly sizing routine has a baseline return assumption when set_kelly_weights computes position sizes. quantile, quantile_std, and quantile_vol are the three thresholds the ranking and filtering routines use to select the candidate names — they determine how extreme a standardized signal must be, how large a recent standard deviation must be, and how volatile an asset must be to survive the filters applied in add_rank and add_filter. npositions caps the number of simultaneous holdings that process and send_orders will try to maintain, and lookback defines the rolling window length used by the standard-deviation and standardization calculations that rebalance and set_kelly_weights rely on (note this is a much shorter default than EqualVolatility’s lookback). offset introduces a signal lag/shift so the strategy can operate on prior-bar information, order_frequency controls the rebalance cadence in bar units (playing the same role as rebalance_days in the other strategy variants), and cheat_on_open toggles whether orders may be sent using open prices versus waiting for confirmed closes. Together these parameters let the pipeline — from add_rank through process, set_kelly_weights, send_orders, and the lifecycle hooks prenext/next — be configured for different risk regimes and execution cadences without changing implementation."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 17,
        "character": 4
      },
      "end": {
        "line": 24,
        "character": 24
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)\n        self.count = 1\n        self.rank = pd.Series()\n        self.filter = []\n        self.top = self.bottom = self.longs = self.shorts = self.closes = None\n        self.order_valid = False\n        self.values = []",
    "narration": "MeanReversion.__init__ bootstraps the mean-reversion strategy’s runtime state by first delegating to base.Strategy.__init__ so the instance inherits the framework’s common Backtrader wiring and bookkeeping (as we covered in Strategy.__init__), then it initializes the lightweight counters and containers that the rest of the class expects during the per-bar lifecycle. It sets count to one as the running bar counter that next and prenext will increment and use to decide when rebalances are allowed, creates rank as an empty pandas Series that add_rank will populate with each asset’s percent-return scores, and makes filter an empty list used by add_filter and add_rank to track which datafeeds are eligible. It initializes top, bottom, longs, shorts, and closes to None so process can later compute and assign the selected long/short/closes lists; order_valid starts false to act as the gate that next/prenext flip when enough history and the rebalance cadence are met; and values is an empty list that set_kelly_weights will append portfolio values to for computing Kelly-based sizing. This initialization follows the same pattern seen in the other strategy constructors—call the shared Strategy initializer, then establish strategy-specific attributes—so all downstream methods (add_rank, add_filter/process, send_orders, close_positions, set_kelly_weights, next/prenext) have the correctly typed, empty structures to read from and write to as bars are processed."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 25,
        "character": 4
      },
      "end": {
        "line": 41,
        "character": 77
      }
    },
    "code": "    def add_rank(self):\n        for i, d in enumerate(self.datas):\n            if len(d) < self.params.lookback + self.params.offset:\n                continue\n            if i not in self.filter:\n                continue\n            prev = d.close.get(size=self.params.lookback, ago=self.params.offset)[0]\n            pct_ret = (d.close[0] / prev) - 1\n            self.rank.loc[i] = pct_ret\n        if self.params.npositions > 0:\n            self.top = list(self.rank.nlargest(self.params.npositions).index)\n            self.bottom = list(self.rank.nsmallest(self.params.npositions).index)\n        else:\n            quantile_top = self.rank.quantile(1 - self.params.quantile)\n            self.top = list(self.rank[self.rank >= quantile_top].index)\n            quantile_bottom = self.rank.quantile(self.params.quantile)\n            self.bottom = list(self.rank[self.rank <= quantile_bottom].index)",
    "narration": "MeanReversion.add_rank walks the universe of Backtrader data feeds attached to the strategy and populates the per-asset ranking Series used to decide which names to long, short, or close. Remember the rank Series was created in MeanReversion.__init__ and add_filter builds the set of candidate indices in self.filter; add_rank skips any data feed that does not yet have enough history to compute a lookback return or that is not in self.filter. For each eligible feed it retrieves a prior close from the configured lookback window (respecting the offset), computes the percentage return from that prior close to the current close, and stores that percentage in self.rank at the feed’s numeric index. After scanning all feeds it turns the rank Series into selection lists: if params.npositions is positive it takes the npositions largest and smallest returns as self.top and self.bottom respectively; otherwise it computes quantile thresholds and selects all assets above the top quantile and below the bottom quantile. These self.top and self.bottom lists are the inputs that MeanReversion.process uses to build the longs, shorts, and closes lists, which downstream routines such as send_orders, set_kelly_weights, and close_positions rely on to size and submit trades. The method therefore enforces the guard clauses around data sufficiency, applies the framework’s precomputed filter, writes the ranking and selected index lists to instance state, and implements the core mean-reversion decision rule of shorting recent winners and buying recent losers."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 42,
        "character": 4
      },
      "end": {
        "line": 58,
        "character": 46
      }
    },
    "code": "    def add_filter(self):\n        sd = pd.Series()\n        vol = pd.Series()\n        for i, d in enumerate(self.datas):\n            if len(d) < self.params.lookback + self.params.offset:\n                continue\n            lookback = d.close.get(size=self.params.lookback, ago=self.params.offset)\n            returns = np.diff(np.log(lookback))[1:]\n            sd.loc[i] = np.std(returns)\n            lookback = d.close.get(size=min(126, len(d)), ago=self.params.offset)\n            returns = np.diff(np.log(lookback))[1:]\n            vol.loc[i] = np.std(lookback)\n        quantile_std = sd.quantile(1 - self.params.quantile_std)\n        quantile_vol = vol.quantile(1 - self.params.quantile_vol)\n        sd = list(sd[sd <= quantile_std].index)\n        vol = list(vol[vol <= quantile_vol].index)\n        self.filter = list(set(sd) | set(vol))",
    "narration": "MeanReversion.add_filter is the strategy’s volatility-based screener that trims the full Backtrader data universe down to the candidates that will be ranked and traded; it is invoked from MeanReversion.process and directly determines which assets add_rank will consider. It walks the attached Backtrader datas, skipping any feed that does not yet have enough bars to satisfy the lookback plus offset guard, and for each eligible series it samples closes from the past via the Backtrader data API, computes log returns and records a short-window standard deviation into a pandas Series labeled sd, then samples a longer window (up to 126 bars or the series length) and records a second volatility metric into a separate pandas Series labeled vol. After the per-asset loop it computes upper cutoff thresholds as the (1 - quantile) values for both the short-run and longer-run measures (driven by params.quantile_std and params.quantile_vol), selects the asset indices whose short-run or long-run measure is at or below those thresholds, unions those two sets, and writes that union to self.filter so subsequent ranking only considers those indices. The function uses numpy and pandas for the numerical work and relies on the Backtrader data get semantics you’ve seen elsewhere; when verbose behavior or regime context is needed the routine can tie into Strategy.log for output and to VixTermStructure.get or the file-backed std helper elsewhere in the module for additional regime or cross-check information, but its primary dataflow is: read historical closes → compute per-asset volatility measures → compute quantile cutoffs → produce a filtered index list that feeds add_rank and the rest of the trading pipeline."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 59,
        "character": 4
      },
      "end": {
        "line": 67,
        "character": 10
      }
    },
    "code": "    def process(self):\n        self.add_filter()\n        self.add_rank()\n        self.longs = [d for (i, d) in enumerate(self.datas) if i in self.bottom]\n        self.shorts = [d for (i, d) in enumerate(self.datas) if i in self.top]\n        self.closes = [d for d in self.datas if (\n            (d not in self.longs) and\n            (d not in self.shorts)\n        )]",
    "narration": "MeanReversion.process is the per-bar coordinator that turns the quantitative screening and ranking logic into concrete baskets the rest of the strategy will act on. It first invokes MeanReversion.add_filter so the volatility-based screening described in add_filter populates the strategy’s filter set, then invokes MeanReversion.add_rank so the recent return rankings are computed and the top and bottom index lists are populated. After those two calculations it maps the numeric index sets produced by add_rank into actual Backtrader datafeed objects: longs becomes the datafeeds whose indices appear in bottom (the assets the mean-reversion logic wants to buy), shorts becomes the datafeeds whose indices appear in top (the assets to short), and closes becomes the remaining datafeeds that are neither long nor short so they can be closed out. Those three lists are saved as instance attributes so subsequent steps in the bar lifecycle—set_kelly_weights, send_orders, and close_positions that are called from next/prenext—have the concrete datafeeds to size, order, and close. The method therefore acts as the bridge between the statistical work in add_filter/add_rank (which read historical bars and compute volatility/return metrics) and the order-execution plumbing the Strategy base and send_orders implement."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 68,
        "character": 4
      },
      "end": {
        "line": 78,
        "character": 61
      }
    },
    "code": "    def send_orders(self):\n        for d in self.longs:\n            if len(d) < self.params.lookback + self.params.offset:\n                continue\n            split_target = 1 * self.params.target_percent / len(self.longs)\n            self.order_target_percent(d, target=split_target)\n        for d in self.shorts:\n            if len(d) < self.params.lookback + self.params.offset:\n                continue\n            split_target = -1 * self.params.target_percent / len(self.shorts)\n            self.order_target_percent(d, target=split_target)",
    "narration": "MeanReversion.send_orders is the final executor that turns the lists of long and short candidates into actual portfolio targets so Backtrader can place orders. It expects the universe membership to have been prepared by MeanReversion.process (which fills self.longs, self.shorts and self.closes) and it relies on the initial attributes set in MeanReversion.__init__ so the lists exist. For each instrument in the longs list it first guards against insufficient history by comparing the data feed length to the lookback plus offset parameters, skipping any asset that doesn't yet have enough bars for the signal window; it then computes an equal split of the overall target exposure by dividing the configured target_percent across the number of long names and asks Backtrader to set the instrument’s portfolio weight to that positive fraction. It repeats the same flow for the shorts list but negates the split so the requested portfolio weight is short. Because close_positions is called before send_orders, instruments that are no longer in either bucket are already queued to be closed and therefore send_orders only needs to assign target percents for the active long and short buckets; the actual placement is performed by Backtrader when order_target_percent is invoked for each data feed. This routine therefore implements the strategy’s simple equal-split sizing across the ranked buckets, using the lookback/offset guards to avoid acting on insufficient data and relying on the framework’s order_target_percent plumbing to effect the trades."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 79,
        "character": 4
      },
      "end": {
        "line": 94,
        "character": 45
      }
    },
    "code": "    def set_kelly_weights(self):\n        value = self.broker.get_value()\n        self.values.append(value)\n        kelly_lookback = 20\n        if self.count > kelly_lookback:\n            d = pd.Series(self.values[-kelly_lookback:])\n            r = d.pct_change()\n            mu = np.mean(r)\n            std = np.std(r)\n            if std == 0.0:\n                return\n            f = (mu)/(std**2)\n            if f == np.nan:\n                return\n            self.params.target_percent = max(0.2, min(2.0, f / 2.0))\n            print(self.params.target_percent)",
    "narration": "MeanReversion.set_kelly_weights updates the strategy’s position sizing by computing a simple Kelly-style bet fraction from recent portfolio-level returns and writing that into params.target_percent so the order executor can use it. Each time it runs it queries the broker for the current portfolio value via broker.get_value and appends that snapshot to the self.values list created in MeanReversion.__init__. Once there are more than a fixed lookback of value observations (20), it builds a pandas Series from the most recent window, converts those prices into period returns via a percent-change, and summarizes those returns by mean and standard deviation. It guards against degenerate cases by returning early if the observed volatility is zero or if the computed fraction is NaN. The core sizing rule follows a Kelly-style formula where the mean is divided by variance to produce a raw fraction; the implementation then halves that raw Kelly fraction (a half-Kelly choice) and clamps the resulting target between a lower and upper bound so the strategy never sets an extreme target percent. The method has two side effects: it mutates params.target_percent with the bounded, half-Kelly value and prints that value to the console; downstream, send_orders consumes params.target_percent to split and place portfolio targets during the normal order-execution flow."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 95,
        "character": 4
      },
      "end": {
        "line": 97,
        "character": 25
      }
    },
    "code": "    def close_positions(self):\n        for d in self.closes:\n            self.close(d)",
    "narration": "MeanReversion.close_positions walks the list stored in self.closes and, for each Backtrader data feed in that list, instructs the framework to flatten any existing position by calling the strategy-level close operation. self.closes is populated earlier in MeanReversion.process (which itself calls add_filter and add_rank), so close_positions is the explicit unwinding step in the per-bar lifecycle: it runs after process determines which names should be long, short, or closed and before send_orders sets new portfolio targets. Its sole effect is side-effecting — submitting close orders to Backtrader’s execution layer for each candidate — and it does not return a value."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 98,
        "character": 4
      },
      "end": {
        "line": 111,
        "character": 23
      }
    },
    "code": "    def next(self):\n        self.order_valid = (\n            self.count > (self.params.lookback + self.params.offset) and\n            self.count % self.params.order_frequency == 0\n        )\n        if self.order_valid:\n            self.process()\n            self.close_positions()\n            self.send_orders()\n        elif self.order_rejected:\n            self.send_orders()\n            self.order_rejected = False\n        self.set_kelly_weights()\n        self.count += 1",
    "narration": "MeanReversion.next is the per-bar orchestrator that Backtrader calls as the engine steps through time; its role is to decide when the strategy is ready to re-run the screening/ranking/execution cycle and to perform a single cycle when appropriate. It first computes a boolean readiness flag by checking the running counter against the lookback and offset requirements and by enforcing the configured order_frequency cadence so trades only happen after enough history has accumulated and only at the desired intervals. When that readiness flag is true it invokes MeanReversion.process to rebuild the candidate baskets (you already saw that process calls add_filter and add_rank), then calls MeanReversion.close_positions to submit closes for assets that are no longer in the target baskets, and finally calls MeanReversion.send_orders to turn the computed longs and shorts into actual portfolio targets. If the readiness check fails but there was a previously rejected order, next attempts to re-send orders via MeanReversion.send_orders and clears the order_rejected flag (order_rejected is initialized by Strategy.__init__). Regardless of branching, next always calls MeanReversion.set_kelly_weights to record the broker value and update sizing information used by future allocations, and then increments the internal counter so the cadence logic advances. The overall control-flow enforces the strategy’s guard rails (minimum lookback, offset, and rebalance frequency), retries on rejected orders, and centralizes the per-bar lifecycle that drives screening, position closure, and order submission."
  },
  {
    "open-file": "backtest/algos/MeanReversion.py",
    "range": {
      "start": {
        "line": 112,
        "character": 4
      },
      "end": {
        "line": 125,
        "character": 23
      }
    },
    "code": "    def prenext(self):\n        self.order_valid = (\n            self.count > (self.params.lookback + self.params.offset) and\n            self.count % self.params.order_frequency == 0\n        )\n        if self.order_valid:\n            self.process()\n            self.close_positions()\n            self.send_orders()\n        elif self.order_rejected:\n            self.send_orders()\n            self.order_rejected = False\n        self.set_kelly_weights()\n        self.count += 1",
    "narration": "prenext implements the strategy’s per-bar orchestration during Backtrader’s warm-up phase: it first evaluates whether the current bar should trigger rebalancing by comparing the running counter (initialized in MeanReversion.__init__) against the lookback plus offset requirement and the configured order_frequency cadence; when that check passes it drives the main pipeline by calling MeanReversion.process to re-run the volatility filter and ranking, MeanReversion.close_positions to exit names that fell out of the active baskets, and MeanReversion.send_orders to convert the newly computed long and short lists into portfolio targets; if the bar is not a scheduled rebalancing bar but an earlier order was rejected it retries order placement by invoking MeanReversion.send_orders again and clears the order_rejected flag; in all cases it then invokes MeanReversion.set_kelly_weights to record current broker value and update the Kelly-derived sizing history before incrementing the internal count so the next bar advances correctly. This mirrors the behavior in MeanReversion.next so the same screening, sizing and execution steps are available during the prenext lifecycle, ensuring the mean-reversion ranking, filtering and Kelly position-sizing remain consistent even while feeds are still warming up."
  },
  {
    "create-folder": "api"
  },
  {
    "create-file": "api/iex.py"
  },
  {
    "open-file": "api/iex.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 4,
        "character": 0
      }
    },
    "code": "import json\nimport os\nimport requests\nfrom urllib.parse import urlparse",
    "narration": "The file pulls in a tiny runtime toolbox that supports making HTTP requests to IEX, parsing the responses, and reading environment or filesystem configuration so the adapter can return normalized daily bars to the engine. json is there to decode and encode the IEX payloads or local cache files; os is used for environment-driven configuration (API tokens, cache directories, endpoint overrides) and for basic filesystem operations when the adapter persists or looks up stored responses; requests provides the HTTP client used to call IEX’s REST endpoints to retrieve the historical bar JSON; and urlparse from urllib.parse is used to inspect and normalize endpoint URLs (extracting hosts, paths, or validating a base URL before building request URIs). Compared with other modules that import pandas and fast_arrow for heavier data-frame and marketdata objects, this adapter intentionally keeps a lightweight stack focused on HTTP and JSON handling so it can produce clean, normalized bars for downstream consumers like MeanReversion.add_filter and MeanReversion.add_rank, and then hand those series off to VixTermStructure.get for market‑regime enrichment. The import pattern mirrors other files’ use of os and json for config and caching but replaces heavier data libraries with HTTP/url utilities appropriate for a network-backed data-provider adapter."
  },
  {
    "open-file": "api/iex.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 5,
        "character": 14
      }
    },
    "code": "IEX_TOKEN = ''",
    "narration": "IEX_TOKEN is a module-level placeholder that holds the IEX Cloud API credential the IEX adapter uses when dailyHistorical builds and issues authenticated HTTP requests; declaring it alongside API_ENDPOINT makes the adapter’s external-facing configuration explicit at the top of the file so the request-building logic can reference a known symbol for the token. It follows the same pattern as API_ENDPOINT, which is the companion constant for the service base URL, and differs from config_path which points to the local JSON file the module can read to populate runtime values like the token. The surrounding imports such as requests, os, json and urlparse give the adapter the ability to load and parse a config file or environment values and then attach IEX_TOKEN to outgoing calls; after dailyHistorical retrieves raw bars using that credential, the code then calls VixTermStructure.get to enrich the returned series for downstream backtests and reports. The empty string is therefore a deliberate placeholder expecting the real API key to be supplied externally at runtime via the project’s configuration loading path."
  },
  {
    "open-file": "api/iex.py",
    "range": {
      "start": {
        "line": 6,
        "character": 0
      },
      "end": {
        "line": 6,
        "character": 17
      }
    },
    "code": "API_ENDPOINT = ''",
    "narration": "API_ENDPOINT is a module-level placeholder that holds the base URL the IEX adapter uses to build its HTTP calls when dailyHistorical fetches and normalizes daily bars; conceptually it supplies the network address that the adapter combines with path/query pieces and the IEX_TOKEN to form the actual request URIs. It sits alongside IEX_TOKEN as a complementary configuration value (IEX_TOKEN carries authentication while API_ENDPOINT carries the service location) and is expected to be populated from the local configuration referenced by config_path or an equivalent runtime mechanism; the file’s imports for requests and urlparse are the plumbing that dailyHistorical uses to act on API_ENDPOINT and perform the HTTP round-trip before enriching results with VixTermStructure.get for downstream backtests and reports."
  },
  {
    "open-file": "api/iex.py",
    "range": {
      "start": {
        "line": 7,
        "character": 0
      },
      "end": {
        "line": 7,
        "character": 77
      }
    },
    "code": "config_path = os.path.join(os.path.dirname(__file__), '../config.local.json')",
    "narration": "config_path is computed at module import to point the IEX adapter at a local JSON configuration file living one directory up from the adapter’s source file. Conceptually this tells the IEX data-provider where to load runtime configuration like the IEX_TOKEN, API_ENDPOINT and dataset PATHS instead of embedding those values as literals inside the module. The computation uses the adapter file’s directory as the anchor so path resolution is stable regardless of the process working directory and uses OS path utilities so separators are correct across platforms. In the data pipeline this allows dailyHistorical to retrieve credentials and file locations from a single local config file before it fetches and normalizes bars and then calls VixTermStructure.get to annotate the history with market‑regime context. Compared with the nearby IEX_TOKEN and API_ENDPOINT placeholders (which are empty strings) and the hardcoded PATHS mapping, config_path centralizes configuration into an external JSON so the adapter can populate those runtime values at load time."
  },
  {
    "open-file": "api/iex.py",
    "range": {
      "start": {
        "line": 8,
        "character": 0
      },
      "end": {
        "line": 11,
        "character": 53
      }
    },
    "code": "with open(config_path) as config_file:\n    config = json.load(config_file)\n    IEX_TOKEN = config['iex-cloud-api-token-test']\n    API_ENDPOINT = config['iex-api-endpoint-sandbox']\n",
    "narration": "This snippet opens the JSON file referenced by config_path and parses it to populate the two runtime constants the IEX adapter needs: IEX_TOKEN is set from the test API token entry and API_ENDPOINT is set from the sandbox endpoint entry. Those values supply the authentication and target URL that the adapter’s dailyHistorical fetch uses when it requests and normalizes daily bars from IEX, and the resulting normalized history is what later gets enriched by VixTermStructure.get and ultimately feeds the strategy pipeline that MeanReversion.process, MeanReversion.add_filter, and MeanReversion.add_rank operate on. Compared with the nearby placeholder lines that simply define empty IEX_TOKEN and API_ENDPOINT and the earlier config_path assignment, this code replaces those placeholders by actually loading the concrete, environment-specific values from the local JSON configuration."
  },
  {
    "open-file": "api/iex.py",
    "range": {
      "start": {
        "line": 12,
        "character": 0
      },
      "end": {
        "line": 21,
        "character": 67
      }
    },
    "code": "def dailyHistorical(ticker, range):\n    print('Retrieving data for: {0}'.format(ticker))\n    url = API_ENDPOINT + '/stock/' + ticker + '/chart/' + range\n    resp = requests.get(urlparse(url).geturl(), params={\n        'token': IEX_TOKEN,\n        'chartCloseOnly': True\n    })\n    if resp.status_code == 200:\n        return resp.json()\n    raise Exception('Response %d - ' % resp.status_code, resp.text)",
    "narration": "dailyHistorical is the IEX adapter's single-symbol fetcher that the framework uses to pull raw daily bar data into the data pipeline so Backtrader feeds and downstream strategy code can consume it; in the larger system run_strategy and the strategy lifecycle rely on this adapter to turn a ticker and a range into a usable time series that MeanReversion.add_filter, MeanReversion.add_rank and the rest of the strategy code will operate on. When invoked, dailyHistorical logs the symbol being retrieved, composes the IEX chart URL for the given ticker and range, and issues an HTTP GET with the project’s IEX token and a flag asking for close-only chart points; the function treats a 200 response as the happy path and returns the parsed JSON payload immediately for normalization and attachment to Backtrader data feeds, while any non-200 reply triggers an exception that surfaces the status and response body. Remember VixTermStructure.get we examined earlier; the adapter’s fetched price history is intended to be enriched with VIX term-structure values obtained via that helper so downstream backtests and reports get market-regime context alongside prices. The visible control flow is therefore simple: print, build URL, perform network request, return parsed data on success, raise on failure, with the side effects being a network call and console output."
  },
  {
    "create-file": "api/yahoo.py"
  },
  {
    "open-file": "api/yahoo.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 5,
        "character": 0
      }
    },
    "code": "import time\nimport pandas as pd\nimport yfinance as yf\nfrom datetime import date\nfrom tools.log import log",
    "narration": "Because the Yahoo adapter's job is to fetch, normalize, and log daily price series for the pipeline, the imports reflect the pieces it needs to do that work. The standard library time is brought in to drive simple timing behaviors such as sleep-based retry/backoff and rate-limiting between network calls. Pandas is used to represent and massage the returned price data into the DataFrame-shaped time series the rest of the data layer expects, including dtype and index normalization before downstream save_ticker/update_ticker and bulk save_all hand off the results. The yfinance client is the adapter's direct conduit to Yahoo’s public endpoints and supplies the higher-level download and ticker-info utilities the adapter uses instead of managing raw HTTP requests itself. The date class from datetime is used to compute and validate start/end ranges and to convert between calendar dates and the index types pandas will consume. Finally, the log function from tools.log is used to emit the strategy-integrated, runtime logs the file produces during fetches and retries so downloads are visible in the framework’s reporting. These choices mirror a common pattern you’ve already seen elsewhere in the codebase where modules import pandas and date handling and wire into tools.log; the difference here is the direct dependency on yfinance (and the explicit use of time for retry timing) rather than the token-and-endpoint configuration approach that the IEX adapter used as we examined earlier."
  },
  {
    "open-file": "api/yahoo.py",
    "range": {
      "start": {
        "line": 8,
        "character": 0
      },
      "end": {
        "line": 12,
        "character": 80
      }
    },
    "code": "def get_daily(ticker, start=None):\n    if start == None:\n        return yf.download(ticker, period='max', interval='1d')\n    else:\n        return yf.download(ticker, start=start, end=date.today(), interval='1d')",
    "narration": "get_daily is the Yahoo Finance adapter's single-symbol daily bar retriever that sits in the data layer and supplies the time series that save_ticker and update_ticker persist or append to the local price cache. It accepts a ticker and an optional start date and then decides between two paths: if no start date is provided it asks yfinance for the entire available history for that ticker, otherwise it asks yfinance for data beginning at the provided start up through today, returning a pandas DataFrame of daily bars either way. As part of the download flow it also triggers the VixTermStructure download so the same call brings back market-regime context that VixTermStructure.__init__ will parse into term-structure and contango slices; that auxiliary download prints a short status message while it scrapes and normalizes the VIX term structure. The data returned by get_daily flows directly into save_ticker where it is written out as a CSV, and into update_ticker which first determines the last stored date via get_last_date and then uses get_daily to fetch only the missing rows; Strategy.log is used elsewhere in the update path to emit verbose progress messages. The branching logic here is deliberately simple — one guard for a missing start and one fallback that anchors the end of the range to today — and mirrors the pattern used by get_daily_async, which follows the same conditional but handles multiple tickers and groups results by ticker. Compared with dailyHistorical, get_daily relies on yfinance rather than constructing authenticated HTTP requests to IEX, so its role is to provide a higher-level, convenience-backed download that integrates VIX context for the framework’s save/update pipeline."
  },
  {
    "open-file": "api/yahoo.py",
    "range": {
      "start": {
        "line": 15,
        "character": 0
      },
      "end": {
        "line": 19,
        "character": 100
      }
    },
    "code": "def get_daily_async(tickers, start=None):\n    if start == None:\n        return yf.download(tickers, period='max', interval='1d', group_by=\"ticker\")\n    else:\n        return yf.download(tickers, start=start, end=date.today(), interval='1d', group_by=\"ticker\")",
    "narration": "get_daily_async is the Yahoo adapter that the data layer uses to fetch daily price series for multiple tickers and to surface VIX term-structure context during bulk downloads; it accepts a list of tickers and an optional start date and returns the pandas structure that downstream callers like save_all iterate over. On entry it branches on whether start is provided: if start is absent it requests the full available history for the tickers, otherwise it requests daily bars from the supplied start up through today's date; in both cases it asks for one-day bars and asks yfinance to group the returned table by ticker so the result is organized per-symbol. As part of its download workflow it also invokes VixTermStructure.download to pull the VIX term-structure snapshot (which prints a status line and parses the VIX table from the external site) so the dataset download is accompanied by market-regime context. The shape produced by get_daily_async matches what save_all expects: a grouped DataFrame where save_all can pick out each ticker's first valid row and write the per-symbol CSVs. Functionally it mirrors get_daily for single-ticker use but operates across a batch of tickers and returns the grouped result back into the project’s data ingestion pipeline."
  },
  {
    "open-file": "api/yahoo.py",
    "range": {
      "start": {
        "line": 21,
        "character": 0
      },
      "end": {
        "line": 33,
        "character": 16
      }
    },
    "code": "def retry_ticker(ticker, retry=3):\n    tries = 0\n    while tries < retry:\n        try:\n            t = yf.Ticker(ticker)\n            return t\n        except e:\n            print('ERROR:', ticker)\n            print(e)\n            log.log(type(e).__name__, e)\n            time.sleep(1)\n            tries += 1\n    return None;",
    "narration": "retry_ticker is a synchronous helper in the Yahoo Finance adapter that takes a ticker symbol and attempts to construct a yfinance Ticker handle, returning that handle on the first successful attempt or None after the configured number of retries; it drives the adapter’s tolerance to transient network or provider errors so higher-level callers like get_info can proceed to read attributes such as info or balance_sheet. The function initializes a simple attempt counter and loops until the attempt limit (the retry parameter, default three) is reached; on the happy path it immediately returns the yfinance Ticker object created via yf.Ticker. If an exception occurs, it emits a console error line for the ticker, prints the exception, records the exception through the shared logging helper log.log (the same logging utility used elsewhere in the adapter and complementary to per-strategy logging such as Strategy.log that we reviewed earlier), pauses for one second to avoid tight retry loops, increments the attempt counter, and then tries again. After all attempts are exhausted it returns None so callers can apply their guard logic (get_info checks for a None return and reports a download issue). The control flow therefore cleanly separates the happy path (immediate Ticker handle) from the retry-and-log error path and the final failure path (None), enabling the data layer to be robust to intermittent Yahoo/HTTP problems while leaving error handling and reporting to the caller."
  },
  {
    "open-file": "api/yahoo.py",
    "range": {
      "start": {
        "line": 36,
        "character": 0
      },
      "end": {
        "line": 67,
        "character": 22
      }
    },
    "code": "def get_info(ticker):\n    t = retry_ticker(ticker)\n    if t is None:\n        print('Download Issue:', ticker)\n        return\n\n    info_dict = t.info\n    info = pd.DataFrame.from_dict(info_dict, orient='index').iloc[:, 0].rename('Info')\n\n    try:\n        balance_sheet = t.balance_sheet\n        financials = t.financials\n        cashflow = t.cashflow\n    except IndexError as e:\n        print('ERROR:', ticker)\n        print(e)\n        log.log(type(e).__name__, e)\n        return info\n\n    try:\n        balance_sheet = balance_sheet.iloc[:, 0]\n        financials = financials.iloc[:, 0]\n        cashflow = cashflow.iloc[:, 0]\n    except AttributeError as e:\n        print('ERROR:', ticker)\n        print(e)\n        log.log(type(e).__name__, e)\n        return info\n\n    return_info = pd.concat([balance_sheet, financials, cashflow], axis=0)\n    print(ticker)\n    return return_info",
    "narration": "get_info takes a ticker symbol and tries to produce a single, flattened set of fundamental fields for that symbol by first obtaining a yfinance Ticker wrapper via retry_ticker (the retry logic you saw earlier). If retry_ticker fails the function prints a download-failure message and returns early. Once it has the Ticker, it pulls the basic info mapping into a pandas structure so there is always a minimal info payload to return. It then attempts to read three standard financial statements from the Ticker—balance sheet, income/financials, and cashflow—and guards against two common failure modes: if the statements access raises an IndexError (e.g., empty tables or unexpected indexing) it logs the exception via the project logging wrapper we reviewed previously (log.log / Strategy.log) and returns the basic info; if the statements exist but are not table-like objects that support column selection it catches AttributeError, logs and again returns the basic info. On the happy path it selects the most recent column from each of the three statement tables, concatenates those Series vertically into one combined Series of fields, prints the ticker for realtime feedback, and returns that combined fundamentals Series. Conceptually get_info is the synchronous worker that download_info delegates to for async execution and is analogous to all_info except that get_info pulls live per-ticker statements from the remote yfinance wrapper and flattens the current-period columns, whereas all_info assembles data from local files or prebuilt loaders."
  },
  {
    "create-file": "backtest/util/universe.py"
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import os\nimport pandas as pd",
    "narration": "os is brought in so the Universe code can interact with the local filesystem to locate and open the CSV files that define each basket: building paths, checking existence, and joining directory names so Universe._read_csv can reliably find symbol lists no matter where the process runs. pandas is used to parse and manipulate those CSVs into structured in-memory tables so the Universe constructor and _read_csv can normalize columns, drop or dedupe rows, select the ticker column, and return a clean list or Series of symbols that get and the rest of the pipeline will consume. Compared with the other module that also imported pandas_datareader and datetime to handle network fetches and date ranges, the imports here are intentionally minimal because Universe is a local, file-based catalog layer rather than a networked data adapter; earlier we saw the IEX adapter use a config_path and JSON-parsed tokens for authenticated HTTP calls, whereas Universe relies on os and pandas to expose stable, normalized universes to the backtest engine."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 12,
        "character": 28
      }
    },
    "code": "class Universe:\n    def __init__(self, assets=None):\n        ...\n\n    def _read_csv(self, path):\n        ...",
    "narration": "Universe is a lightweight container abstraction that represents a named basket of tickers strategies and tooling can request when building a run. Its constructor, __init__, accepts an optional assets argument and stores whatever list is provided on the instance so callers always have a consistent place to read the symbol list from. The helper method, _read_csv, performs the work of turning a packaged CSV into that list: it resolves a file path relative to the module, parses the CSV with pandas (expecting the tickers in the CSV column used by the project), and returns a plain Python list of ticker strings for the caller to assign to the instance. Concrete universes like SP500, FAANG, SP500_TECH and SP100 subclass Universe and use _read_csv during their initialization to populate self.assets with the predefined baskets. Those asset lists then flow into the data layer — for example, the IEX adapter’s dailyHistorical that we discussed earlier — which iterates the tickers to fetch and normalize historical bars, so the strategy and backtesting engine receive a consistent universe of symbols. The control flow is straightforward: __init__ stores whatever assets the subclass or caller supplies; _read_csv handles file resolution and parsing and returns the canonical list that the subclass assigns to the instance."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 6,
        "character": 4
      },
      "end": {
        "line": 7,
        "character": 28
      }
    },
    "code": "    def __init__(self, assets=None):\n        self.assets = assets",
    "narration": "Universe.__init__ accepts an optional assets argument and stores it on the instance as the assets attribute. This simple assignment is the mechanism by which a Universe instance carries its ticker list forward so that methods like _read_csv and get can expose a concrete basket to the rest of the framework; concrete universes such as FAANG build a tickers list and call the Universe initializer to populate that attribute. At runtime the assets attribute therefore becomes the canonical source of tickers that tooling and the data layer (for example the dailyHistorical adapter you already saw) will use to fetch and prepare the time series consumed by strategies."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 9,
        "character": 4
      },
      "end": {
        "line": 12,
        "character": 28
      }
    },
    "code": "    def _read_csv(self, path):\n        ticker_csv_path = os.path.join(os.path.dirname(__file__), path)\n        tickers = pd.read_csv(ticker_csv_path, header=None)[1]\n        return list(tickers)",
    "narration": "Universe._read_csv resolves the relative path it receives against the module file's directory, opens that CSV with pandas while treating the file as headerless, and extracts the second column as the ticker column before returning those values as a plain Python list. SP500.__init__, SP100.__init__, and SP500_TECH.__init__ call _read_csv twice to pull a master list and an exclusion list, perform the exclusion filtering, and then hand the resulting list into the Universe constructor so the backtest has a consistent set of symbols to operate on. Because the function anchors paths to the module directory and returns a simple list, it centralizes how predefined basket files are located and parsed so the rest of the data pipeline (for example, the adapter that dailyHistorical uses to fetch per-symbol bars) receives a stable, predictable universe to iterate over. The routine is straightforward with no branching: it expects the file to exist, expects tickers to live in the second CSV column, and passes the resulting list back to the callers."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 15,
        "character": 0
      },
      "end": {
        "line": 20,
        "character": 33
      }
    },
    "code": "class SP500(Universe):\n    def __init__(self):\n        ...",
    "narration": "SP500 is a concrete Universe subclass that builds the canonical S&P 500 ticker set the rest of the backtest framework consumes. On construction it reads a canonical tickers CSV via the Universe._read_csv helper, then reads a second CSV of tickers to exclude via the same helper, removes any excluded symbols from the tickers list, and then delegates to the Universe constructor to register the resulting asset list. Conceptually this implements a simple supply-and-filter pipeline: the source CSV supplies the universe candidates, the exclude CSV applies a cleanup rule (for delisted or otherwise undesirable symbols), and the final list becomes the unified asset set that strategies and data adapters (for example the IEX-backed dailyHistorical we reviewed earlier) will use to fetch price series and drive backtests. The pattern is the same one SP500_TECH and SP100 follow—each subclass sources a different predefined list and funnels it through the same read/filter/initialize flow—whereas FAANG represents a smaller, explicit subset following the same Universe initialization pattern."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 16,
        "character": 4
      },
      "end": {
        "line": 20,
        "character": 33
      }
    },
    "code": "    def __init__(self):\n        tickers = self._read_csv('../../data/spy/tickers.csv')\n        exclude = self._read_csv('../../data/info/exclude.csv')\n        tickers = [t for t in tickers if t not in exclude]\n        super().__init__(tickers)",
    "narration": "SP500.__init__ builds the concrete S&P 500 basket that the framework exposes to strategies by first delegating to Universe._read_csv to load the canonical ticker list from the on-disk CSV, then calling Universe._read_csv again to load a separate exclusion list, removing any tickers that appear in that exclusion set, and finally handing the filtered ticker list up to Universe.__init__ so it becomes the instance's assets attribute. In terms of data flow, the CSV rows are resolved and parsed by the shared _read_csv helper into a plain Python list, that list is filtered in-memory to drop excluded symbols, and the resulting list is passed into the Universe container for downstream use by backtests and tooling. Control flow is straightforward: the happy path results in a cleaned, canonical universe; the function follows the same read–filter–delegate pattern used by SP100.__init__ and SP500_TECH.__init__, while FAANG"
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 23,
        "character": 0
      },
      "end": {
        "line": 26,
        "character": 33
      }
    },
    "code": "class FAANG(Universe):\n    def __init__(self):\n        ...",
    "narration": "FAANG is a concrete Universe subclass that provides a ready-made, named basket of the five high-profile consumer tech names (Facebook/Meta, Apple, Amazon, Netflix, and Google) for use in backtests and tooling. Its constructor builds that small, fixed list and delegates to Universe.__init__ so the assets attribute is populated the same way as other universes; as a result, any part of the framework that asks for the FAANG universe gets an instance whose assets list flows into the data-loading and strategy layers exactly like SP500 or SP100. Unlike SP500, which sources its tickers from a CSV via Universe._read_csv, FAANG defines its universe inline for convenience and determinism, and the UNIVERSE_DICT maps the string name 'faang' to the FAANG class so callers can request it by name."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 24,
        "character": 4
      },
      "end": {
        "line": 26,
        "character": 33
      }
    },
    "code": "    def __init__(self):\n        tickers = ['FB', 'AAPL', 'AMZN', 'NFLX', 'GOOG']\n        super().__init__(tickers)",
    "narration": "FAANG.__init__ constructs the predefined FAANG universe by specifying the five core mega-cap tech names (Facebook/Meta, Apple, Amazon, Netflix and Google) and then delegates to Universe.__init__ so that those symbols become the instance's assets list available to strategies and tooling. This follows the same concrete-Universe subclass"
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 29,
        "character": 0
      },
      "end": {
        "line": 34,
        "character": 33
      }
    },
    "code": "class SP500_TECH(Universe):\n    def __init__(self):\n        ...",
    "narration": "SP500_TECH is a Universe subclass whose initializer builds a curated technology-focused S&P 500 basket by calling Universe._read_csv to load the tech symbol list and to load a separate exclusion list, applying a filter that removes any tickers present in the exclusion list, and then delegating to Universe.__init__ with the filtered list so the assets attribute holds the final set. This follows the same predefined-universe pattern used by SP500, SP100, and FAANG but adds the explicit exclusion step so the framework hands strategies and tooling a validated, reproducible tech subset that aligns with the project’s local data and validation pipeline; the data flows from the two CSV sources through the filter into the Universe container, and the primary control decision is the filter that drops excluded symbols before the instance is initialized for downstream backtests and reporting."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 30,
        "character": 4
      },
      "end": {
        "line": 34,
        "character": 33
      }
    },
    "code": "    def __init__(self):\n        tickers = self._read_csv('../../data/spy/sp500-tech.csv')\n        exclude = self._read_csv('../../data/info/exclude.csv')\n        tickers = [t for t in tickers if t not in exclude]\n        super().__init__(tickers)",
    "narration": "SP500_TECH.__init__ builds the technology-focused universe used by backtests by loading a predefined CSV of tech-weighted S&P tickers and then removing any symbols listed in the shared exclude file before handing the cleaned list to the Universe constructor. It calls Universe._read_csv to resolve and read the tech CSV into a plain list of tickers, calls Universe._read_csv again to load the exclude list, filters the tech tickers by removing any matches found in the exclude list, and then delegates to Universe.__init__ (via super) so the resulting assets list is stored on the instance. Functionally this creates a reproducible, filesystem-backed asset set that strategies and tooling can request; the exclude step enforces a centralized place for removing bad or delisted symbols. The flow and intent mirror SP500.__init__ and SP100.__init__, differing only in which CSV is read, so the project gets a consistent pattern for defining concrete universes."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 37,
        "character": 0
      },
      "end": {
        "line": 42,
        "character": 33
      }
    },
    "code": "class SP100(Universe):\n    def __init__(self):\n        ...",
    "narration": "SP100 is a concrete Universe subclass that, on instantiation, uses Universe._read_csv to load the project's S&P 100 symbol list and then loads a separate exclusion list; it filters the S&P 100 tickers by removing any entries found in the exclusion list and passes the resulting, cleaned ticker list into Universe.__init__ so the instance's assets attribute becomes the vetted S&P 100 universe the rest of the backtest framework can request. This implements the same CSV-backed-universe pattern used by SP500, SP500_TECH, and FAANG, but SP100 differs by applying an explicit exclusion step to ensure delisted or otherwise unwanted symbols are removed before strategies or tooling consume the universe."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 38,
        "character": 4
      },
      "end": {
        "line": 42,
        "character": 33
      }
    },
    "code": "    def __init__(self):\n        tickers = self._read_csv('../../data/spy/sp100.csv')\n        exclude = self._read_csv('../../data/info/exclude.csv')\n        tickers = [t for t in tickers if t not in exclude]\n        super().__init__(tickers)",
    "narration": "SP100.__init__ builds the concrete S&P 100 universe used by the backtest framework by loading the canonical symbol list and applying the project-wide exclusion list before handing the result to the Universe container. Concretely, it uses Universe._read_csv to load the S&P 100 CSV (relying on _read_csv’s module-relative path resolution) and again to load the exclude CSV from the info folder; it then filters the S&P 100 tickers to remove any symbols present in the exclude list, and finally calls Universe.__init__ with the filtered list so the instance’s assets attribute contains the cleaned S&P 100 basket. This follows the same subclassing/provider pattern used by SP500.__init__ and SP500_TECH.__init__, centralizing symbol sourcing and exclusion so strategies and tooling elsewhere in the pipeline always receive a consistent, prefiltered universe."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 45,
        "character": 0
      },
      "end": {
        "line": 46,
        "character": 34
      }
    },
    "code": "def get(universe):\n    return UNIVERSE_DICT[universe]",
    "narration": "get is the tiny, canonical accessor that strategies and tooling use to retrieve a named universe from the file's registry: it accepts a universe identifier and returns whatever is stored under that key in UNIVERSE_DICT. UNIVERSE_DICT maps string names to the concrete Universe entries you saw earlier — SP500, FAANG, SP500_TECH, and SP100 — so callers receive the registered Universe value (typically a Universe subclass) and can instantiate or use it according to the framework's expectations. Conceptually this implements a simple registry/lookup pattern that centralizes the predefined baskets so the rest of the backtester can request standard universes by name rather than importing individual classes or CSV paths; the actual CSV loading and asset construction remain the responsibility of the Universe subclasses you looked at before, such as SP500 and the file-reading logic in Universe._read_csv. The function is a direct dictionary lookup with no branching, validation, or side effects beyond returning the mapped value."
  },
  {
    "open-file": "backtest/util/universe.py",
    "range": {
      "start": {
        "line": 49,
        "character": 0
      },
      "end": {
        "line": 54,
        "character": 1
      }
    },
    "code": "UNIVERSE_DICT = {\n    'sp500': SP500,\n    'faang': FAANG,\n    'sp500_tech': SP500_TECH,\n    'sp100': SP100\n}",
    "narration": "UNIVERSE_DICT is a small registry that ties the canonical string names used throughout the framework to their concrete Universe subclasses, mapping the names sp500, faang, sp500_tech and sp100 to the SP500, FAANG, SP500_TECH and SP100 classes respectively. This registry is what the get helper looks up when external code or a strategy asks for a named universe, letting callers pass a human-friendly key and receive the corresponding Universe class to instantiate or work with. Functionally it centralizes the available predefined baskets so configuration strings decouple from concrete implementations, and because it is a plain dict a missing key will surface as a lookup error at call time."
  },
  {
    "open-file": "backtest/algos/BaseStrategy.py",
    "range": {
      "start": {
        "line": 2,
        "character": 0
      },
      "end": {
        "line": 51,
        "character": 25
      }
    },
    "code": "class Strategy(bt.Strategy):\n    params = {\n        'riskfreerate': 0.035,\n        'cheat_on_open': False,\n        'verbose': False\n    }\n    def __init__(self, kwargs=None):\n        ...\n    def log(self, txt, date=None):\n        ...\n    def notify_order(self, order):\n        ...",
    "narration": "Strategy is the common Backtrader strategy base that the concrete trading classes inherit from so they can focus solely on trading logic while reusing initialization, logging, and order-handling boilerplate. It exposes three standard pieces: a small params set that provides a riskfreerate, a cheat_on_open flag, and a verbose flag so subclasses can access common run-level configuration; an initializer that calls the Backtrader base initializer, creates and zeroes out order-tracking attributes (an outstanding order handle, the last buy price and commission, and a boolean that marks if an order was rejected), and takes the params.verbose value onto the instance so logging can be controlled centrally; and a log helper that stamps messages with the current date and only emits output when verbose is enabled, enabling consistent human-readable tracing across strategies. notify_order is the endpoint where broker/order state is reconciled: it early-returns for transient Submitted/Accepted states, treats Completed orders as the happy path (distinguishing buys from sells, recording executed price/commission and emitting a log entry), and groups Canceled/Margin/Rejected into a rejection path where it translates the status into a reason string, logs the failed side and symbol plus current cash versus the attempted order cost, and sets the order_rejected flag. At the end of notify_order the strategy clears the outstanding order handle so future order placement logic in subclasses knows it can submit new orders. Concrete strategies such as BuyAndHold, CrossOver, and EqualVolatility build on these attributes and behaviors (CrossOver explicitly invokes the base initializer) so they receive a uniform place to read/write order state, rely on consistent logging, and get standardized error/rejection handling from the engine; notify_order is where Backtrader pushes execution events into the strategy’s state-machine and Strategy centralizes the common control-flow for those events."
  },
  {
    "open-file": "backtest/algos/BaseStrategy.py",
    "range": {
      "start": {
        "line": 3,
        "character": 4
      },
      "end": {
        "line": 7,
        "character": 5
      }
    },
    "code": "    params = {\n        'riskfreerate': 0.035,\n        'cheat_on_open': False,\n        'verbose': False\n    }",
    "narration": "The params attribute establishes the Strategy base class's default configuration values that the rest of the backtest machinery reads at instantiation and uses throughout the run. riskfreerate sets the default risk-free interest used by the base class's performance/returns and Hurst-related logging routines so those metrics have a consistent baseline across concrete strategies. cheat_on_open is the default flag that controls Backtrader's intrabar execution semantics for orders, so the base class and its order-handling logic operate under a known execution assumption unless a subclass or the backtest run explicitly overrides it. verbose is the global logging verbosity toggle that the base class's log method and related notifications consult to decide whether to emit runtime diagnostics. Compared with the larger, strategy-specific params sets seen elsewhere, these three entries are deliberately minimal: they provide cross-cutting, framework-level defaults that follow the Template Method pattern (the base Strategy supplies common hooks and defaults, and concrete strategies like BuyAndHold, CrossOver, or EqualVolatility extend or override them with domain-specific knobs such as target_percent, lookback, or order_frequency). These defaults therefore drive initialization behavior, runtime logging, and order execution semantics consistently across the concrete strategies that inherit from Strategy."
  },
  {
    "open-file": "backtest/algos/BaseStrategy.py",
    "range": {
      "start": {
        "line": 19,
        "character": 4
      },
      "end": {
        "line": 51,
        "character": 25
      }
    },
    "code": "    def notify_order(self, order):\n        if order.status in [order.Submitted, order.Accepted]:\n            return\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.log('BUY {}\\t{:.2f}\\t  Cost: {:.2f}\\tComm: {:.2f}'.format(\n                    order.data._name,\n                    order.executed.price,\n                    order.executed.value,\n                    order.executed.comm))\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n            if order.issell():\n                self.log('SELL {}\\t{:.2f}\\t  Cost: {:.2f}\\tComm: {:.2f}'.format(\n                    order.data._name,\n                    order.executed.price,\n                    order.executed.value,\n                    order.executed.comm))\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            status_reason = {\n                order.Canceled: 'Canceled',\n                order.Margin: 'Margin Called',\n                order.Rejected: 'Rejected'\n            }\n            self.log('Order {}: {} {}'.format(\n                status_reason[order.status],\n                'BUY' if order.isbuy() else 'SELL',\n                order.data._name\n            ))\n            self.log('Cash: {:.2f}, Order: {:.2f}'.format(self.broker.get_cash(),\n                                                          (order.price or 0) * (order.size or 0)))\n            self.order_rejected = True\n        self.order = None",
    "narration": "Strategy.notify_order is the centralized Backtrader callback that normalizes how the system handles order lifecycle events so concrete strategies like BuyAndHold, CrossOver, and EqualVolatility don't each reimplement fill and error handling. Backtrader calls notify_order with an order object and the method first filters out transient states by returning early when the order is merely submitted or accepted, preserving the pending order state set in Strategy.__init__. When the order reaches a completed state the method distinguishes buys from sells: it delegates human-readable reporting to Strategy.log (which prints when verbose is enabled) using the data feed's name and the executed price, value and commission, and it records the executed buy price and commission on the Strategy instance for later reference. If the order is canceled, rejected, or fails due to margin, notify_order maps the numeric status to a descriptive reason, logs the rejection with whether it was a buy or sell and the asset name, logs current cash versus the notional the order represented (safely handling missing price/size), and flips the instance-level order_rejected flag so next() implementations can react (for example, CrossOver.next checks order_rejected to decide whether to retry a buy). Finally, regardless of path, notify_order clears the instance's order reference to indicate there is no longer an outstanding order. Overall, notify_order implements the callback/observer pattern Backtrader expects and provides a small state machine and unified logging surface that keeps higher-level strategy code focused on trading decisions."
  },
  {
    "open-file": "tools/std.py",
    "range": {
      "start": {
        "line": 13,
        "character": 0
      },
      "end": {
        "line": 21,
        "character": 19
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('ticker', nargs=1)\n    PARSER.add_argument('--length', nargs=1, type=int)\n    PARSER.add_argument('-r', '--usereturns', action=\"store_true\")\n    ARGS = PARSER.parse_args()\n    ARG_ITEMS = vars(ARGS)\n    STD_ARGS = {k: (v[0] if isinstance(v, list) else v) for k, v in ARG_ITEMS.items() if v is not None}\n    std(**STD_ARGS)\n",
    "narration": "When this module is run directly the module-level guard creates an ArgumentParser to expose the std utility as a small CLI: it defines a positional ticker argument, an optional length integer, and a boolean flag for usereturns, then parses the process arguments into ARGS. The parsed namespace is converted into a plain dictionary via vars and then normalized into STD_ARGS by taking the first element when any parsed value is a single-item list (because the parser used nargs=1 for some arguments) and by dropping keys whose values are None so only explicitly provided parameters are forwarded. Finally the script calls std with that cleaned keyword-argument mapping so the same std function you already examined is invoked with scalar values (ticker string, optional length int, and a True/False usereturns) and prints the volatility diagnostics. The main-guard ensures this CLI wiring runs only when the file is executed as a script and not when the utilities are imported elsewhere, and the list-flattening pattern used here mirrors other small entrypoint scripts in the project that expose utilities via argparse before delegating to the core helper functions."
  },
  {
    "create-file": "tools/download_prices.py"
  },
  {
    "open-file": "tools/download_prices.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 6,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nfrom datetime import date\nimport pandas as pd\nfrom api import yahoo",
    "narration": "As a standalone utility that populates the local price cache so backtests can reuse up-to-date market data, the file brings in a few small, focused dependencies. os is used for filesystem interactions—checking or creating cache directories and building paths where downloaded CSVs or parquet files are persisted. argparse enables the script to be driven from the command line so callers can specify tickers, universes, date ranges, or update modes without touching code. The date symbol from datetime supplies a simple, calendar-aware type for default start/end values and for any date arithmetic the script performs when deciding what to request from the data provider. pandas is pulled in as the tabular-data workhorse for reading, validating, concatenating, and writing the price series that become the local cache. Finally, importing yahoo from the api package hands the script the project’s provider wrapper for fetching market data (the get_daily / get_daily_async entry points live there), so the utility interacts with the unified data layer rather than calling yfinance or HTTP directly. Compared with similar modules elsewhere in the project, this file keeps the imports minimal: other scripts add datetime.datetime/timedelta helpers, async tooling, direct yfinance usage, or the tools.log logger, but this utility intentionally relies on the project’s api.yahoo wrapper plus pandas and OS primitives to perform focused download-and-persist work."
  },
  {
    "open-file": "tools/download_prices.py",
    "range": {
      "start": {
        "line": 9,
        "character": 0
      },
      "end": {
        "line": 11,
        "character": 114
      }
    },
    "code": "def save_ticker(ticker):\n    historical = yahoo.get_daily(ticker)\n    historical.to_csv(os.path.join(os.path.dirname(__file__), '../data/price/{ticker}.csv'.format(ticker=ticker)))",
    "narration": "save_ticker is the simple entry-point helper that accepts a ticker symbol, pulls the full daily history for that symbol via yahoo.get_daily, and then persists the resulting DataFrame as a CSV into the project's local price cache directory so backtests can read a stable, on-disk copy of the time series. The function performs no branching or incremental logic: it performs a network fetch (get_daily ultimately using yfinance) and writes the output to disk, effectively overwriting or creating the per-ticker price file. Conceptually it is the single-ticker counterpart to save_all, which batches symbols and uses get_daily_async while trimming leading empty rows before saving each CSV, and to update_ticker, which instead inspects an existing file to determine the last stored date and fetches/appends only newer rows; SpyTickers.download plays a similar cache-oriented role for symbol lists rather than prices. save_ticker’s place in the pipeline is therefore straightforward: populate or refresh one instrument’s historical price CSV so the rest of the backtesting framework can reuse that cached data."
  },
  {
    "open-file": "tools/download_prices.py",
    "range": {
      "start": {
        "line": 14,
        "character": 0
      },
      "end": {
        "line": 24,
        "character": 68
      }
    },
    "code": "def save_all(tickers):\n    group_size = 10\n    for i in range(0, len(tickers), group_size):\n        ticker_group = list(tickers)[i: i + group_size]\n        print(ticker_group)\n        historical = yahoo.get_daily_async(ticker_group)\n        for ticker in ticker_group:\n            first_valid = historical[ticker][historical[ticker].notnull().any(axis=1)].index[0]\n            historical[ticker].loc[first_valid:].to_csv(os.path.join(\n                os.path.dirname(__file__),\n                '../data/price/{ticker}.csv'.format(ticker=ticker)))",
    "narration": "save_all is the entry-point utility that seeds or refreshes the framework's local daily price cache so backtests can reuse up-to-date market data. It accepts a collection of tickers, chops that collection into batches of ten to avoid overly large provider requests, and prints each batch as it is processed so you can follow progress on the console. For each batch it asks yahoo.get_daily_async to download the group history in one call — get_daily_async returns a grouped dataframe structure from yfinance — and then iterates ticker-by-ticker within the returned group. For each ticker it locates the first row that contains any non-null field (this drops the leading all-NaN rows yfinance can produce for symbols with different listing dates), slices the dataframe from that first-valid date through the end, and writes the result out as a CSV into the project's price cache under the data/price directory so subsequent backtests load a clean, trimmed history. The batching pattern here is the main difference from save_ticker, which downloads and saves a single ticker synchronously, and from update_all, which loops per ticker and delegates to"
  },
  {
    "open-file": "tools/download_prices.py",
    "range": {
      "start": {
        "line": 27,
        "character": 0
      },
      "end": {
        "line": 30,
        "character": 30
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('-t', '--ticker', nargs='+')\n    ARGS = PARSER.parse_args()\n",
    "narration": "When run as a standalone script, the module's entry point creates an argparse.ArgumentParser object (stored in PARSER), registers a ticker command-line option that is expected to receive one or more symbol strings, and then parses the incoming command-line into ARGS so the rest of the script can read ARGS.ticker. This makes the utility interactive from the shell: a user supplies one or many tickers and the parsed list is handed to the downstream logic that updates the local price cache — for example calling the single-ticker fetch path for a one-off symbol or the bulk path represented by save_all which in turn uses get_daily_async to fetch groups of tickers. The pattern is consistent with the module's imports that include argparse and with save_all's expectation of an iterable of tickers; the use of the one-or-more argument form ensures the script always receives at least one symbol to operate on."
  },
  {
    "open-file": "tools/download_prices.py",
    "range": {
      "start": {
        "line": 32,
        "character": 0
      },
      "end": {
        "line": 39,
        "character": 62
      }
    },
    "code": "    if ARGS.ticker:\n        if len(ARGS.ticker) > 1:\n            save_all(ARGS.ticker)\n        else:\n            save_ticker(ARGS.ticker[0])\n    else:\n        TICKER_CSV_PATH = os.path.join(os.path.dirname(__file__), '../data/spy/tickers.csv')\n        TICKERS = pd.read_csv(TICKER_CSV_PATH, header=None)[1]\n",
    "narration": "The conditional decides whether the utility should refresh specific tickers passed in via ARGS or fall back to the project's canonical ticker list: if ARGS.ticker is present the code checks how many tickers were supplied and routes to either save_all when multiple tickers are provided or save_ticker for a single ticker, so callers can request bulk asynchronous updates or a synchronous single-symbol fetch respectively (save_all groups requests and uses the async daily fetch path; save_ticker uses the single-ticker daily fetch). If no ARGS.ticker is provided the code constructs TICKER_CSV_PATH to point at the local tickers CSV used by the project, reads that CSV without a header, and extracts the second column into TICKERS so the script has a default universe to operate on for subsequent bulk updates. This branching makes the utility flexible: it accepts explicit ticker input for targeted refreshes or derives the list of symbols from the persisted tickers CSV when invoked without arguments, feeding either save_ticker or save_all which persist the downloaded daily price data into the local price cache used by the backtests."
  },
  {
    "open-file": "tools/download_prices.py",
    "range": {
      "start": {
        "line": 41,
        "character": 0
      },
      "end": {
        "line": 43,
        "character": 26
      }
    },
    "code": "        save_all(TICKERS)\n        save_ticker('SPY')\n        save_ticker('RSP')\n",
    "narration": "save_all invoked with TICKERS kicks off a bulk refresh of the local daily-price cache for the entire ticker list produced elsewhere (SpyTickers.download is the place that populates that list), using the data layer's bulk daily-fetch path so the project's historical-price store is populated or updated in one pass; the subsequent save_ticker calls for SPY and RSP perform single-ticker fetch-and-persist operations against the same data layer (the single-ticker get_daily path), ensuring those two market-proxy ETFs have their own up-to-date series in the cache even if they are not part of the bulk set or need an immediate refresh. Together these calls drive the data flow from external providers into the local cache that backtests read: the happy path fetches and writes fresh daily prices, while any skipping, incremental update, or error handling is handled inside the data-layer helpers save_all and save_ticker."
  },
  {
    "create-file": "tools/update_prices.py"
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 6,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nfrom datetime import date, datetime, timedelta\nfrom api import yahoo\nfrom tools.log import log",
    "narration": "The file pulls in a small set of utilities needed to drive an incremental price refresh workflow: os is used for filesystem and path operations so the updater can find and inspect the local cached price files; argparse supplies the command-line plumbing so the script can be invoked with options (for example to target a single ticker, a universe, or a date range); date, datetime, and timedelta are imported from the datetime module so the updater can read the last-stored date, compute the start/end dates for missing bars, and timestamp progress messages; api.yahoo provides the project’s Yahoo data layer wrapper that the updater uses to fetch the new daily bars rather than calling yfinance directly; and tools.log.log is the project’s consistent logging helper used to record progress, errors, and summaries during the batch update. Compared with similar import lists elsewhere in the codebase, this file intentionally omits pandas and direct yfinance usage because it relies on the api.yahoo wrapper to return data in the format the caching layer expects; other scripts that do in-memory analysis bring pandas in, and some simpler utilities import only date or datetime or include time for sleep/rate-limiting."
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 9,
        "character": 0
      },
      "end": {
        "line": 15,
        "character": 67
      }
    },
    "code": "def get_last_date(file_path):\n    with open(file_path, 'rb') as f:\n        f.seek(-2, os.SEEK_END)\n        while f.read(1) != b'\\n':\n            f.seek(-2, os.SEEK_CUR)\n        last_line = f.readline().decode()\n        return datetime.strptime(last_line[:10], '%Y-%m-%d').date()",
    "narration": "get_last_date opens the stored daily-price CSV in binary mode and reads only the file tail to extract the date stamped on the final row. It moves the file cursor backward from the end until it finds the last newline, then reads and decodes that final line, expects the date to occupy the first ten characters in ISO form (YYYY-MM-DD), parses that substring into a date object, and returns it. The function is used by update_ticker to determine the most recent locally cached bar so the updater can decide whether the file is already current (compare against yesterday) or supply a start anchor when calling the data layer to fetch only newer rows; update_ticker handles any OSError or parsing ValueError raised here and logs accordingly. The implementation reads only the tail of the file for efficiency on large histories, and it follows the same backward-seek pattern used by the last helper, which also retrieves the final line but prints it instead of returning a parsed date."
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 18,
        "character": 0
      },
      "end": {
        "line": 39,
        "character": 36
      }
    },
    "code": "def update_ticker(ticker):\n    file_path = os.path.join(os.path.dirname(__file__), '../data/price/{ticker}.csv'.format(ticker=ticker))\n    last_date = None\n    try:\n        last_date = get_last_date(file_path)\n    except OSError as e:\n        print('!!! Read error.')\n        log.log(type(e).__name__, e)\n    except ValueError as e:\n        print('!!! Invalid date format.')\n        log.log(type(e).__name__, e)\n\n    if last_date == date.today() - timedelta(days=1):\n        return\n\n    if last_date is not None:\n        historical = yahoo.get_daily(ticker, last_date)\n        historical = historical[historical.index > last_date.strftime('%Y-%m-%d')]\n        historical.to_csv(file_path, mode='a', header=False)\n    else:\n        historical = yahoo.get_daily(ticker, last_date)\n        historical.to_csv(file_path)",
    "narration": "update_ticker is the per-symbol updater that keeps the local daily-price cache current for the backtesting pipeline: it computes the path to the ticker's CSV under the project's data/price folder, asks get_last_date to discover the most recent stored bar, and then either skips, appends, or writes a full series depending on what it learns. If get_last_date raises an OSError or ValueError the function prints a short message and records the exception through the project's log.log helper and continues; if the last stored date equals yesterday it returns immediately to avoid needless network calls. When a valid last_date exists it uses yahoo.get_daily with that date to fetch bars up to today, filters out any rows that are not strictly newer than the stored last_date so duplicates are avoided, and appends only the new rows to the existing CSV (no header). If no last_date is found it treats the ticker as missing and requests the full history from yahoo.get_daily and writes that out as a new CSV. In the broader workflow update_ticker is invoked by update_all for batch refreshes and complements save_ticker's always-write behavior by providing the incremental path used to keep datasets small and current for backtests."
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 42,
        "character": 0
      },
      "end": {
        "line": 45,
        "character": 29
      }
    },
    "code": "def update_all(tickers):\n    for i, ticker in enumerate(tickers):\n        print(i, '-', ticker)\n        update_ticker(ticker)",
    "narration": "update_all takes a collection of tickers and walks them one by one; for each symbol it emits a simple progress line that shows the loop index and the ticker, then hands control to update_ticker to do the actual work. The data flow is straightforward: the tickers list comes from the command-line or the project’s canonical universe, update_ticker inspects the local price CSV (using get_last_date), computes the missing-range, asks the Yahoo data layer for new daily bars (via get_daily), and either appends the new rows or writes a fresh file, while any informational lines are funneled through the project logger and verbose Strategy.log when applicable. The control flow is a single enumerated loop with the happy path delegating to update_ticker for each symbol and with error handling and file I/O performed inside update_ticker, so update_all itself remains a thin dispatcher that provides progress visibility. Compared with save_all, which batches symbols and uses the bulk async fetch path, update_all processes symbols synchronously and ensures each ticker’s cache file is individually validated and updated. The visible side effects are console output for progress and logfile entries plus the CSV updates under the project’s price cache so the backtesting engine can rely on up-to-date daily data."
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 48,
        "character": 0
      },
      "end": {
        "line": 51,
        "character": 30
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('-t', '--ticker', nargs='+')\n    ARGS = PARSER.parse_args()\n",
    "narration": "As noted earlier, the module-level guard prepares the command-line entry point by instantiating an ArgumentParser stored in PARSER, registering a ticker option that can accept one or more symbols, and parsing the incoming arguments into ARGS so the rest of the script can read ARGS.ticker. That parsed value is what the control flow described previously uses to decide whether to call save_all for a bulk refresh or save_ticker for a single-symbol update, so the CLI maps directly into the incremental refresh workflow that keeps the local daily-price cache current. This pattern mirrors how update_all and update_ticker operate at runtime — the parser hands a list or single symbol to the updater logic, and those updater functions then iterate and persist new bars via the yahoo data layer while logging progress."
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 53,
        "character": 0
      },
      "end": {
        "line": 58,
        "character": 91
      }
    },
    "code": "    if ARGS.ticker:\n        update_all(ARGS.ticker)\n    else:\n        DATA_PATH = os.path.join(os.path.dirname(__file__), '../data/price/')\n        FILE_LIST = os.listdir(DATA_PATH)\n        TICKERS = [f[:-4] for f in FILE_LIST if os.path.isfile(os.path.join(DATA_PATH, f))]\n",
    "narration": "If ARGS.ticker is present the script hands that list straight to update_all so the caller-provided symbols are refreshed; otherwise the code builds a default universe by pointing DATA_PATH at the repository’s price cache directory, enumerating the entries there, and producing TICKERS by keeping only filesystem entries (not subdirectories) and turning each filename into a symbol by removing the four-character file extension (the existing CSVs in the price store). That TICKERS list is the local-cache-derived universe the rest of the updater will operate on. In practice this branch lets the updater either honor an explicit CLI target via update_all (which you’ve already seen iterates and calls update_ticker for each symbol) or fall back to refreshing every symbol that already has a CSV in the data/price store."
  },
  {
    "open-file": "tools/update_prices.py",
    "range": {
      "start": {
        "line": 60,
        "character": 0
      },
      "end": {
        "line": 60,
        "character": 27
      }
    },
    "code": "        update_all(TICKERS)\n",
    "narration": "Calling update_all with TICKERS triggers a synchronous, per-symbol refresh loop that walks the canonical ticker list and delegates each symbol to the single-ticker updater. update_all enumerates the tickers so the script can report progress (index and symbol) as it goes, and for every entry it invokes update_ticker, which is responsible for inspecting the local CSV to discover the last stored date, handling read errors, deciding whether anything needs to be fetched (it short-circuits if the file is already current), and then using the api.yahoo daily-fetch path to retrieve and either append new rows or create the file. In the context of the framework, this call implements the incremental, file-by-file refresh path that complements the bulk async path used elsewhere by save_all: update_all drives the serial, single-ticker fetch/append workflow so the local daily-price cache is brought forward ticker by ticker while emitting simple progress output and letting update_ticker handle the file and fetch logic."
  },
  {
    "create-file": "tools/fin_calc.py"
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 4,
        "character": 0
      }
    },
    "code": "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nfrom datetime import datetime, timedelta",
    "narration": "The file pulls in a compact set of numeric and time utilities that match its role as the centralized performance and risk calculator: numpy is used for fast vectorized math and NaN-aware numeric operations that underpin log and N‑day return calculations, pandas provides Series/DataFrame handling and time-index alignment needed to compute rolling returns, aggregate windows, and to join asset and benchmark series for beta/alpha; scipy.stats (aliased as stats) supplies the statistical routines and distribution helpers used for significance testing, regression-like correlation and slope estimates, and tail‑risk functions that VaR and CVaR rely on; and datetime’s datetime and timedelta are used for precise date arithmetic when the functions need to build lookback windows or convert N‑day windows into calendar ranges. This is a slimmer import set than some other modules that bring in more of scipy’s distributions, statsmodels, or plotting libraries; fin_calc deliberately focuses on the core numeric, statistical, and date primitives so its functions remain a lightweight, reusable backend that other parts of the system (including Strategy.log hooks and reporting utilities you’ve seen elsewhere) can call without dragging in heavy dependencies."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 6,
        "character": 39
      }
    },
    "code": "def log_returns(data):\n    return np.log(data).diff().iloc[1:]",
    "narration": "log_returns converts an incoming price series or DataFrame of prices into per-period continuously compounded returns by taking the natural logarithm of the price values, differencing that log series to produce the period log-return, and dropping the initial undefined entry so the returned series begins at the first valid return. This produces the canonical, additive return series that downstream tooling expects: get_returns calls fin_calc.log_returns to obtain the time series used for reporting total and annualized performance, risk routines and the various strategy helpers and rebalancers that otherwise repeatedly compute the same log-diff pattern inline. The function intentionally returns raw period log returns (it does not set a first-value baseline or convert to cumulative returns as _log_returns does), leaving aggregation, annualization or tail-risk calculations to callers and ensuring a single, consistent implementation of the common np.log(...).diff().iloc"
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 7,
        "character": 0
      },
      "end": {
        "line": 17,
        "character": 16
      }
    },
    "code": "def calc_beta(x_name, window, returns_data):\n    window_inv = 1.0 / window\n    x_sum = returns_data[x_name].rolling(window, min_periods=window).sum()\n    y_sum = returns_data.rolling(window, min_periods=window).sum()\n    xy_sum = returns_data.mul(returns_data[x_name], axis=0).rolling(window, min_periods=window).sum()\n    xx_sum = np.square(returns_data[x_name]).rolling(window, min_periods=window).sum()\n    xy_cov = xy_sum - window_inv * y_sum.mul(x_sum, axis=0)\n    x_var = xx_sum - window_inv * np.square(x_sum)\n    betas = xy_cov.divide(x_var, axis=0)[window - 1:]\n    betas.columns.name = None\n    return betas",
    "narration": "calc_beta computes a rolling, per-asset beta series of every column in the provided returns_data against the benchmark column named by x_name, using a window-length sample so downstream functions like top_alpha and calc_alpha can work with beta estimates aligned to the same ndays returns. It first computes the scalar inverse of the window to reuse in the algebraic formula, then forms four rolling sums over full windows: the sum of the benchmark returns, the sum of each asset’s returns, the sum of elementwise products between each asset and the benchmark, and the sum of the benchmark squared. Those aggregate sums are combined using the algebraic identities for sample covariance and variance (cov = sum(xy) − (1/n) sum(x) sum(y); var = sum(x^2) − (1/n) sum(x)^2) so the implementation avoids subtracting per-window means explicitly and stays fully vectorized across many columns for performance. The function then divides the covariance by the benchmark variance to produce beta for each asset, uses the rolling min_periods equal to the window so only full-window estimates are produced, slices off the initial rows that are still incomplete so the index lines up with get_ndays_return’s output, clears the columns name for a clean DataFrame, and returns the beta time series that top_alpha later drops for the market column and feeds into calc_alpha."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 18,
        "character": 0
      },
      "end": {
        "line": 23,
        "character": 16
      }
    },
    "code": "def calc_alpha(returns, market_returns, risk_free_returns, beta):\n    returns_over_risk_free = returns.subtract(risk_free_returns, axis=0)\n    market_over_risk_free = market_returns - risk_free_returns\n    beta_market_risk_free = beta.multiply(market_over_risk_free, axis=0)\n    alpha = returns_over_risk_free - beta_market_risk_free.values\n    return alpha",
    "narration": "calc_alpha is the small numeric core that turns the n‑day return series and the betas computed by calc_beta into Jensen’s alpha for each ticker and date so the rest of the framework can score and report stock-level outperformance. When top_alpha calls calc_alpha it has already produced n‑day returns for stocks, the market, and the risk‑free series via get_ndays_return and computed rolling betas via calc_beta; calc_alpha then computes three things in sequence: it first forms each asset’s excess return by subtracting the risk‑free n‑day return from the asset n‑day returns, it forms the market’s excess return by subtracting the same risk‑free series from the market n‑day returns, and it multiplies the beta matrix by that market excess return (aligning on the time index) to get the portion of each asset’s excess return explained by market exposure. The final alpha is the asset excess return minus that beta‑scaled market excess, producing a DataFrame of per‑date, per‑ticker alphas that top_alpha uses for ranking. There are no branches or side effects here; the function relies on pandas alignment and the upstream rolling/window trimming performed by calc_beta and get_ndays_return so the returned alpha series are already shape‑compatible for downstream universe scoring, Strategy.log hooks, and reporting."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 24,
        "character": 0
      },
      "end": {
        "line": 26,
        "character": 39
      }
    },
    "code": "def get_ndays_return(daily_returns, ndays=22):\n    ndays_returns = (1 + daily_returns).rolling(ndays, min_periods=ndays).apply(np.prod, raw=True) - 1\n    return ndays_returns.iloc[ndays-1:]",
    "narration": "get_ndays_return takes a pandas Series or DataFrame of per-day simple returns (daily_returns) and turns them into rolling, compounded N‑day returns (ndays defaults to 22, roughly a trading month). It does this by accumulating the multiplicative growth over each fixed-length window — effectively multiplying one plus each daily return across the window and subtracting one to get the total simple return for that N‑day period — and it requires a full window before emitting a value so partial windows are treated as missing. The function then trims off the initial rows that precede the first complete N‑day window so the returned series/frame aligns to the end of each N‑day interval; that alignment matches how calc_beta also trims to the same window offset, which is important because top_alpha uses these N‑day returns for stocks, market, and risk_free before feeding them into calc_alpha to compute Jensen’s alpha. Compared with log_returns, which produces per-period continuously compounded returns via log differences, get_ndays_return produces multi‑day simple compounded returns over a fixed lookback; compared with get_returns, which reports full-period total and annualized returns, get_ndays_return yields a rolling series of N‑day outcomes suitable for scoring and ranking in the universe and reporting pipelines."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 27,
        "character": 0
      },
      "end": {
        "line": 38,
        "character": 99
      }
    },
    "code": "def top_alpha(stocks, market, risk_free, window, top_n_count=0):\n    assert(stocks.shape[0] == market.shape[0] and market.shape[0] == risk_free.shape[0]\n           ), 'inputs do not have same shape: {} {} {}'.format(stocks.shape[0], market.shape[0], risk_free.shape[0])\n    market_name = 'market_returns'\n    market = market.rename(market_name)\n    returns_data = pd.concat([market, stocks], axis=1)\n    betas = calc_beta(market_name, window, returns_data).drop(market_name, axis=1)\n    stocks_nday_returns = get_ndays_return(stocks, window)\n    market_nday_returns = get_ndays_return(market, window)\n    risk_free_nday_returns = get_ndays_return(risk_free, window)\n    alpha = calc_alpha(stocks_nday_returns, market_nday_returns, risk_free_nday_returns, betas)\n    return alpha.iloc[-1].nlargest(top_n_count) if top_n_count > 0 else alpha.sort(ascending=False)",
    "narration": "top_alpha is the orchestration entry that produces Jensen‑alpha scores and optional top‑N rankings for a universe by wiring together the beta estimator, N‑day return aggregator, and alpha calculator so the rest of the framework can score and pick names for strategies and reports. It first asserts the three incoming series tables—stocks, market, and risk_free—have the same row count to ensure time alignment, then renames the market series to a fixed benchmark column name and concatenates it with the stock columns into a single returns_data frame so calc_beta can treat the renamed market column as the independent variable. It calls calc_beta to get per‑asset, rolling betas (dropping the benchmark column from the beta table so only asset betas remain), then uses get_ndays_return to turn daily series for stocks, market, and risk_free into ndays returns that are aligned with the beta output’s lookback offset. Those aligned ndays returns and the betas are handed to calc_alpha, which produces Jensen’s alpha for each ticker and date by removing the risk‑free component and subtracting the beta‑scaled market excess return. Finally, top_alpha either returns a snapshot ranking of the most recent date’s alpha values limited to top_n_count when a positive top_n_count is provided, or returns the full alpha time series sorted in descending order when top_n_count is zero, so downstream ranking, universe selection, and reporting can consume either a top list or the full historical alpha surface."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 39,
        "character": 0
      },
      "end": {
        "line": 40,
        "character": 63
      }
    },
    "code": "def var(returns, confidence):\n    return returns.quantile(confidence, interpolation='higher')",
    "narration": "var accepts a Series or DataFrame of realized returns and a confidence level and produces an empirical value‑at‑risk by asking the underlying pandas quantile machinery for the requested percentile of the return distribution. In practice get_risk passes a low confidence (0.05) so var yields the 5% left‑tail cutoff that marks the loss threshold; the function uses a non‑interpolating, conservative quantile rule that picks the nearest observed return at the percentile instead of averaging between observations, so the output is a discrete historical VaR. The return shape mirrors the input (a scalar for a single Series or a per‑column Series for a DataFrame), which keeps the result aligned with the rest of the pipeline’s vectorized utilities; get_risk then hands that cutoff to cvar to compute the average loss below the VaR. This keeps VaR computation centralized, reproducible, and consistent with the other performance/risk helpers like log_returns, calc_beta, and calc_alpha."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 41,
        "character": 0
      },
      "end": {
        "line": 42,
        "character": 52
      }
    },
    "code": "def cvar(returns, value_at_risk):\n    return returns[returns.lt(value_at_risk)].mean()",
    "narration": "cvar implements the expected shortfall metric used throughout the reporting and risk hooks: given a series of period returns and a value_at_risk threshold (which get_risk obtains by calling var), cvar selects the subset of returns that are worse than that VaR threshold and returns their average. In practice that means it computes the average loss conditional on exceeding the VaR cutoff, producing the conditional value at risk that get_risk then pairs with the VaR to populate the VAR/cVAR table for both the historical returns and each Monte Carlo scenario returned by get_simulations. Conceptually, cvar centralizes the tail‑risk calculation so the framework can compare average extreme losses across assets and simulation regimes in a consistent, vectorized way."
  },
  {
    "open-file": "tools/fin_calc.py",
    "range": {
      "start": {
        "line": 43,
        "character": 0
      },
      "end": {
        "line": 44,
        "character": 30
      }
    },
    "code": "if __name__ == '__main__':\n    print('fin_calc imported')\n",
    "narration": "The conditional at the end checks whether fin_calc is being executed as the top-level program, and when that is true it emits a single-line confirmation to stdout indicating the module was brought into a runtime context. In practice this means the message appears only when someone runs the fin_calc file directly from the interpreter and does not run when other parts of the backtesting framework import fin_calc to call utilities like log_returns, calc_beta, or calc_alpha. Unlike the module-level imports and constant definitions that always execute on import, this guarded print is explicitly context-dependent and therefore provides a lightweight, human-readable signal useful for ad-hoc execution or manual testing without affecting the normal import-time behavior relied upon by the rest of the system."
  },
  {
    "create-file": "tools/hurst.py"
  },
  {
    "open-file": "tools/hurst.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 10,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy import stats\nimport statsmodels\nfrom statsmodels.tsa.stattools import adfuller\nfrom matplotlib import pyplot as plt\nfrom tools import fin_calc",
    "narration": "The file brings in operating-system and command-line helpers so the hurst utility can be invoked from a shell and locate or persist files: os handles path and environment concerns and argparse enables parsing CLI flags and arguments. Pandas and NumPy provide the core data and numeric primitives used throughout the Hurst and variance-ratio calculations — time-index alignment, Series/DataFrame slicing, and fast vectorized math that feed the log-return and N‑day return computations you already saw in log_returns. SciPy and specifically scipy.stats offer the distribution and significance tools used for the diagnostic tests and any p‑value or statistic calculations around variance ratios and related measures, while statsmodels.tsa.stattools.adfuller supplies the Augmented Dickey‑Fuller unit‑root test to assess stationarity as a complement to Hurst-based trend/mean‑reversion inference. Matplotlib.pyplot is pulled in to render diagnostic charts and persistence plots that analysts inspect after the numeric diagnostics run. Finally, the project’s fin_calc module is reused for financial helper routines (the same family of utilities that other scripts rely on for returns, volatility and related finance calculations). Compared with the similar import set in main, this file shifts emphasis from direct distribution imports toward the stationarity test from statsmodels and uses the top‑level tools.fin_calc import rather than a relative import and the broader set of SciPy distribution functions, reflecting hurst’s role as a diagnostic/analysis script that combines numerical series work, formal stationarity testing, and visual reporting."
  },
  {
    "open-file": "tools/hurst.py",
    "range": {
      "start": {
        "line": 11,
        "character": 0
      },
      "end": {
        "line": 15,
        "character": 22
      }
    },
    "code": "def hurst_exp(d):\n    lags = range(2, 100)\n    tau = [np.sqrt(np.std(d.diff(lag))) for lag in lags]\n    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n    return poly[0]*2.0",
    "narration": "hurst_exp takes a numeric time series (typically the logged spread or price series produced earlier by load_data/main) and estimates the Hurst exponent by measuring how the dispersion of lagged differences scales with lag. It iterates lags from 2 up to 99, computes the standard deviation of the series differences at each lag (the same dispersion concept measured by std elsewhere) and then takes a secondary square root so that the quantity being regressed scales as lag to the power H/2. It then fits a straight line on the log of lag versus the log of that dispersion vector; the fitted slope corresponds to half the Hurst exponent, so hurst_exp returns twice the slope as the Hurst estimate. In practice this yields a single scalar H that the rest of the tooling (and logging via Strategy.log or the main function) uses to classify the series as trending (H>0.5), mean-reverting (H<0.5), or close to a random walk (H≈0.5). The function assumes a sufficiently long, NaN-free series so the log–log regression across the chosen lag range is meaningful."
  },
  {
    "open-file": "tools/hurst.py",
    "range": {
      "start": {
        "line": 16,
        "character": 0
      },
      "end": {
        "line": 23,
        "character": 24
      }
    },
    "code": "def variance_ratio(ts, lag=2):\n    ts = np.asarray(ts)\n    n = len(ts)\n    mu = sum(ts[1:n] - ts[:n-1]) / n\n    m = (n - lag + 1) * (1 - lag / n)\n    b = sum(np.square(ts[1:n] - ts[:n-1] - mu)) / (n - 1)\n    t = sum(np.square(ts[lag:n] - ts[:n - lag] - lag * mu)) / m\n    return t / (lag * b)",
    "narration": "variance_ratio computes a classic variance‑ratio diagnostic on a numeric time series to help decide whether a spread behaves like a random walk, a trending series, or a mean‑reverting series; in this project it complements hurst_exp as an out‑of‑backtest instrument-level diagnostic that main calls after load_data constructs the spread. Conceptually it first coerces the input into a fast numeric array and measures the average one‑period increment, then forms two variance estimators: a one‑period (short‑horizon) variance built from demeaned consecutive differences and a multi‑period (lagged) variance built from differences separated by the specified lag and centered by lag times the one‑period mean. The function applies the usual overlapping‑observations adjustment when computing the multi‑period variance so the numerator uses the effective sample size m, while the denominator uses the sample variance of single‑period differences; the returned value is the ratio of the multi‑period variance to lag times the single‑period variance. Interpreting the output is straightforward: values near one indicate a random walk, values greater than one indicate persistence/trending, and values less than one indicate mean reversion. The data flow in practice is: load_data produces the price series used to build the spread in main, main takes the log of that spread and passes it into hurst_exp and variance_ratio to produce two complementary diagnostics (main prints them; higher‑level wrappers in other contexts route similar outputs through Strategy.log for timestamped console recording). There is no branching or exception handling in variance_ratio; it performs purely numeric aggregation and"
  },
  {
    "open-file": "tools/hurst.py",
    "range": {
      "start": {
        "line": 24,
        "character": 0
      },
      "end": {
        "line": 39,
        "character": 35
      }
    },
    "code": "def main(tickers):\n    df = load_data(tickers[0], tickers[1]).iloc[-1000:]\n    a = df[tickers[0]]\n    b = df[tickers[1]]\n    series = a - b\n    h = hurst_exp(np.log(series))\n    print('Hurst:\\t\\t', h)\n    vr = variance_ratio(np.log(series), 2)\n    print('Var Ratio:\\t', vr)\n    ylag = np.roll(series, 1)\n    ylag[0] = 0\n    ydelta = series.diff(1)\n    ydelta[0] = 0\n    beta, _ = np.polyfit(ydelta, ylag, 1)\n    halflife = -np.log(2) / beta\n    print('Half Life:\\t', halflife)",
    "narration": "main is the lightweight entry point that pulls a two-column price frame from the project's data layer, converts those columns into a spread, runs three complementary diagnostics on that spread, and emits the results to the console. It begins by invoking load_data to read the two tickers' adjusted-close series and truncates to the most recent 1,000 observations so the diagnostics work on a consistent recent window. It then forms a price spread by subtract"
  },
  {
    "open-file": "tools/hurst.py",
    "range": {
      "start": {
        "line": 40,
        "character": 0
      },
      "end": {
        "line": 49,
        "character": 13
      }
    },
    "code": "def load_data(ticker_a, ticker_b):\n    a_path = os.path.join(os.path.dirname(__file__), f'../data/price/{ticker_a}.csv')\n    a = pd.read_csv(a_path, index_col=0)['Adj Close'].rename(ticker_a)\n    b_path = os.path.join(os.path.dirname(__file__), f'../data/price/{ticker_b}.csv')\n    b = pd.read_csv(b_path, index_col=0)['Adj Close'].rename(ticker_b)\n    df = pd.DataFrame({\n        ticker_a: a,\n        ticker_b: b\n    }).dropna()\n    return df",
    "narration": "load_data is the simple I/O gateway that takes the two ticker identifiers provided by main and turns the project’s locally cached price files into a clean, two-column pandas time series that downstream diagnostics can consume. It constructs a file path for each ticker relative to the module’s location, reads the local CSV for each symbol, extracts the adjusted close column and renames each extracted Series to its ticker, then assembles those two Series into a single DataFrame. It then drops any rows with missing values so the returned DataFrame contains only dates where both tickers have valid adjusted-close observations; that alignment is important because main immediately computes a spread between the two series and passes the result to the Hurst and variance-ratio routines which expect contiguous, aligned data. The function performs file I/O only and returns the sanitized, aligned DataFrame of adjusted closes for use by the diagnostic code called by main."
  },
  {
    "open-file": "tools/hurst.py",
    "range": {
      "start": {
        "line": 50,
        "character": 0
      },
      "end": {
        "line": 54,
        "character": 22
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('tickers', nargs=2)\n    ARGS = PARSER.parse_args()\n    main(ARGS.tickers)\n",
    "narration": "The module-level entry guard sets up a tiny command-line surface so you can run the hurst utility from a shell and pass it the two tickers you want to analyze. Inside that guard a new argparse parser is created (bound to the PARSER name), a single required positional argument called tickers is declared to accept exactly two symbols, and the parsed values are captured in ARGS; finally main is invoked with ARGS.tickers so the rest of the pipeline kicks off. In terms of data flow, those two ticker strings flow into main which calls load_data to produce the two-column price frame, builds the spread, and runs hurst_exp and variance_ratio (and the half‑life routine) before emitting the diagnostics. This follows the same entry-point pattern used elsewhere in the project to keep scripts import-safe, but it is intentionally minimal here: it enforces exactly two positional tickers rather than exposing additional CLI flags."
  },
  {
    "create-file": "backtest/algos/BuyAndHold.py"
  },
  {
    "open-file": "backtest/algos/BuyAndHold.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import BaseStrategy as base",
    "narration": "The line brings the package-local BaseStrategy module into the BuyAndHold module under the alias base so the minimal buy‑and‑hold strategy can reuse the framework’s shared strategy plumbing rather than reimplementing it. BaseStrategy exposes a Strategy class that centralizes common lifecycle setup and helpers that all strategies in the project rely on (setup work the strategy lifecycle needs before next runs, consistent logging/housekeeping, and any shared utilities that tie strategies into the data and reporting pipeline). BuyAndHold’s initializer delegates to the base Strategy initializer to get that consistent setup and then implements only the simple buy‑once, hold logic; this follows an inheritance/Template Method pattern used across the codebase, and mirrors the same import pattern you see elsewhere where internal strategy modules import BaseStrategy while other files import external libraries like numpy or backtrader."
  },
  {
    "open-file": "backtest/algos/BuyAndHold.py",
    "range": {
      "start": {
        "line": 4,
        "character": 0
      },
      "end": {
        "line": 23,
        "character": 39
      }
    },
    "code": "class BuyAndHold(base.Strategy):\n    params = (\n        ('target_percent', 0.99),\n    )\n\n    def __init__(self):\n        ...\n\n    def buy_and_hold(self):\n        ...\n\n    def next(self):\n        ...",
    "narration": "BuyAndHold is a minimal Backtrader Strategy subclass used as the entry‑point benchmark to validate the data and plumbing that load_data and main provide to the engine. Its params tuple declares a single target_percent defaulting to 0.99, __init__ wires the strategy into Backtrader via the usual base.Strategy lifecycle, and the behavioral logic is split between a buy_and_hold helper and next. buy_and_hold computes an equal allocation by dividing the configured target_percent across all data feeds provided to the strategy and then issues portfolio target orders using Backtrader’s target‑percent ordering facility, while next simply delegates to buy_and_hold so that on each incoming bar the strategy establishes the initial long positions and then enforces those allocations thereafter. Because BuyAndHold inherits from base.Strategy, it relies on the parent for logging and order notifications and therefore contains only the allocation decision and ordering"
  },
  {
    "open-file": "backtest/algos/BuyAndHold.py",
    "range": {
      "start": {
        "line": 5,
        "character": 4
      },
      "end": {
        "line": 7,
        "character": 5
      }
    },
    "code": "    params = (\n        ('target_percent', 0.99),\n    )",
    "narration": "The params declaration defines the strategy's configurable defaults for Backtrader's parameter mechanism, exposing a single parameter named target_percent with a default of ninety‑nine percent; at runtime Backtrader will make that value available on the strategy instance so the buy_and_hold helper and next method can compute how much of the portfolio to allocate when placing the one-time buy order. In the context of the project's architecture — where this file serves as the minimal buy‑and‑hold benchmark used to validate the data and plumbing that main and load_data prepare and that diagnostics like hurst_exp and variance_ratio analyze — target_percent determines the baseline capital allocation for comparison against other strategies. The pattern here mirrors other parameter declarations in the codebase: one variant defines parameters as a mapping with a slightly different default allocation, another variant adds an extra kwargs slot alongside the same target_percent default, and the BuyAndHold strategy class uses the same tuple-style params entry; functionally they all serve the same Backtrader convention of making strategy configuration explicit and overridable by the backtest harness."
  },
  {
    "open-file": "backtest/algos/BuyAndHold.py",
    "range": {
      "start": {
        "line": 9,
        "character": 4
      },
      "end": {
        "line": 10,
        "character": 36
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)",
    "narration": "When a BuyAndHold instance is created, BuyAndHold.__init__ delegates its setup to the shared base Strategy initializer so that the strategy arrives with the standard Backtrader plumbing already configured. The call into base.Strategy.__init__ performs the Backtrader-level initialization, establishes canonical instance attributes used throughout the framework (order tracking, purchase price and commission placeholders, an order rejection flag and the verbose toggle), and thereby wires in the common logging and order‑notification helpers such as Strategy.log and Strategy.notify_order. Unlike CrossOver.__init__ or LeveragedEtfPair.__init__, which call the same base initializer and then attach indicators, assertions, or additional state, BuyAndHold.__init__ intentionally does not add any extra attributes — it returns None but produces the side effects of populating the standard strategy fields so the minimal next()/buy_and_hold flow can operate against a fully initialized strategy instance and validate the data-and-plumbing pipeline that main and load_data previously prepared."
  },
  {
    "open-file": "backtest/algos/BuyAndHold.py",
    "range": {
      "start": {
        "line": 12,
        "character": 4
      },
      "end": {
        "line": 15,
        "character": 61
      }
    },
    "code": "    def buy_and_hold(self):\n        for d in self.datas:\n            split_target = self.params.target_percent / len(self.datas)\n            self.order_target_percent(d, target=split_target)",
    "narration": "buy_and_hold is the simple allocator used by the BuyAndHold strategy: when next decides it needs to enter or re-enter the market it iterates over the Backtrader data feeds attached to the strategy, computes an equal per-asset target by taking the strategy parameter target_percent and dividing it by the number of feeds, and then issues an order_target_percent for each feed so the broker adjusts each instrument to that fractional share of the portfolio. in terms of data flow, the assets being iterated are the data feeds produced by the project’s load_data layer and diagnostics run earlier by main, and the order_target_percent calls translate that desired allocation into broker-side orders handled by Backtrader. control-flow-wise, buy_and_hold is intentionally simple and idempotent: it assumes there is at least one data feed and enforces an equal-weight buy-and-hold allocation whenever next determines the strategy is flat or needs to retry after a rejected order. compared to WeightedHold.buy_and_hold, which scales the same target_percent by per-instrument weights, BuyAndHold.buy_and_hold uses an even split across all feeds, making it a minimal baseline to validate the data and plumbing and to serve as a benchmark for more sophisticated allocation schemes."
  },
  {
    "open-file": "backtest/algos/BuyAndHold.py",
    "range": {
      "start": {
        "line": 17,
        "character": 4
      },
      "end": {
        "line": 23,
        "character": 39
      }
    },
    "code": "    def next(self):\n        if not self.position:\n            self.buy_and_hold()\n\n        elif self.order_rejected:\n            self.buy_and_hold()\n            self.order_rejected = False",
    "narration": "BuyAndHold.next is the per-bar decision routine for the buy-and-hold baseline strategy: on each invocation it first checks whether the strategy already holds a position and, if not, delegates to buy_and_hold to place the initial allocation across the available data feeds. If an earlier order was rejected, next will also call buy_and_hold again to retry the intended allocation and then clear the order_rejected flag so the retry is not repeated indefinitely. Conceptually this implements a simple enter‑once-and-hold workflow where buy_and_hold is responsible for splitting the strategy's target_percent evenly across self.datas and issuing the target-percent orders; next only triggers that entry on the happy path (no position) or the recovery path (order_rejected). Compared with WeightedHold.next, which calls buy_and_hold every bar regardless, and CrossOver.next, which uses additional gating around an order handle and a buysell signal and can emit sell orders, BuyAndHold.next keeps the control flow minimal with two branches to either initiate the one-time entry or retry a failed entry, and it updates the order_rejected instance attribute as its only side effect."
  },
  {
    "create-file": "backtest/algos/CrossOver.py"
  },
  {
    "open-file": "backtest/algos/CrossOver.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import backtrader as bt\nfrom . import BaseStrategy as base",
    "narration": "Remember the earlier imports that pulled in operating-system helpers and argparse for CLI use; alongside those, this file brings in backtrader and the project's BaseStrategy. backtrader supplies the runtime primitives the backtesting engine uses — the Strategy lifecycle, order and broker abstractions, indicators, and the Cerebro execution environment — so importing it gives CrossOver access to the framework it will run inside. Importing BaseStrategy under the local name base ties CrossOver into the project’s shared strategy scaffold: BaseStrategy centralizes common behavior such as structured logging via Strategy.log, order management, and any standardized sizing or lifecycle hooks, and CrossOver builds on that scaffold by implementing its specific signal logic. This follows the template-style pattern used across the strategies where a concrete strategy specializes the generic behavior defined by BaseStrategy. You’ll see the same backtrader import convention in many modules, and other modules sometimes import multiple concrete strategies from the package, but here the file explicitly pulls in only BaseStrategy (aliased to base) because CrossOver subclasses and reuses that shared implementation rather than reimplementing common concerns."
  },
  {
    "open-file": "backtest/algos/CrossOver.py",
    "range": {
      "start": {
        "line": 3,
        "character": 0
      },
      "end": {
        "line": 23,
        "character": 41
      }
    },
    "code": "class CrossOver(base.Strategy):\n    params = {\n        'target_percent': 0.95\n    }\n    def __init__(self):\n        ...\n    def next(self):\n        ...",
    "narration": "CrossOver is a concrete trading strategy class that plugs into the project's backtesting engine as a simple, signal-driven allocator: it subclasses Strategy and, during initialization, constructs two short and medium simple moving averages and a CrossOver indicator that watches their relationship; it also exposes a target_percent parameter defaulting to 0.95 that represents the desired portfolio exposure on a buy signal. At runtime the next method implements the decision flow: it first respects any outstanding order to avoid duplicate submissions, then if there is no current position it will open one when the fast SMA crosses above the slow SMA (or if a prior order was rejected) by asking the engine to move the position toward the target_percent and clearing the rejection flag; if there is an existing position it will close it when the fast SMA crosses below the slow SMA by asking the engine to reduce exposure to zero. All activity is reported via Strategy.log so the same logging and order bookkeeping initialized in Strategy.__init__ (order, order_rejected, buyprice/buycomm placeholders) are used; functionally CrossOver follows the same subclassing pattern other strategies in the codebase use but differs in that its trade signals come from a simple MA crossover indicator rather than periodic rebalance, ranking, or mean‑reversion logic."
  },
  {
    "open-file": "backtest/algos/CrossOver.py",
    "range": {
      "start": {
        "line": 4,
        "character": 4
      },
      "end": {
        "line": 6,
        "character": 5
      }
    },
    "code": "    params = {\n        'target_percent': 0.95\n    }",
    "narration": "The params assignment establishes the strategy's default configuration by declaring a target_percent parameter with a default of ninety‑five percent; within the backtesting pipeline this value is consumed by the CrossOver strategy to decide how large a position to target relative to available capital when next() generates buy or sell signals, and Strategy.log will surface changes or decisions driven by that sizing choice. Conceptually it plays the role of a tunable hyperparameter so the entry/exit logic can remain generic while allowing runs to vary allocation aggressiveness. This pattern mirrors other parameter declarations in the codebase: one variant exposes parameters as a tuple of name/value pairs with a slightly different default of ninety‑nine percent, and another tuple-based declaration also includes an additional kwargs parameter; the CrossOver class itself repeats a params declaration with the same ninety‑five percent default. The different declarations are simply alternate syntactic ways the framework accepts to expose strategy defaults for downstream use in next() and by external run-time configuration."
  },
  {
    "open-file": "backtest/algos/CrossOver.py",
    "range": {
      "start": {
        "line": 7,
        "character": 4
      },
      "end": {
        "line": 11,
        "character": 80
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)\n        self.sma5 = bt.indicators.MovingAverageSimple(period=5)\n        self.sma30 = bt.indicators.MovingAverageSimple(period=30)\n        self.buysell = bt.indicators.CrossOver(self.sma5, self.sma30, plot=True)",
    "narration": "CrossOver.__init__ configures the base bookkeeping and the indicator inputs that drive the strategy’s per-bar decision logic. It first delegates to base.Strategy.__init__ so the common fields used across strategies (order tracking, buy price/commission placeholders, the order_rejected flag and the verbosity switch) are initialized exactly as other strategies expect. It then constructs two simple moving averages — a five‑bar SMA and a thirty‑bar SMA — and assigns them to sma5 and sma30 so Backtrader will update their values automatically as new bars arrive. Finally it builds a CrossOver indicator that watches those two SMAs and exposes a buysell signal on the instance; that indicator is set up to be available for plotting as well. By attaching sma5, sma30, and buysell to the instance, the initialization wires the data"
  },
  {
    "open-file": "backtest/algos/CrossOver.py",
    "range": {
      "start": {
        "line": 12,
        "character": 4
      },
      "end": {
        "line": 23,
        "character": 41
      }
    },
    "code": "    def next(self):\n        if self.order:\n            return\n        if not self.position:\n            if self.buysell > 0 or self.order_rejected:\n                self.log('BUY CREATE, {:.2f}'.format(self.data.close[0]))\n                self.order = self.order_target_percent(target=self.params.target_percent)\n                self.order_rejected = False\n        else:\n            if self.buysell < 0:\n                self.log('SELL CREATE, {:.2f}'.format(self.data.close[0]))\n                self.order = self.close()",
    "narration": "CrossOver.next is the per-bar decision routine that turns the crossover indicator constructed in CrossOver.__init__ into actual broker orders while using the Strategy logging and order bookkeeping established in Strategy.__init__. On each invocation it first guards against issuing another order when one is already pending by returning early if the strategy has a non-null order, which prevents overlapping orders. If there is no open position, it looks for a buy opportunity: it interprets the buysell indicator produced in __init__ and will place a market-sizing order via order_target_percent when the indicator signals a positive crossover or when a previous order was rejected (order_rejected is the flag that Strategy.notify_order sets on rejected/canceled orders), and it logs a buy creation message that includes the current close price using Strategy.log. When placing that buy it stores the returned order object back on the order attribute and clears the order_rejected flag so the attempt won’t be retried unnecessarily. If the strategy already holds a position, it instead watches for a negative crossover and, on such a sell signal, logs a sell creation message with the current close and issues a close order via the strategy’s close method, again saving the returned order to the order attribute. The control flow therefore has three clear paths: do nothing when an order is pending, attempt to enter on a buy signal or after a rejected attempt, and exit when a sell signal appears; sizing for entries comes from the target_percent parameter defined on the CrossOver params."
  },
  {
    "create-file": "backtest/algos/LeveragedEtfPair.py"
  },
  {
    "open-file": "backtest/algos/LeveragedEtfPair.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import numpy as np\nfrom . import BaseStrategy as base",
    "narration": "These two imports bring in the numerical toolbox and the project’s strategy base class that LeveragedEtfPair builds on. numpy is imported to provide fast array and vector operations used when the strategy computes ratios, rolling statistics, weights, and any numeric transformations the rebalance routine needs; those calculations operate on the historical bar arrays supplied by the data layer. BaseStrategy is imported under the local alias base so the LeveragedEtfPair can inherit common lifecycle hooks, logging, parameter plumbing, and helper routines (for example the buy_and_hold allocator you’ve already seen) that standardize how strategies interact with the Backtrader engine and the broker. The aliasing follows the same pattern used elsewhere in the codebase and the two import orders you’ve seen are functionally equivalent; here they prepare the strategy to take in feeds from the data pipeline, run numpy-driven computations each bar, and then call into BaseStrategy’s utilities to execute rebalances."
  },
  {
    "open-file": "backtest/algos/LeveragedEtfPair.py",
    "range": {
      "start": {
        "line": 3,
        "character": 0
      },
      "end": {
        "line": 24,
        "character": 39
      }
    },
    "code": "class LeveragedEtfPair(base.Strategy):\n    params = {\n        'leverages': [1, 1],\n        'rebalance_days': 21,\n        'target_percent': -0.95\n    }\n    def __init__(self):\n        ...\n    def rebalance(self):\n        ...\n    def next(self):\n        ...",
    "narration": "LeveragedEtfPair is a concrete Strategy implementation wired into the backtesting engine to manage a two-ETF leveraged pairing by periodically rebalancing to a set of target exposures. Its params declare a leverages vector, a rebalance_days frequency, and a target_percent that represents the net portfolio exposure (the default is negative to indicate a net short sizing convention). In __init__ it first invokes the base Strategy initializer, asserts that exactly two data feeds are attached (because the strategy is explicitly a pair allocator), and then constructs an internal leverage vector by taking the absolute values of the user-supplied leverages and scaling them so their sum equals the configured target_percent — that scaling step ensures the two instruments split the overall desired portfolio exposure according to the relative leverages. The rebalance routine walks the attached data feeds and uses the broker-facing order_target_percent facility provided by the Strategy base class to set each instrument to its computed fractional exposure, so the broker adjusts position sizes to match the leverage-weighted target. The next method hooks this rebalancing into the per-bar lifecycle: it triggers the initial rebalance and then re-applies the rebalance logic on the cadence defined by rebalance_days so the pair is kept at the intended exposures as prices evolve. Functionally, LeveragedEtfPair follows the same design pattern other strategies in the project use when they expose a rebalance method (for example EqualVolatility) and relies on the base Strategy for logging and order notification, but its responsibility is narrow and specific: maintain a leverage-scaled allocation between two ETFs rather than generating signal-driven entries like CrossOver or a passive allocation like BuyAndHold."
  },
  {
    "open-file": "backtest/algos/LeveragedEtfPair.py",
    "range": {
      "start": {
        "line": 4,
        "character": 4
      },
      "end": {
        "line": 8,
        "character": 5
      }
    },
    "code": "    params = {\n        'leverages': [1, 1],\n        'rebalance_days': 21,\n        'target_percent': -0.95\n    }",
    "narration": "LeveragedEtfPair declares a params attribute to seed the strategy’s runtime configuration so the rebalance routine and the per-bar logic know how to size and when to act. The leverages parameter is a small list of multipliers intended to correspond to the two ETF data feeds the strategy trades; those multipliers are applied when computing target positions so each leg can be scaled independently (for example to model 2x/−1x relationships). The rebalance_days parameter establishes the cadence: the rebalance routine checks the bar count against this integer and only reshuffles the pair at that interval, which keeps the strategy’s reallocation behavior deterministic and tied to the incoming bar stream. The target_percent parameter defines the desired portfolio exposure used by the ordering logic; unlike CrossOver’s positive default exposure, this strategy sets a negative target_percent by default to express the pair’s short-biased or net-short sizing intention when the rebalance routine issues orders. Conceptually this follows the same params pattern used elsewhere in the codebase — providing a compact, declarative default configuration (similar to the other strategies that also expose rebalance_days and target_percent and sometimes a lookback) — but it differs by including per-leg leverages and by defaulting the sign of exposure to negative to encode the pair’s directional bias. These values feed into init, rebalance, and next so the strategy converts desired exposure into actual order_target_percent calls against the attached feeds."
  },
  {
    "open-file": "backtest/algos/LeveragedEtfPair.py",
    "range": {
      "start": {
        "line": 9,
        "character": 4
      },
      "end": {
        "line": 13,
        "character": 90
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)\n        assert len(self.datas) == 2, \"Exactly 2 datafeeds needed for this strategy!\"\n        self.leverages = np.abs(self.params.leverages)\n        self.leverages = self.params.target_percent * self.leverages / sum(self.leverages)",
    "narration": "LeveragedEtfPair.__init__ begins by invoking base.Strategy.__init__ so the usual Strategy bookkeeping is present (order state, buy price/comm, verbose flag, etc.), then enforces that the strategy is attached to exactly two data feeds because the algorithm is explicitly designed to operate on a pair. After that it computes the per-asset leverage targets: it first takes the absolute values of the leverages supplied via the strategy parameters to make the sizing direction-agnostic, and then rescales those absolute leverages so their sum matches the strategy-level target_percent parameter. The resulting normalized vector is stored on the instance as self.leverages; that attribute is the data passed to LeveragedEtfPair.rebalance, which uses it to place order_target_percent calls for each feed. Compared with PairSwitching.__init__, which only asserts two feeds, LeveragedEtfPair.__init__ adds the additional step of computing and normalizing exposure targets so the rebalance routine can allocate the portfolio to the two leveraged ETFs in proportion to the requested leverages while respecting the overall target_percent."
  },
  {
    "open-file": "backtest/algos/LeveragedEtfPair.py",
    "range": {
      "start": {
        "line": 14,
        "character": 4
      },
      "end": {
        "line": 16,
        "character": 59
      }
    },
    "code": "    def rebalance(self):\n        for i, d in enumerate(self.datas):\n            self.order_target_percent(d, self.leverages[i])",
    "narration": "Within the backtesting allocation layer, LeveragedEtfPair.rebalance is the short routine that applies the strategy's precomputed leverage targets to the broker on each rebalance event. LeveragedEtfPair.__init__ prepares self.leverages by taking the absolute leverages parameter and scaling them so their sum equals the strategy's target_percent, and rebalance consumes that array: it walks the attached Backtrader data feeds in order and issues an order to make each feed represent the corresponding leverage-weighted fraction of the portfolio. Data flows in from self.leverages (set during initialization) and from the sequence of feeds in self.datas; the outgoing side is the broker where order_target_percent adjusts positions to those targets. Control flow is a simple indexed loop so each element of the leverage vector maps positionally to a data feed (the init assertion enforces exactly two feeds for the class), and the routine is invoked on schedule from LeveragedEtfPair.next (and once at init) so rebalancing happens at the configured cadence or after rejected orders. Compared with NCAV.rebalance, which conditionally allocates only to a long subset and explicitly zeroes others, LeveragedEtfPair.rebalance always assigns the precomputed per-feed targets without conditional filtering, relying on the earlier Strategy.__init__ bookkeeping for order state and rejection handling."
  },
  {
    "open-file": "backtest/algos/LeveragedEtfPair.py",
    "range": {
      "start": {
        "line": 17,
        "character": 4
      },
      "end": {
        "line": 24,
        "character": 39
      }
    },
    "code": "    def next(self):\n        if self.order:\n            return\n        if len(self) % self.params.rebalance_days == 0:\n            self.rebalance()\n        elif self.order_rejected:\n            self.rebalance()\n            self.order_rejected = False",
    "narration": "LeveragedEtfPair.next is the per-bar decision routine that controls when the strategy reweights its two-ETF exposures; it acts as the entry point that triggers LeveragedEtfPair.rebalance under the strategy's scheduling and error-retry rules. On each invocation it first checks for an outstanding order and, if one exists, returns immediately so no new orders are placed while a previous order is pending. If there is no active order it uses the running bar count to decide cadence: when the current bar index is an exact multiple of the strategy parameter rebalance_days it calls rebalance, which in turn walks the attached data feeds and issues order_target_percent calls sized by the normalized leverages computed during LeveragedEtfPair.__init__. If an order was previously rejected, next also calls rebalance to retry the intended sizing and then clears the order_rejected flag on the instance. The structure mirrors the project's common pattern (see PairSwitching.next, EqualVolatility.next, NCAV.next) of a guard against active orders plus a periodic rebalance path and an order-rejection retry path, with the data flow moving from this scheduling logic into LeveragedEtfPair.rebalance where per-feed target percent orders are placed according to the leverages prepared at initialization."
  },
  {
    "create-file": "backtest/algos/NCAV.py"
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 4,
        "character": 0
      }
    },
    "code": "import numpy as np\nimport pandas as pd\nfrom . import BaseStrategy as base\nfrom data.info.info import all_balance",
    "narration": "The file imports numpy and pandas to provide the numeric and tabular tooling the NCAV algorithm needs to compute ratios, sort candidates, and manipulate per-ticker fundamentals; these libraries are used where the strategy converts raw balance-sheet numbers into NCAV metrics and slices the universe for ranking. It imports the project's BaseStrategy under the alias base so NCAV can inherit the shared lifecycle hooks, logging, params handling, and brokerage/order helpers that every concrete strategy in the framework uses. It also pulls in the all_balance helper from the data.info.info module so the strategy can obtain the preprocessed balance-sheet data for the attached data feeds; that call ties the strategy into the project’s data layer by requesting per-symbol balance information that the NCAV selection logic consumes. Compared to similar import patterns elsewhere, NCAV adds pandas and the all_balance data helper where simpler strategies only needed numpy and the base strategy; that reflects NCAV’s requirement for richer, tabular fundamentals rather than just price-series math. Together these imports connect the numeric processing, the shared strategy behavior, and the fundamentals data flow that NCAV uses during initialization, filtering, and rebalancing."
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 38,
        "character": 39
      }
    },
    "code": "class NCAV(base.Strategy):\n    params = {\n        'rebalance_days': 252,\n        'target_percent': 0.95,\n        'ncav_limit': 1.5\n    }\n    def __init__(self):\n        ...\n    def filter(self):\n        ...\n    def rebalance(self):\n        ...\n    def next(self):\n        ...",
    "narration": "NCAV is a concrete Strategy subclass that implements a classic Net Current Asset Value screening and scheduled rebalancing workflow to produce a value-driven, equally sized portfolio allocation. Its params expose three knobs: rebalance_days controls how often the strategy recomputes the NCAV universe and adjusts positions, target_percent sets the overall portfolio exposure the strategy tries to achieve (the same sizing role target_percent played for CrossOver and L4), and ncav_limit is the numeric cutoff used by the NCAV filter. During initialization NCAV delegates up to base.Strategy to get the shared bookkeeping and then builds any local state it needs: a snapshot of per-feed fundamental metadata, an empty list to hold the selected long candidates, and the standard order-tracking flags so it can avoid overlapping order activity. The filter method walks the attached data feeds, derives the NCAV metric from the fundamentals bundled with each feed, and selects the instruments that meet the ncav_limit cutoff; it populates the internal list of long candidates and logs the selection so the run can be audited. The rebalance method is the allocator: it compares the current portfolio to the filtered candidate list, closes positions for instruments that dropped out, and issues size-adjusting orders so the chosen names share the portfolio equally up to the target_percent aggregate exposure — conceptually the same equal-weight allocation behavior you saw in BuyAndHold.buy_and_hold but gated by the NCAV selection rather than a single initial allocation. next is the per-bar decision driver: it first guards against ordering while an order is outstanding, then on the scheduled cadence determined by rebalance_days it invokes filter followed by rebalance to refresh the portfolio; if prior orders were rejected it attempts a corrective rebalance and clears the rejection flag. Data flows from the raw"
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 6,
        "character": 4
      },
      "end": {
        "line": 10,
        "character": 5
      }
    },
    "code": "    params = {\n        'rebalance_days': 252,\n        'target_percent': 0.95,\n        'ncav_limit': 1.5\n    }",
    "narration": "The params block declares three strategy hyperparameters used throughout NCAV: rebalance_days set to 252, target_percent set to 0.95, and ncav_limit set to 1.5. In Backtrader strategies these defaults are exposed to the instance and drive behavior in __init__, filter, rebalance, and next. rebalance_days of 252 encodes an annual rebalancing cadence in trading days (so the strategy will schedule filter and rebalance work roughly once per year), which contrasts with the other strategies you’ve seen that use 21 for a roughly monthly cadence and sometimes include additional lookback parameters. target_percent at 0.95 means the strategy aims to deploy 95% of portfolio equity into the selected equal-weighted NCAV positions while keeping a small cash buffer. ncav_limit at 1.5 is the screening threshold used by the filter method: only tickers whose computed NCAV ratio meets or exceeds this multiplier are eligible for the portfolio. Together these three parameters control the selection strictness, how much capital gets allocated to the selected names, and how frequently the selection/allocation pipeline runs; the data layer provides the fundamentals for the NCAV calculation via numpy/pandas, and the scheduled next/rebalance flow then converts the filtered list and target_percent into concrete orders that the rebalance routine applies to the broker (as discussed earlier for LeveragedEtfPair.next and rebalance)."
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 11,
        "character": 4
      },
      "end": {
        "line": 14,
        "character": 22
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)\n        self.info = all_balance([d._name for d in self.datas])\n        self.long = []",
    "narration": "NCAV.__init__ starts by invoking base.Strategy.__init__ so the strategy instance has the standard Strategy bookkeeping (order state, pricing/commission tracking, verbose flag, etc.) already initialized as the other strategies do. It then builds a cached, per-universe fundamentals table by calling all_balance with the list of attached data feed names (obtained from each data feed’s name) and stores that DataFrame on the instance as self.info; this gives NCAV.filter a ready lookup table of balance-sheet rows for only the tickers the strategy is running on rather than reloading the full file at runtime. Finally it initializes self.long to an empty list to hold the tickers that pass the NCAV screen; NCAV.filter will populate this list based on the cached self.info and NCAV.rebalance will consume it to issue target-weight orders. Together these steps set up the instance state so NCAV.next can periodically call filter and rebalance without additional setup."
  },
  {
    "create-folder": "data"
  },
  {
    "create-folder": "data/info"
  },
  {
    "create-file": "data/info/info.py"
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 3,
        "character": 0
      }
    },
    "code": "import os\nimport pickle\nimport pandas as pd",
    "narration": "The file brings in os to handle filesystem concerns—resolving and joining paths and locating module-relative files so the fundamentals cache can be read from and written to the repository in a portable way; pickle to perform binary serialization and deserialization of the on-disk cache so the per-ticker fundamentals and any Python containers can be persisted and reloaded quickly; and pandas (imported as pd) to represent and manipulate the tabular statement data (balance sheet, income, cashflow) that the data providers and analytics expect to consume and return. This follows the same module-local cache pattern seen elsewhere in the project where a helper resolves a module-relative path and uses pickle.load to reconstruct a saved info object, while other modules additionally import numpy when numeric-array work is required; here the focus is on filesystem-backed caching and DataFrame-centric read/write for the fundamentals pipeline."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 45,
        "character": 0
      },
      "end": {
        "line": 49,
        "character": 15
      }
    },
    "code": "def all_balance(tickers=None):\n    data = load_file('BALANCE')\n    if tickers:\n        return data[data.Ticker.isin(tickers)]\n    return data",
    "narration": "all_balance is the simple file-backed accessor that provides the balance-sheet dataset to the rest of the system: it asks load_file for the BALANCE CSV (load_file pulls the appropriate file path from PATHS and returns a pandas DataFrame) and then either returns that full DataFrame or, when a tickers list is supplied, returns a filtered DataFrame containing only rows whose Ticker value is one of those entries. Because NCAV.__init__ invokes all_balance with the names of the strategy's attached data feeds, all_balance supplies the per-universe balance-sheet table that NCAV.filter later indexes by Report Date to compute per-share NCAV and decide longs; in contrast, balance and load_info are the per-ticker/per-date wrappers and all_info concatenates multiple statement files, so all_balance plays the middle role of supplying the raw or universe-subset balance-sheet table for downstream screening and rebalance logic."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 26,
        "character": 0
      },
      "end": {
        "line": 27,
        "character": 90
      }
    },
    "code": "def load_file(info_type):\n    return pd.read_csv(os.path.join(os.path.dirname(__file__), PATHS[info_type]), sep=';')",
    "narration": "load_file is the single, centralized reader that turns a requested fundamentals type into a pandas DataFrame for the rest of the pipeline to consume: it takes the info_type key, looks up the corresponding file location from the PATHS mapping, resolves that path relative to the module, and asks pandas to read the CSV using the project’s semicolon delimiter convention, returning the resulting DataFrame. Because load_info, all_balance, and all_info all delegate their initial I/O to load_file, the function standardizes how the three statement CSVs are located and parsed so downstream routines can focus on filtering by Ticker and Report Date (as load_info does) or concatenating per-statement frames (as all_info does) without repeating path resolution or CSV parsing details. The call is synchronous and deterministic—there’s no branching or fallback inside load_file—so callers receive either a ready-to-filter DataFrame or a raised exception from the underlying file access/CSV parse. In intent and shape it mirrors the project’s other loader utility that handles pickled blobs, but load_file specifically provides the CSV-to-DataFrame path used by NCAV during initialization when all_balance invokes it to obtain the balance-sheet universe for the strategy."
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 15,
        "character": 4
      },
      "end": {
        "line": 22,
        "character": 35
      }
    },
    "code": "    def filter(self):\n        self.long = []\n        for d in self.datas:\n            infos = self.info[self.info.Ticker == d._name]\n            info = infos.loc[(pd.to_datetime(infos['Report Date']) > self.data.datetime.datetime()).idxmax()]\n            ncav = (info['Total Current Assets'] - info['Total Liabilities']) / info['Shares (Basic)']\n            if ncav > self.params.ncav_limit:\n                self.long.append(d)",
    "narration": "NCAV.filter walks the universe of attached data feeds and builds the list of candidates that pass the Net Current Asset Value screen so rebalance has a clear target set. It starts from the fundamentals table that NCAV.__init__ obtained via all_balance and stored on self.info, then iterates over each data feed in self.datas; for each ticker it selects the matching rows in the fundamentals table and picks the most recent fundamental report whose report date is after the strategy’s current bar datetime. From that chosen balance-sheet row it computes NCAV per share by taking current assets minus total liabilities and dividing by basic shares outstanding, and compares that per-share NCAV to the threshold in params.ncav_limit. If the computed NCAV exceeds the threshold the feed is appended to self.long. The outcome is a filtered list of long candidates that NCAV.rebalance will later use to assign equal slices of the strategy’s target_percent among the selected tickers."
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 23,
        "character": 4
      },
      "end": {
        "line": 29,
        "character": 56
      }
    },
    "code": "    def rebalance(self):\n        for d in self.datas:\n            if d in self.long:\n                split_target = self.params.target_percent / len(self.long)\n                self.order_target_percent(d, target=split_target)\n            else:\n                self.order_target_percent(d, target=0.0)",
    "narration": "NCAV.rebalance walks every data feed attached to the strategy and enforces the portfolio targets determined by the NCAV screening: for each feed that is present in the list of screened longs prepared by NCAV.filter it computes an equal-weight allocation by dividing the strategy-level target_percent across the number of selected longs and sends that desired percent to the execution layer via order_target_percent so the broker moves toward that allocation; for each feed not in the screened long list it sends a zero target to order_target_percent to fully exit any exposure. In practice this ties the balance-sheet-based selection performed during NCAV.__init__ and NCAV.filter into concrete broker orders at each scheduled rebalance triggered by NCAV.next, behaving like the other rebalance/buy routines (for example LeveragedEtfPair.rebalance and BuyAndHold.buy_and_hold) but allocating only across the filtered NCAV-qualified universe and doing so with equal-sized splits."
  },
  {
    "open-file": "backtest/algos/NCAV.py",
    "range": {
      "start": {
        "line": 30,
        "character": 4
      },
      "end": {
        "line": 38,
        "character": 39
      }
    },
    "code": "    def next(self):\n        if self.order:\n            return\n        if len(self) % self.params.rebalance_days == 0:\n            self.filter()\n            self.rebalance()\n        elif self.order_rejected:\n            self.rebalance()\n            self.order_rejected = False",
    "narration": "NCAV.next is the per-bar decision entry point that enforces scheduling and safe order behavior before handing off to the allocation logic; it first guards against overlapping execution by returning immediately when there is an outstanding order, preserving the Strategy bookkeeping inherited via NCAV.__init__. On scheduled bars determined by the backtester bar count and the strategy parameter for rebalance interval, NCAV.next runs the NCAV screening routine to refresh the candidate list and then invokes NCAV.rebalance so the allocation layer translates the screened long list into concrete target-percent orders across the feeds. If an order was previously rejected, NCAV.next treats the next bar as an immediate retry: it calls NCAV.rebalance again and clears the order_rejected flag so the strategy can recover and reissue targets. In practice this means NCAV.next coordinates the data-driven filter that builds self.long, the timing driven by len(self) and params.rebalance_days, and the side-effecting rebalance calls that submit order instructions to the broker — a pattern very similar to LeveragedEtfPair.next and EqualVolatility.next but with the added NCAV-specific filtering step before reweighting."
  },
  {
    "create-file": "backtest/algos/PairSwitching.py"
  },
  {
    "open-file": "backtest/algos/PairSwitching.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import numpy as np\nfrom . import BaseStrategy as base",
    "narration": "The two imports bring in the numeric toolkit and the local strategy foundation the PairSwitching strategy builds on: numpy is pulled in to provide vectorized math, array ops, and numeric utilities that the pair-rotation logic will use for computing ratios, thresholds, and any rolling or elementwise comparisons across the two feeds, while the relative import of BaseStrategy (aliased as base) gives access to the project's Strategy scaffolding — the backtesting bookkeeping, order/state helpers, and the Strategy lifecycle methods that PairSwitching inherits from and initializes (recall LeveragedEtfPair.__init__ calls the same base.Strategy initialization and enforces two datafeeds). This mirrors the common pattern used elsewhere in the codebase where strategies import the shared base module and numpy the same way, keeping strategy implementations lightweight and consistent with the engine’s data-flow and lifecycle expectations."
  },
  {
    "open-file": "backtest/algos/PairSwitching.py",
    "range": {
      "start": {
        "line": 3,
        "character": 0
      },
      "end": {
        "line": 30,
        "character": 39
      }
    },
    "code": "class PairSwitching(base.Strategy):\n    params = {\n        'rebalance_days': 21,\n        'target_percent': 0.95,\n        'lookback': 60\n    }\n    def __init__(self):\n        ...\n    def switch(self):\n        ...\n    def next(self):\n        ...",
    "narration": "PairSwitching is a concrete Strategy subclass that implements a simple pair-rotation entry strategy on top of the framework’s base Strategy plumbing: PairSwitching.__init__ wires up the strategy parameters (rebalance_days, target_percent, lookback) and any small per-run state so the strategy can schedule periodic decisions, while PairSwitching.next is the per-bar driver that advances that schedule and triggers a rebalance when the configured interval is hit. The decision logic lives in PairSwitching.switch: it reads the two data feeds’ historical closes using Backtrader’s time-indexed series, computes log returns over the configured lookback (using the numeric utilities already available to the project), compares the two instruments and then directs the broker to move exposure wholly onto the higher-return leg by issuing order_target_percent calls that set the winner to the configured target_percent and the loser to zero; the method also emits log entries recording the action. Data flow is straightforward: price series come from the data layer feeds attached to the Strategy, PairSwitching computes a lookback return for each feed, and the resulting allocation targets are sent to the broker via the Strategy order APIs. In terms of control flow, the happy path is the scheduled rebalance that calls switch and places two target-percent orders; edge behavior is handled implicitly by Backtrader’s indexing when accessing prior bars and by the base Strategy order lifecycle for order state and notifications. Conceptually, PairSwitching is simpler than NCAV or MeanReversion because it relies solely on price-based relative performance between two instruments rather than cross-sectional ranking or fundamental filters, and it resembles LeveragedEtfPair in being a pair-focused allocator but differs by toggling exposure between the pair rather than applying predefined leverages to both legs."
  },
  {
    "open-file": "backtest/algos/PairSwitching.py",
    "range": {
      "start": {
        "line": 4,
        "character": 4
      },
      "end": {
        "line": 8,
        "character": 5
      }
    },
    "code": "    params = {\n        'rebalance_days': 21,\n        'target_percent': 0.95,\n        'lookback': 60\n    }",
    "narration": "The params attribute declares three run-time knobs that shape how PairSwitching behaves: rebalance_days controls the cadence of decisioning so the strategy only attempts a rotation every set number of bars (the default 21 implements an approximate monthly rebalance), target_percent is the portfolio exposure the strategy will seek for the chosen leg when the switch helper issues orders (the default 0.95 targets ninety-five percent allocation toward the selected instrument and flows through the same order_target_percent execution path the framework and NCAV.rebalance use), and lookback defines the length of historical data the strategy consults when computing its pair comparison metric (the default 60 causes the switch logic to base decisions on a 60-bar history, typically using numpy-backed operations to compute ratios or rolling statistics). These defaults match the common PairSwitching variant in the repo; compared to the other params variants, one alternative adds a leverages vector and uses a negative target_percent to invert exposure for a leveraged/short-oriented run, while another variant shortens lookback to 21 for a faster, higher-frequency rotation."
  },
  {
    "open-file": "backtest/algos/PairSwitching.py",
    "range": {
      "start": {
        "line": 9,
        "character": 4
      },
      "end": {
        "line": 11,
        "character": 84
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)\n        assert len(self.datas) == 2, \"Exactly 2 datafeeds needed for this strategy!\"",
    "narration": "PairSwitching.__init__ first delegates to base.Strategy.__init__ so the strategy inherits the base backtesting bookkeeping and helpers — the order tracking, last executed prices/commissions, the order_rejected flag, and the verbose logging behavior provided by Strategy.__init__. After that, PairSwitching.__init__ enforces that exactly two datafeeds are attached by asserting the datafeed count, because the pair-rotation logic and the per-bar driver in PairSwitching.next assume a binary pair: all ratio calculations, comparisons, and the switch helper operate on two instruments only. This early assertion fails fast on misconfiguration, preventing the later scheduling and order logic from running against an unsupported number of feeds. The pattern mirrors LeveragedEtfPair.__init__, which also calls base.Strategy.__init__ and asserts two feeds before adding pair-specific state."
  },
  {
    "open-file": "backtest/algos/PairSwitching.py",
    "range": {
      "start": {
        "line": 12,
        "character": 4
      },
      "end": {
        "line": 22,
        "character": 89
      }
    },
    "code": "    def switch(self):\n        prev0 = self.data0.close[-self.params.lookback]\n        prev1 = self.data1.close[-self.params.lookback]\n        return0 = np.log(self.data0.close[0]) - np.log(prev0)\n        return1 = np.log(self.data1.close[0]) - np.log(prev1)\n        if return0 > return1:\n            self.order_target_percent(data=self.data1, target=0)\n            self.order_target_percent(data=self.data0, target=self.params.target_percent)\n        else:\n            self.order_target_percent(data=self.data0, target=0)\n            self.order_target_percent(data=self.data1, target=self.params.target_percent)",
    "narration": "PairSwitching.switch reads the close series from the two attached feeds, uses the configured lookback to pick the past price for each instrument, and computes a simple log-return from that lookback point to the current bar using numpy’s log arithmetic. It then compares the two log-returns: if the first feed has outperformed the second it zeroes the target on the second feed and sends a target-percent order for the strategy-level target_percent to the first feed; otherwise it does the opposite. Those order_target_percent calls communicate desired allocations to the execution layer the same way NCAV.rebalance does, so switch implements a one-step rotation decision (one main comparison branch and an else fallback) that the PairSwitching.next scheduler invokes periodically while respecting the Strategy.order guard. The comparison is intentionally done with log returns for numerical stability and to make the decision purely relative over the specified lookback window."
  },
  {
    "open-file": "backtest/algos/PairSwitching.py",
    "range": {
      "start": {
        "line": 23,
        "character": 4
      },
      "end": {
        "line": 30,
        "character": 39
      }
    },
    "code": "    def next(self):\n        if self.order:\n            return\n        if (len(self) - 1) % self.params.rebalance_days == 0:\n            self.switch()\n        elif self.order_rejected:\n            self.switch()\n            self.order_rejected = False",
    "narration": "PairSwitching.next is the per-bar driver that decides when to invoke the pair-rotation logic encapsulated in PairSwitching.switch and it follows the same scheduling and safety conventions established by the framework’s base Strategy and the other strategy implementations you’ve already seen. Like NCAV.next, LeveragedEtfPair.next, and EqualVolatility.next it first checks the strategy bookkeeping for an outstanding order and returns immediately if one exists, preventing overlapping executions and preserving the inherited order state. On the regular schedule it uses the strategy’s rebalance_days to decide when to act; here the schedule test subtracts one from the current bar count before applying the modulus so the rebalance cadence aligns with the intended bar index (this is why the arithmetic differs slightly from the other strategies that use the raw length). When the schedule fires, PairSwitching.next calls PairSwitching.switch to compute returns across the two feeds and issue the order_target_percent calls that rotate exposure between the pair. If an earlier attempt produced an order rejection, PairSwitching.next also calls switch as a recovery path and then clears the order_rejected flag so the strategy can resume normal scheduling on subsequent bars."
  },
  {
    "create-file": "backtest/algos/WeightedHold.py"
  },
  {
    "open-file": "backtest/algos/WeightedHold.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "from . import BaseStrategy as base\nimport numpy as np",
    "narration": "WeightedHold brings in the project’s Strategy scaffolding by importing BaseStrategy (available under the local alias base) and also pulls in numpy for numerical utilities. BaseStrategy supplies the backtesting lifecycle, bookkeeping, and order/state helpers that every concrete strategy uses — WeightedHold relies on that foundation to call the shared buy_and_hold behavior and to issue order targets via the base strategy plumbing. numpy is present to perform the lightweight numeric work the wrapper needs: normalizing and manipulating the target-weight vectors that get mapped to the attached data feeds, handling any array-level computations or NaNs efficiently, and doing per-bar arithmetic without looping in Python. This mirrors the pattern used in PairSwitching and other concrete strategies that import the same base scaffolding and add numpy when they need elementwise or vector math; here the minimal import set reflects WeightedHold’s role as a thin adapter that maps configured weights into base strategy execution."
  },
  {
    "open-file": "backtest/algos/WeightedHold.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 29,
        "character": 27
      }
    },
    "code": "class WeightedHold(base.Strategy):\n    params = (\n        ('kwargs', None),\n        ('target_percent', 0.99),\n    )\n\n    def __init__(self):\n        ...\n\n    def buy_and_hold(self):\n        ...\n\n    def next(self):\n        ...",
    "narration": "WeightedHold is a thin Strategy subclass whose job in the framework is to enforce a static, per-feed allocation rather than computing targets from market signals; it sits at the entry point level and leverages the base.Strategy plumbing for lifecycle and order handling. On construction the strategy wires up two configurable parameters — a kwargs passthrough for run-level compatibility and a target_percent that represents the total fraction of portfolio capital to deploy — and then immediately calls the buy_and_hold routine to establish initial positions. buy_and_hold walks the attached data feeds in order, multiplies the strategy-level target_percent by the pre-supplied weight for each feed, and sends that computed fraction to the execution layer via order_target_percent so the broker moves the portfolio toward those per-feed targets. next simply calls buy_and_hold on every bar so allocations are refreshed continuously; because the logic directly issues target-percent orders, the control flow is straightforward: compute desired percent from target_percent and weights, then call the execution API to converge holdings to those targets. Compared with NCAV.rebalance, which derives targets from a screening and equal-weights selected longs, WeightedHold is deliberately simpler — it applies a static weight vector across all feeds (much like BuyAndHold but splitting by explicit weights rather than equal shares) and relies on the base Strategy bookkeeping and notify_order/log helpers for runtime order state and diagnostics."
  },
  {
    "open-file": "backtest/algos/WeightedHold.py",
    "range": {
      "start": {
        "line": 6,
        "character": 4
      },
      "end": {
        "line": 9,
        "character": 5
      }
    },
    "code": "    params = (\n        ('kwargs', None),\n        ('target_percent', 0.99),\n    )",
    "narration": "Inside WeightedHold the params tuple declares the strategy’s configurable defaults that the project’s Strategy plumbing will pick up during initialization: it provides a placeholder named kwargs initialized to None and sets target_percent to 0.99. In the context of the backtesting framework that value is the strategy-level desired portfolio exposure that buy_and_hold will use to compute per-feed targets and then drive order_target_percent calls so positions are established and maintained at roughly 99% of capital. The kwargs entry acts as a flexible passthrough for any extra runtime options the wrapper or higher-level runner might supply without changing the explicit parameter list. This follows the same tuple-of-parameter convention used by other Strategy subclasses such as the earlier variant that only exposed target_percent with the same 0.99 default, whereas an alternative style elsewhere uses a dict-style params and a slightly different default target_percent of 0.95; the practical effect is simply different default allocation behavior and an optional slot for extra arguments in WeightedHold."
  },
  {
    "open-file": "backtest/algos/WeightedHold.py",
    "range": {
      "start": {
        "line": 11,
        "character": 4
      },
      "end": {
        "line": 21,
        "character": 107
      }
    },
    "code": "    def __init__(self):\n        base.Strategy.__init__(self)\n\n        if self.params.kwargs:\n            self.params.weights = [float(w) for w in self.params.kwargs]\n        else:\n            self.params.weights = [1 for d in self.datas]\n\n        w_pos = [w for w in self.params.weights if w >= 0]\n        w_neg = [w for w in self.params.weights if w < 0]\n        self.weights = [(w / sum(w_pos)) if w in w_pos else (-w / sum(w_neg)) for w in self.params.weights]",
    "narration": "WeightedHold.__init__ first invokes the base Strategy initializer to get the framework bookkeeping and order/state fields set up (the same initialization other strategies use so things like order tracking and verbose logging behave consistently). It then builds the per-feed weight vector that buy_and_hold will use: if the caller supplied a kwargs weight list, those values are cast to floats; otherwise a default list of ones is created with one entry per attached data feed to represent equal raw weights. The initializer then partitions that raw weight list into non‑negative and negative groups and normalizes each sign group independently so that within the longs the proportions sum to unity and within the shorts the proportions sum to unity; negative entries are converted into their normalized magnitudes. The resulting self.weights attribute therefore contains per-feed positive allocation fractions (normalized within their original sign group) which buy_and_hold and next read to compute each feed’s target percent of the portfolio. This logic ensures user-provided signed weights are converted into stable, comparable allocation shares while leveraging the Strategy base setup already covered."
  },
  {
    "open-file": "backtest/algos/WeightedHold.py",
    "range": {
      "start": {
        "line": 23,
        "character": 4
      },
      "end": {
        "line": 26,
        "character": 61
      }
    },
    "code": "    def buy_and_hold(self):\n        for i, d in enumerate(self.datas):\n            split_target = self.params.target_percent * self.weights[i]\n            self.order_target_percent(d, target=split_target)",
    "narration": "WeightedHold.buy_and_hold is the small, deterministic allocation routine that enforces the strategy-level targets by iterating over the attached data feeds and issuing order targets to the broker. It relies on the weights prepared in WeightedHold.__init__ — those per-feed weights live on self.params.weights — and for each feed it computes a feed-level target as the strategy-wide target_percent scaled by that feed’s weight, then hands that target to the execution layer via order_target_percent so the broker moves the position toward the desired exposure. The loop uses the feed index to align each weight with the corresponding data feed, so the mapping is positional (feed order → weight). Because it is invoked both during initialization and on every next, it both establishes the initial allocation and refreshes it each bar. Conceptually this mirrors the equal-split behavior in BuyAndHold but replaces the equal division with explicit per-feed weighting; any negative or nonstandard weights produced in __init__ flow directly into the target_percent sent to the broker. The method depends on the Strategy bookkeeping set up by Strategy.__init__ (order tracking and related state) so order_target_percent integrates with the framework’s existing order lifecycle."
  },
  {
    "open-file": "backtest/algos/WeightedHold.py",
    "range": {
      "start": {
        "line": 28,
        "character": 4
      },
      "end": {
        "line": 29,
        "character": 27
      }
    },
    "code": "    def next(self):\n        self.buy_and_hold()",
    "narration": "WeightedHold.next is the per-bar entry point Backtrader calls to drive the strategy’s decisions; its entire responsibility is to invoke buy_and_hold so the strategy enforces its target allocations on every incoming bar. Because WeightedHold.__init__ has already prepared the weights list and the strategy-level target_percent, buy_and_hold iterates the attached data feeds, computes each feed’s fractional allocation by multiplying the strategy target_percent by the corresponding weight, and issues an order_target_percent call for every feed so the execution layer moves the portfolio toward those weighted targets. Unlike NCAV.next, which contains guards around outstanding orders and scheduling logic, WeightedHold.next does not perform any gating or state checks itself — it simply refreshes the allocation each bar by delegating to buy_and_hold, making it a minimal wrapper that continuously reasserts the configured weight-based portfolio targets."
  },
  {
    "create-file": "tools/stats.py"
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 9,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom scipy.stats import norm, laplace, t, levy_stable\nfrom scipy.stats import kstest, chisquare\nfrom . import fin_calc",
    "narration": "The file pulls in standard utility libraries for runtime and invocation concerns: the operating-system utilities to read or write files and the command-line argument parser so the analytics module can be executed or configured from the shell, and the datetime utilities to tag or align computed statistics with backtest timestamps. It brings in numpy and pandas to represent time series, compute vectorized returns and moments, and manipulate tabular results that will feed the reporting layer. For statistical modeling and distribution work it imports scipy and several concrete probability distributions — Gaussian, Laplace, Student t, and a stable-law family — because the module fits return series to parametric forms and needs access to PDF/CDF routines and fitting infrastructure. Goodness-of-fit routines are imported (Kolmogorov–Smirnov and chi-square test helpers) so the code can evaluate how well those distributions describe empirical returns when producing the consolidated stats used by reports. Finally, it references the local fin_calc helper so domain-specific financial metrics and transformations (the same kinds of calculations the rest of the backtest pipeline expects) are reused rather than reimplemented. Compared to other modules you’ve seen, this file follows the same pattern of using numpy/pandas and scipy, but it explicitly pulls in a wider set of distribution families and direct goodness-of-fit functions and omits plotting or time-series testing packages like statsmodels; that reflects its role as a numerical/risk analytics engine rather than a visualization or stationarity-testing component. Remember WeightedHold and the strategy outputs it generates — those return series are the primary inputs here, and these imports provide the numeric, statistical, and I/O tools needed to turn strategy returns into the moments, fitted distributions, VaR/CVaR, and simulation summaries that get aggregated by get_stats for reporting."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 10,
        "character": 0
      },
      "end": {
        "line": 17,
        "character": 51
      }
    },
    "code": "def get_returns(data):\n    total_returns = (data.iloc[-1] - data.iloc[0]) / data.iloc[0]\n    annual_returns = (1 + total_returns) ** (255 / len(data)) - 1\n    returns = fin_calc.log_returns(data)\n    print('\\n  Returns:')\n    print(f'   - total:\\t{round(total_returns * 100,2)}%')\n    print(f'   - annual:\\t{round(annual_returns * 100, 2)}%')\n    return (returns, total_returns, annual_returns)",
    "narration": "get_returns takes a price series produced by the data layer and turns it into the basic return metrics the reporting layer expects: it first computes a cumulative simple return over the sample by comparing the last and first price points and then converts that cumulative return into an annualized return using a trading-day scale (the code uses 255 as the trading-day proxy and raises the gross return to the appropriate power to annualize), and it also delegates to log_returns to produce the series of log returns used for downstream statistical work. The function emits concise console output showing the total and annualized returns as percentages so get_stats (which iterates over each dataset and drives the rest of the analysis) can present immediate summary numbers, and it returns a tuple containing the log-return series, the cumulative/simple total return, and the annualized return so later steps such as get_moments, get_simulations, get_risk, and get_fit can operate on those values. The choice to return log returns (computed by log_returns via the log-difference and dropping the initial NaN) supports the project’s analytical goals of using additive, stationary increment series for moments, simulations, and distribution fitting, while the printed totals provide a quick human-readable check during reporting runs."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 18,
        "character": 0
      },
      "end": {
        "line": 25,
        "character": 18
      }
    },
    "code": "def get_moments(returns):\n    moments = scipy.stats.describe(returns)\n    print('\\n  Moments:')\n    print(f'   - mean:\\t{round(moments.mean, 5)}')\n    print(f'   - std:\\t{round(np.sqrt(moments.variance), 5)}')\n    print(f'   - skew:\\t{round(moments.skewness, 5)}')\n    print(f'   - kurt:\\t{round(moments.kurtosis, 5)}')\n    return moments",
    "narration": "Within the reporting pipeline that consolidates strategy results, get_moments is the simple analyzer that turns a returns series into a compact set of descriptive statistics and prints them for human inspection. It takes the returns vector (the log returns produced upstream) and asks SciPy to describe the distribution, receiving back the standard describe result that includes mean, variance, skewness, and kurtosis. The function then prints a short \"Moments\" block: it reports the mean directly, computes standard deviation by taking the square root of the reported variance, and prints skewness and kurtosis; each printed value is rounded to five decimal places for concise console output. Finally, get_moments returns the SciPy describe object so downstream callers can programmatically access the same statistics if needed. There is no branching or error handling; the function’s purpose is purely to centralize and standardize the moment estimation and provide a readable console summary as part of the larger get_stats workflow."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 26,
        "character": 0
      },
      "end": {
        "line": 43,
        "character": 22
      }
    },
    "code": "def get_simulations(returns):\n    simulation_size = 100000\n    sim_index = ['normal', 'laplace', 'student-t', 'levy-stable']\n    sim_list = [norm, laplace, t, levy_stable]\n    assert len(sim_index) == len(sim_list), 'Mismatch lengths'\n    simulations = {}\n    for name, sim in zip(sim_index, sim_list):\n        fit_params = []\n        if name == 'levy-stable':\n            def pconv(\n                alpha, beta, mu, sigma): return(\n                    alpha, beta, mu - sigma * beta * np.tan(np.pi * alpha / 2.0), sigma)\n            fit_params = pconv(*sim._fitstart(returns))\n        else:\n            fit_params = sim.fit(returns)\n        rvs = pd.Series(sim.rvs(*fit_params, size=simulation_size))\n        simulations[name] = {'sim': sim, 'rvs': rvs, 'params': fit_params}\n    return simulations",
    "narration": "get_simulations produces Monte Carlo samples from a handful of candidate parametric distributions by fitting each distribution to the empirical log returns and returning the fitted distribution objects, their fitted parameters, and a large vector of simulated draws for downstream risk and goodness-of-fit routines. It sets a fixed simulation_size of one hundred thousand, enumerates the distribution names and the corresponding scipy.stats distribution objects (normal, Laplace, Student-t, and Lévy-stable), and asserts the two lists line up. Then it iterates over the paired names and distribution objects: for the normal, Laplace, and Student-t it fits the distribution directly to the provided returns to obtain fit parameters; for the Lévy-stable distribution it uses the distribution’s internal fit-start routine and then applies a location re-centering transformation (the offset that depends on alpha, beta, sigma and the tangent of pi*alpha/2) because of the stable family’s parameterization idiosyncrasy. With the fitted parameters in hand it generates simulation_size random variates from the fitted distribution, wraps those draws in a pandas Series, and stores a record under the distribution name containing the distribution object, the simulated draws, and the fitted parameters. The returned simulations dictionary is designed to feed get_risk and get_fit so the framework can compute simulated VaR/CVaR and perform Kolmogorov–Smirnov style goodness-of-fit checks against the historical returns."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 44,
        "character": 0
      },
      "end": {
        "line": 58,
        "character": 18
      }
    },
    "code": "def get_risk(returns, sims):\n    print('\\n  Risk:')\n    confidence_level = .05\n    var = fin_calc.var(returns, confidence_level)\n    cvar = fin_calc.cvar(returns, var)\n    risk_values = [[var, cvar]]\n    for key in sims:\n        rvs = sims[key]['rvs']\n        sim_var = fin_calc.var(rvs, confidence_level)\n        sim_cvar = fin_calc.cvar(rvs, sim_var)\n        risk_values.append([sim_var, sim_cvar])\n    risk_columns = ['VAR', 'cVAR']\n    risk_df = pd.DataFrame(risk_values, columns=risk_columns, index=['historical', *sims.keys()])\n    print(risk_df)\n    return risk_df",
    "narration": "get_risk takes the return series produced earlier by get_returns and the simulations dictionary produced by get_simulations, prints a short header, and then computes a standardized set of tail-risk metrics that the reporting layer can consume. It sets a 5% confidence level and first computes the historical value-at-risk by delegating to fin_calc.var and then the conditional value-at-risk by delegating to fin_calc.cvar (fin_calc.var uses a low quantile as the VaR and fin_calc.cvar averages returns below that VaR). It then iterates over each fitted simulation in sims, extracts the simulated return variates, computes the simulation VaR and cVaR the same way, and accumulates those pairs alongside the historical pair. Finally it assembles a pandas DataFrame with columns labeled VAR and cVAR and an index whose first row is labeled historical followed by the simulation names, prints that table to the console, and returns it. The function therefore centralizes the comparison of empirical tail risk against model-implied tails so get_stats can present a concise risk summary when verbose output is enabled."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 59,
        "character": 0
      },
      "end": {
        "line": 64,
        "character": 81
      }
    },
    "code": "def get_fit(data, sims):\n    print('\\n  Goodness of Fit: (p-value > 0.05)')\n    for key in sims:\n        sim = sims[key]\n        ks = kstest(data, lambda x, s=sim: s['sim'].cdf(x, *s['params']))\n        print(f'\\t{key}: \\t{ks.pvalue >= 0.05}\\t(p-value {round(ks.pvalue, 5)})')",
    "narration": "get_fit is the small reporting routine that, when invoked by get_stats in verbose mode, checks whether each parametric simulation produced by get_simulations is a plausible generator of the empirical returns produced by get_returns. It begins by announcing a goodness-of-fit check, then iterates over the sims dictionary; for each entry it extracts the fitted distribution object and the fitted parameter tuple that get_simulations stored. It then runs a Kolmogorov–Smirnov test (kstest) using the empirical returns as the sample and the simulation's cumulative distribution function constructed with those fitted parameters as the reference distribution. The function interprets the resulting p-value against the conventional 0.05 threshold, printing a boolean indicating whether the null hypothesis (that the returns come from that fitted distribution) is not rejected and also printing the rounded p-value. Data flows in from get_returns as the data argument and from get_simulations as sims, and the only side effect is console output summarizing which simulated distributions pass the 5% goodness-of-fit test; there is no mutation of inputs and no additional control-flow branching beyond the loop over simulations."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 65,
        "character": 0
      },
      "end": {
        "line": 73,
        "character": 41
      }
    },
    "code": "def get_stats(datas, verbose=False):\n    for data in datas:\n        print(f'--- {data.name} --- ({data.index[0].date()}, {data.index[-1].date()})')\n        returns, total_returns, annual_returns = get_returns(data)\n        get_moments(returns)\n        if verbose:\n            simulations = get_simulations(returns)\n            get_risk(returns, simulations)\n            get_fit(returns, simulations)",
    "narration": "get_stats is the orchestration entry point the reporting layer calls to produce a compact performance and risk summary for each backtest data feed: it iterates over the supplied datas list, prints a short header with the feed name and date range, and then sequences the key analytics helpers. For each feed it calls get_returns to compute and print total and annualized performance and to produce the log returns series used downstream; it then calls get_moments to compute and print distributional moments for those log returns. If verbose is enabled, get_stats proceeds to the heavier analyses by calling get_simulations to fit several candidate distributions and generate large simulated samples, then passes those simulations and the empirical returns into get_risk to compute VaR and CVaR comparisons, and into get_fit to run goodness-of-fit tests (KS) between the empirical series and the fitted distributions. Data flows in a simple pipeline: the per-feed price series enters get_returns, which yields the returns series that feeds get_moments and, conditionally, get_simulations; the simulations produced are consumed by get_risk and get_fit. The function’s primary side effect is console output; structurally it serves as a lightweight facade/coordinator that composes the lower-level analytics functions into the consolidated statistics summary the reporting/analysis layer expects."
  },
  {
    "open-file": "tools/stats.py",
    "range": {
      "start": {
        "line": 74,
        "character": 0
      },
      "end": {
        "line": 91,
        "character": 41
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('tickers', nargs='+')\n    PARSER.add_argument('-v', '--verbose', action=\"store_true\")\n    PARSER.add_argument('-s', '--start', nargs=1, type=int)\n    PARSER.add_argument('-e', '--end', nargs=1, type=int)\n    ARGS = PARSER.parse_args()\n    TICKERS = ARGS.tickers\n    START = ARGS.start or [1900]\n    END = ARGS.end or [2100]\n    START_DATE = datetime(START[0], 1, 1)\n    END_DATE = datetime(END[0], 1, 1)\n    DATA = []\n    for ticker in TICKERS:\n        datapath = os.path.join(os.path.dirname(__file__), f'../data/price/{ticker}.csv')\n        ticker_data = pd.read_csv(datapath, index_col='Date', parse_dates=True)['Adj Close'].rename(ticker)\n        DATA.append(ticker_data.loc[START_DATE: END_DATE])\n    get_stats(DATA, verbose=ARGS.verbose)\n",
    "narration": "This entry-point parses command-line arguments to turn a list of ticker symbols and optional start/end years into a set of price series that feed into get_stats for the reporting layer. It reads the required positional tickers and a verbose flag, accepts optional single-year start and end parameters, and substitutes broad default years when those options are omitted so the date window effectively spans the entire dataset by default. For each ticker it builds a path into the project's local price store relative to the module, loads the adjusted close column into a pandas Series, renames that series to the ticker symbol, and trims it to the requested date range using the constructed start and end datetimes (January 1 of the supplied years). The trimmed series are collected into a list named DATA and then handed to get_stats along with the verbose setting so the centralized performance analytics compute the summary used by the reporting/analysis layer. This mirrors the project's other CSV-reading utilities such as clean_tickers, but whereas clean_tickers only filters tickers based on coverage, this code actually materializes the per-ticker series and slices them before invoking the analytics pipeline."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 9,
        "character": 1
      }
    },
    "code": "PATHS = {\n    'BALANCE': './us-balance-quarterly.csv',\n    'CASHFLOW': './us-cashflow-quarterly.csv',\n    'INCOME': './us-income-quarterly.csv'\n}",
    "narration": "PATHS is a module-level dictionary constant that centralizes the on-disk locations of the three quarterly US statement CSV caches by mapping the logical statement keys BALANCE, CASHFLOW, and INCOME to their respective filenames (the us-balance-quarterly.csv, us-cashflow-quarterly.csv, and us-income-quarterly.csv files). Its job is to serve as the single source of truth for where the file-backed fundamentals live so load_file and the higher-level accessors like all_info and load_info can resolve and open the right CSVs consistently; load_file uses the PATHS entry for a requested statement type together with the module directory to locate and read the file into pandas, and all_info concatenates the three statement tables returned via those loaders to produce combined fundamentals. Because this file-backed caching is how the data layer exposes validated, reusable balance sheet, income, and cashflow data to downstream consumers such as Robinhood.setup and NCAV, PATHS is the simple mapping that ties the logical statement names used across the module to the actual cached CSV artifacts on disk."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 12,
        "character": 0
      },
      "end": {
        "line": 17,
        "character": 19
      }
    },
    "code": "def load(path='./info.p'):\n    info_path = os.path.join(os.path.dirname(__file__), path)\n\n    with open(info_path, 'rb') as f:\n        info = pickle.load(f)\n        return info",
    "narration": "The load function is the simple file-backed cache reader that the data layer uses to retrieve previously persisted company fundamentals so they can be reused by consumers like Robinhood.setup and small_cap. It resolves a module-relative filename by joining the module directory with the provided path (defaulting to a local pickled file), opens that file in binary read mode, and uses the pickle machinery to reconstruct the Python object that was stored there; the reconstructed object (typically a mapping of tickers to their balance sheet, income, and cashflow data) is returned to the caller. In the pipeline, small_cap takes the returned mapping and materializes it into a DataFrame for screening and ratio calculations, while Robinhood.setup and other analytics consume the same cached fundamentals for provider initialization and metric computations. load complements the save function which writes the same pickled structure, and it differs from load_file which reads CSVs into DataFrames; load is a straightforward, module-relative unpickle step with file I/O as its only side effect."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 20,
        "character": 0
      },
      "end": {
        "line": 23,
        "character": 62
      }
    },
    "code": "def save(info, path='./info.p'):\n    info_path = os.path.join(os.path.dirname(__file__), path)\n    with open(info_path, 'wb') as f:\n        pickle.dump(info, f, protocol=pickle.HIGHEST_PROTOCOL)",
    "narration": "The save function is the simple writer that persists the in-memory fundamentals snapshot so the rest of the backtesting stack can reuse validated company statements across runs: it takes the info structure handed to it (the combined balance, income, and cashflow data the data layer produces) and serializes it to a file named info.p by default, resolving that filename relative to the module’s directory so the cache lives next to the utility. It opens the target file for binary writing and writes the serialized object using Python’s pickle with the highest available protocol, which produces a compact binary representation; because it opens the file for write, an existing cache file will be replaced. save is a leaf utility invoked by save_info, and its output is what the load function later reopens and deserializes for consumers like Robinhood.setup and the NCAV analytics, enabling the framework’s separation of data acquisition and strategy/reporting by providing a reusable, file-backed fundamentals cache."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 30,
        "character": 0
      },
      "end": {
        "line": 38,
        "character": 36
      }
    },
    "code": "def load_info(info_type, ticker, date=None):\n    data = load_file(info_type)\n    t_data = data[data.Ticker == ticker]\n\n    if date is None:\n        return t_data\n    else:\n        row_label = (pd.to_datetime(t_data['Report Date']) > date).idxmax()\n        return t_data.loc[row_label]",
    "narration": "load_info is the centralized per-statement accessor that the balance, cashflow, and income helpers call to retrieve a ticker's quarterly rows from the file-backed fundamentals cache. It begins by delegating to load_file (which reads the appropriate CSV from the PATHS mapping) to get the full DataFrame for the requested statement type, then filters that DataFrame down to rows whose Ticker matches the supplied ticker. If no date is passed, load_info simply returns that filtered DataFrame so callers can iterate over the historic reports. If a date is supplied, load_info converts the Report Date values to timestamps, finds the first report whose date is strictly after the supplied cutoff, and returns that single row for the caller; this is how callers can ask for the nearest subsequent quarter (the behavior the unit tests exercise). In the project architecture this function sits in the mid-level data layer: it shapes on-disk CSV caches into per-ticker, optionally date-scoped records that analytics and data providers like Robinhood.setup and NCAV consume, and the all_info/all_balance helpers build on the same load_file/load_info pattern to produce combined or multi-ticker views."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 41,
        "character": 0
      },
      "end": {
        "line": 42,
        "character": 45
      }
    },
    "code": "def balance(ticker, date=None):\n    return load_info('BALANCE', ticker, date)",
    "narration": "As the company-fundamentals accessor for balance-sheet data, balance takes a ticker and an optional date and simply delegates to load_info asking for the BALANCE statement. load_info in turn uses load_file (which reads the CSV named in PATHS for the requested statement) to get the on-disk quarterly data, filters that table down to rows for the given ticker, and then either returns the full per-ticker quarterly DataFrame when no date is supplied or converts the Report Date column to datetimes and returns the single row whose report date is the first one after the supplied date. This lets callers request either the full historical balance-sheet series for a single company or a single snapshot closest after a point in time; all_info composes balance with the matching cashflow and income wrappers to produce combined views, and tests like TestInfo.test_balance exercise both the series- and snapshot-style returns. balance is therefore a thin, named accessor that centralizes how the BALANCE CSV from PATHS is exposed to the rest of the data layer and analytics."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 52,
        "character": 0
      },
      "end": {
        "line": 53,
        "character": 46
      }
    },
    "code": "def cashflow(ticker, date=None):\n    return load_info('CASHFLOW', ticker, date)",
    "narration": "cashflow is a tiny, purpose-built accessor that callers use when they need a company's quarterly cash flow statement; it accepts a ticker and an optional cutoff date and simply delegates to load_info asking for the CASHFLOW dataset. callers like all_info, TestInfo.test_cashflow, and other data-provider entry points hand cashflow a ticker (and sometimes a date) and receive either the full per-company DataFrame of quarterly rows or a single report row when a date is provided. load_info handles the heavier lifting: it reads the underlying CSV via load_file, filters to the requested ticker, and then either returns all rows or selects the first report whose Report Date is strictly after the supplied date; cashflow exists so callers don’t have to repeat the info_type literal or the filtering logic. balance and income follow the same thin-wrapper pattern, so cashflow fits a consistent, symmetric interface for per-statement access across the three statement types; when all_info composes per-ticker sets it uses these wrappers to build a combined table and then removes duplicated columns. In short, cashflow is a focused convenience layer that standardizes how the rest of the system fetches validated, file-backed cash flow data for a ticker or a specific point in time."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 56,
        "character": 0
      },
      "end": {
        "line": 57,
        "character": 44
      }
    },
    "code": "def income(ticker, date=None):\n    return load_info('INCOME', ticker, date)",
    "narration": "income is a thin, statement-specific accessor that serves the project's need for a centralized, file-backed way to fetch income statement fundamentals for a ticker so downstream pieces like Robinhood.setup and NCAV can rely on a consistent dataset. It simply delegates to load_info asking for the INCOME statement; load_info in turn uses load_file (which reads the CSV named by the PATHS constant you already saw) to load the quarterly statements, filters the table to rows for the requested ticker, and then returns either the entire per-ticker DataFrame when no date is supplied or a single report row corresponding to the first Report Date after the supplied date. In practice income behaves exactly like balance and cashflow, providing the per-statement view that all_info later concatenates across BALANCE, CASHFLOW, and INCOME (deduping columns) for combined access, and the unit tests verify both the earliest rows and a date-scoped lookup return the expected Report Date and Net Income values."
  },
  {
    "open-file": "data/info/info.py",
    "range": {
      "start": {
        "line": 60,
        "character": 0
      },
      "end": {
        "line": 66,
        "character": 54
      }
    },
    "code": "def all_info(ticker=None):\n    if ticker is None:\n        info = pd.concat([load_file('BALANCE'), load_file('CASHFLOW'), load_file('INCOME')], axis=1)\n        return info.loc[:, ~info.columns.duplicated()]\n    else:\n        info = pd.concat([balance(ticker), cashflow(ticker), income(ticker)], axis=1)\n        return info.loc[:, ~info.columns.duplicated()]",
    "narration": "all_info is the centralized accessor that returns a combined view of the three quarterly US statements so callers can either fetch the entire cached universe or the consolidated fundamentals for a single ticker. If called with no ticker, it loads each statement file via load_file (which reads the CSVs referenced in PATHS) and horizontally concatenates those full tables, then removes any duplicated column names so the result is a single wide DataFrame keyed by the original row indices and with one copy of each column. If called with a specific ticker, it asks the per-statement helpers balance, cashflow, and income (each of which delegates to load_info to filter the file-backed data for that ticker and optionally a date) and concatenates those three per-ticker DataFrames side-by-side, again dropping duplicate columns before returning. The function therefore implements a simple guard-clause branching: a universe path that aggregates on-disk CSVs and a per-ticker path that aggregates filtered rows; its duplicate-column suppression protects against overlapping field names that would otherwise appear multiple times when statements are merged. TestInfo.test_all_info exercises both behaviors by asserting the total unique ticker count for the universe load and the expected number of rows when requesting AAPL."
  },
  {
    "create-file": "tools/download_info.py"
  },
  {
    "open-file": "tools/download_info.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 5,
        "character": 0
      }
    },
    "code": "import os\nimport asyncio\nimport pandas as pd\nfrom api import yahoo\nfrom data.info import info",
    "narration": "The imports set up the minimal toolkit this utility needs to fetch provider metadata and stitch it into the project’s persistent info store: os supplies filesystem primitives used to check paths, create directories, and manipulate files as the results are persisted; asyncio enables asynchronous execution so downloads can run off the main thread or in parallel (this ties directly to download_info, which dispatches yahoo.get_info into the event loop); pandas provides DataFrame utilities to normalize, merge, and write tabular company metadata once the raw provider dicts are returned; the yahoo module from api is the external-provider adapter the script calls to obtain instrument/company info (the same provider wrapper that download_info uses to fetch a single ticker); and info from data.info is the project-level persistence/metadata abstraction used to integrate fetched records into the local cache the rest of the framework consumes. This combination reflects the overall architecture’s separation of concerns—api.* handles data acquisition while data.* handles on-disk caching—similar to other modules that import provider wrappers or yfinance directly, but here the presence of asyncio and the data.info import emphasizes concurrent fetching and standardized persistence rather than command-line parsing or direct yfinance usage seen in some other import lists."
  },
  {
    "open-file": "tools/download_info.py",
    "range": {
      "start": {
        "line": 6,
        "character": 0
      },
      "end": {
        "line": 8,
        "character": 67
      }
    },
    "code": "async def download_info(ticker):\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, yahoo.get_info, ticker)",
    "narration": "download_info is the small asynchronous adapter the data acquisition layer uses to let the batch runner drive many vendor fetches concurrently: it takes a ticker and schedules the synchronous yahoo.get_info routine to run off the event loop so the overall process doesn't block. Concretely, download_info grabs the current asyncio event loop and uses the loop's executor facility to run yahoo.get_info on the default thread pool, then awaits that background work and returns whatever yahoo.get_info produced (the combined info/financials structure or None on failure). By not catching exceptions itself, download_info lets errors from yahoo.get_info surface to the caller, so the download_all_info orchestration that creates tasks for multiple tickers will observe success or failure when it collects task results. This function therefore acts as an adapter that bridges the blocking provider call into the project's async batching pattern used by download_all_info, enabling concurrent downloads while keeping the provider code unchanged."
  },
  {
    "open-file": "tools/download_info.py",
    "range": {
      "start": {
        "line": 9,
        "character": 0
      },
      "end": {
        "line": 17,
        "character": 20
      }
    },
    "code": "def download_all_info(tickers):\n    event_loop = asyncio.new_event_loop()\n    coroutines = [download_info(t) for t in tickers]\n    tasks = [event_loop.create_task(c) for c in coroutines]\n    print(\"Tasks created, running event loop:\")\n    event_loop.run_until_complete(asyncio.wait(tasks))\n    info_dict = {ticker: task.result() for (ticker, task) in zip(tickers, tasks)}\n    event_loop.close()\n    return info_dict",
    "narration": "download_all_info is the synchronous entry routine that takes a list of tickers, spins up a dedicated asyncio event loop, and orchestrates concurrent metadata/fundamentals fetches for each symbol so the results can be persisted into the project’s data store. It builds a collection of download_info coroutines (download_info was shown earlier and delegates to the blocking yahoo.get_info call by running it in an executor), schedules each coroutine as a task on the new event loop, prints a short status message to the console, and runs the loop until all scheduled tasks finish. After completion it pairs each ticker with its corresponding task’s result in order to produce a dictionary mapping tickers to their downloaded info, closes the event loop, and returns that dictionary for downstream saving; if any task raised an exception, retrieving the task result will propagate that error up to the caller. Compared with save_all, which batches historical price downloads and writes CSVs, download_all_info focuses on parallelizing per-symbol info fetches by creating explicit tasks and using a fresh event loop from a synchronous context."
  },
  {
    "open-file": "tools/download_info.py",
    "range": {
      "start": {
        "line": 18,
        "character": 0
      },
      "end": {
        "line": 19,
        "character": 24
      }
    },
    "code": "def save_info(info_dict):\n    info.save(info_dict)",
    "narration": "Within the data pipeline that gathers instrument and company metadata, save_info is the small, public persistence entry point that the download utilities call to persist the collected info dictionary. It takes the info_dict produced by download_info or download_all_info and immediately hands it off to the lower-level save routine; save performs the actual file I/O by serializing the object with pickle using the highest protocol and writing it into the module’s local info cache file. There is no transformation or branching in save_info—its purpose is to provide a stable, simple API for higher-level code to store fetched metadata so the complementary load function and consumers like Robinhood.setup and small_cap can later retrieve the cached fundamentals for reuse."
  },
  {
    "open-file": "tools/download_info.py",
    "range": {
      "start": {
        "line": 20,
        "character": 0
      },
      "end": {
        "line": 27,
        "character": 18
      }
    },
    "code": "if __name__ == '__main__':\n    ticker_csv_path = os.path.join(os.path.dirname(__file__), '../data/spy/tickers.csv')\n    tickers = pd.read_csv(ticker_csv_path, header=None)[1]\n    print('---- DOWNLOADING ----')\n    info_dict = download_all_info(tickers)\n    print('---- SAVING ----')\n    save_info(info_dict)\n    print('Done.')\n",
    "narration": "When executed as a standalone script the main guard first reads a local CSV that contains the target universe of tickers using pandas, extracting the column that holds the ticker symbols. It prints a visual marker to indicate the start of the download phase and then hands that ticker list to download_all_info; as previously explained, download_all_info orchestrates concurrent fetches by driving download_info (which in turn calls the yahoo provider) on an asyncio event loop and returns a mapping of ticker to the provider-returned metadata. After the concurrent fetch completes the script prints a saving marker and calls save_info to persist the aggregated info mapping into the project's info store so the rest of the framework can reuse the cached metadata, then prints a final completion message. The main guard ensures this workflow only runs when the module is executed directly and not when it is imported. This is similar to SpyTickers.download in that both consume the same local tickers CSV and produce a ticker list, but SpyTickers.download contains a fallback scrape-and-write behavior when the CSV is missing, whereas the script here assumes the CSV is present and proceeds directly to bulk fetch-and-save."
  },
  {
    "create-file": "api/robinhood.py"
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 6,
        "character": 0
      }
    },
    "code": "import os\nimport json\nimport pandas as pd\nfrom fast_arrow import StockMarketdata, OptionChain, Option\nfrom fast_arrow import Client as ClientLegacy\nfrom fast_arrow_auth import Client, User",
    "narration": "As with the other modules we stepped through, this file pulls in a small set of utilities to bridge provider responses into the framework’s local data pipeline: os is present for the same filesystem duties you’ve seen before, like any path checks and token or cache file manipulation; json is used to serialize or deserialize provider payloads and persisted auth state; and pandas supplies the tabular normalization utilities the rest of the pipeline expects when turning provider objects into DataFrames for persistence. The fast_arrow package supplies the domain models and helpers that this thin Robinhood client relies on: StockMarketdata, OptionChain, and Option give typed representations for quotes and option chains so the client can fetch and then normalize those market objects into the project’s dataset shape, while the package’s Client is imported under an alias to preserve access to the legacy client surface. Authentication is handled by fast_arrow_auth, which provides the modern Client and the User abstraction used during login/setup flows. Compared to the other Robinhood adapter in the repo that uses low-level HTTP tooling like requests and URL parsing, this file favors the provider SDK models and auth primitives, leaving any synchronous calls to be coordinated by the existing async orchestration utilities elsewhere in the project when needed."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 7,
        "character": 0
      },
      "end": {
        "line": 50,
        "character": 18
      }
    },
    "code": "class Robinhood:\n    def __init__(self):\n        ...\n    def setup(self):\n        ...\n    def login(self):\n        ...\n    def get_quote(self, symbol):\n        ...\n    def get_option_chain(self, symbol, stock):\n        ...\n    def get_next_3_exp_options(self, option_chain):\n        ...",
    "narration": "Robinhood is the thin provider client that the data layer uses to present Robinhood as a repository-style source of broker-style market data for the framework’s download/update and validation workflows. Its initializer and setup routines prepare the runtime environment and internal client state (handling things like any on-disk persistence or credential wiring so subsequent calls can be authenticated), and the login routine performs the actual authentication and client bootstrap so the rest of the class can make API calls. Once authenticated, get_quote acts as the simple adapter that returns a live market quote for a symbol by delegating to the market-data helper the project already uses elsewhere (the same pattern you saw in the StockMarketdata usage from the imports). get_option_chain is responsible for converting the higher‑level stock representation into the provider-specific identifier and then fetching the full option chain via the OptionChain fetch facility, mirroring the OptionChain.fetch pattern identified in the repository. get_next_3_exp_options is a convenience reader over an option chain: it looks at the chain’s expiration dates, excludes past expiries, orders the remaining dates, picks the next three upcoming expirations, and returns the option slices needed downstream so the download and validation routines can pull specific strikes/series; its control flow therefore handles the happy path of returning three future expirations and the edge cases where fewer are available by returning whatever upcoming expirations exist. Conceptually Robinhood encapsulates authentication, identifier translation, and read operations so the rest of the data pipeline—download_info, download_all_info, save_info and the all_info accessors you’ve already seen—can request quotes and option data from a uniform interface without knowing provider-specific details."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 8,
        "character": 4
      },
      "end": {
        "line": 12,
        "character": 40
      }
    },
    "code": "    def __init__(self):\n        username, password, device_token = self.setup()\n        self.username = username\n        self.password = password\n        self.device_token = device_token",
    "narration": "As the thin Robinhood client entry point in the data layer, Robinhood.__init__ bootstraps the object by calling setup to obtain the username, password, and device token from the local configuration and then stores those three values on the instance as username, password, and device_token. setup performs file I/O to read the local config and returns a tuple of credentials and device token, and __init__ simply unpacks that tuple and assigns the values to instance attributes so that later methods can operate without needing credentials passed around. Those instance attributes are the inputs that Robinhood.login will read to construct an authenticated Client, perform network authentication, populate client and user state, and enable the CRUD-style methods like get_quote and get_option_chain to make authenticated requests. __init__ itself has no return value; its observable side effects are the file-driven retrieval of credentials via setup and the assignment of username, password, and device_token onto the Robinhood instance."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 13,
        "character": 4
      },
      "end": {
        "line": 23,
        "character": 49
      }
    },
    "code": "    def setup(self):\n        config_path = os.path.join(os.path.dirname(__file__), '../config.local.json')\n        username = ''\n        password = ''\n        device_token = None\n        with open(config_path) as config_file:\n            config = json.load(config_file)\n            username = config['robinhood']['username']\n            password = config['robinhood']['password']\n            device_token = config['robinhood']['device-token']\n        return (username, password, device_token)",
    "narration": "Robinhood.setup is the small initializer that pulls the stored Robinhood credentials out of the project’s local configuration so the Robinhood client can act as the data-layer repository for quotes and option chains. It builds a filesystem path relative to the module location pointing at the local JSON config, opens and parses that JSON, and extracts the three fields labeled for Robinhood: the username, the password, and the device-token. It returns those three values as a tuple; Robinhood.__init__ immediately unpacks that tuple onto the instance attributes username, password, and device_token so that Robinhood.login can use them to create and authenticate the Client and thereby enable the class’s CRUD-style methods like get_quote and get_option_chain to call the broker endpoints. Compared with load (which reads the project’s pickled info blobs used by small_cap and other universe helpers), setup is focused solely on reading human credentials from JSON; it performs simple file I/O and JSON parsing and does not perform any additional validation or error handling itself, letting exceptions from missing files or missing JSON keys propagate back to the caller."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 24,
        "character": 4
      },
      "end": {
        "line": 38,
        "character": 26
      }
    },
    "code": "    def login(self):\n        self.client = Client(\n            username=self.username,\n            password=self.password,\n            device_token=self.device_token)\n        self.result = self.client.authenticate()\n        self.user = User.fetch(self.client)\n        print(\"Authenticated successfully = {}\".format(self.result))\n        print(\"Account Url = {}\".format(self.client.account_url))\n        print(\"Account Id = {}\".format(self.client.account_id))\n        print(\"Username = {}\".format(self.user[\"username\"]))\n        print()\n        auth_data = self.client.gen_credentials()\n        self.client = ClientLegacy(auth_data)\n        return self.client",
    "narration": "Robinhood.login is the routine that turns the credentials prepared in Robinhood.__init__ (which called setup) into an authenticated client the rest of the data layer will use to fetch quotes and option chains. It first creates a Client instance using the username, password, and device token that were loaded by setup, then performs an authentication network call via the Client.authenticate method and stores that outcome on the instance. Next it retrieves the authenticated user's metadata by invoking User.fetch with the client and records that on the instance as well; it prints the authentication result, account URL and ID exposed by the client, and the username from the fetched user metadata to the console. To support endpoints that expect the legacy authentication shape, it then asks the modern client to produce credentials using gen_credentials and immediately replaces the instance client with a ClientLegacy constructed from those generated credentials, returning the legacy client. In practice this means after login completes the Robinhood instance has performed network authentication, holds both the raw result and the user record, and exposes a client object in the form expected by the repository-style methods get_quote, get_option_chain, and get_next_3_exp_options; the function has observable side effects (instance attributes and console output) and makes multiple network requests but contains no explicit error handling paths."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 39,
        "character": 4
      },
      "end": {
        "line": 40,
        "character": 67
      }
    },
    "code": "    def get_quote(self, symbol):\n        return StockMarketdata.quote_by_symbol(self.client, symbol)",
    "narration": "Robinhood.get_quote is a thin delegator that takes a ticker symbol and hands it, together with the client's authenticated session, off to the fast_arrow quote adapter (StockMarketdata.quote_by_symbol as imported in the module) and returns whatever market-quote payload that adapter produces. In the project's data-layer architecture this implements the repository-style read operation for broker-style quotes: callers ask the Robinhood client for a symbol and get back a provider-native quote without needing to know fast_arrow specifics. Unlike get_option_chain, which first derives a stock's instrument id and calls OptionChain.fetch, get_quote performs a symbol-based lookup and does not parse instrument identifiers itself. There is no branching or local transformation here—the method's control flow simply delegates the call and returns the adapter result—so the returned quote flows directly into the framework's download/update and validation workflows as the broker-formatted market snapshot."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 41,
        "character": 4
      },
      "end": {
        "line": 43,
        "character": 63
      }
    },
    "code": "    def get_option_chain(self, symbol, stock):\n        stock_id = stock[\"instrument\"].split(\"/\")[-2]\n        return OptionChain.fetch(self.client, stock_id, symbol)",
    "narration": "Robinhood.get_option_chain takes the symbol plus the stock dictionary you already obtain from the Robinhood API, pulls the instrument identifier out of the stock's instrument URL by extracting the id segment, and then hands off responsibility to OptionChain.fetch, passing the client's session, that instrument id, and the symbol so the provider adapter actually retrieves the option-chain metadata over the network. There is no branching or local transformation beyond extracting the id; its job is purely to translate an internal stock record into the concrete external call that returns the option-chain object. That returned option-chain is what downstream routines like get_next_3_exp_options use to select upcoming expirations and fetch individual option contracts and market data, so get_option_chain is the thin repository-style adapter that bridges the framework’s stock metadata to the fast_arrow option-chain fetch operation."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 44,
        "character": 4
      },
      "end": {
        "line": 50,
        "character": 18
      }
    },
    "code": "    def get_next_3_exp_options(self, option_chain):\n        option_chain_id = option_chain[\"id\"]\n        expiration_dates = option_chain['expiration_dates']\n        next_3_expiration_dates = expiration_dates[0:3]\n        ops = Option.in_chain(self.client, option_chain_id, expiration_dates=next_3_expiration_dates)\n        ops = Option.mergein_marketdata_list(client, ops)\n        return ops",
    "narration": "get_next_3_exp_options takes the option_chain payload the data layer already retrieved and turns it into a small, enriched option universe the framework can consume: it pulls the option chain identifier and the list of expiration dates out of the option_chain dict, selects the first three upcoming expirations, and asks the fast_arrow Option helper to load all option contracts in that chain limited to those three dates via Option.in_chain using the Robinhood client's connection. After the raw option contracts are fetched, it enriches them with live market fields by invoking Option.mergein_marketdata_list with the same client so each Option object carries quote/market metadata the validation and download workflows expect, and then returns that list of market-enriched Option objects. This method is the focused repository-style accessor that sits downstream of the option-chain fetcher (get_option_chain) and upstream of the project’s download/validation logic, providing a compact, relevant slice of option market data for the next three expirations."
  },
  {
    "open-file": "api/robinhood.py",
    "range": {
      "start": {
        "line": 51,
        "character": 0
      },
      "end": {
        "line": 70,
        "character": 16
      }
    },
    "code": "if __name__ == '__main__':\n    rh = Robinhood()\n    client = rh.login()\n    symbol = 'TLT'\n    stock = rh.get_quote(symbol)\n    print(\"TLT Options:\")\n    option_chain = rh.get_option_chain(symbol, stock=stock)\n    options = rh.get_next_3_exp_options(option_chain)\n    op_df = pd.DataFrame(options, columns=options[0].keys())\n    op_df = op_df[abs(pd.to_numeric(op_df['strike_price']) - pd.to_numeric(stock['last_trade_price'])) <= 2]\n    display_columns = {'expiration_date': 'exp', 'strike_price': 'strike',\n                       'adjusted_mark_price': 'mark', 'bid_price': 'bid', 'ask_price': 'ask',\n                       'break_even_price': 'break_even', 'open_interest': 'open_interest',\n                       'volume': 'volume', 'chance_of_profit_long': 'profit_%_long',\n                       'chance_of_profit_short': 'profit_%_short', 'delta': 'delta',\n                       'implied_volatility': 'implied_vol'}\n    op_df = op_df.sort_values(['expiration_date', 'strike_price']).rename(columns=display_columns)\n    op_df = op_df[display_columns.values()]\n    print(\"Data:\")\n    print(op_df)\n",
    "narration": "When run as a script, this entry routine exercises the Robinhood thin client end-to-end: it instantiates Robinhood, performs authentication via login, and then uses the authenticated client to fetch a live quote for the example symbol TLT. The retrieved quote provides the instrument context that get_option_chain uses (see get_option_chain) to retrieve the exchange option chain for that underlying, and then get_next_3_exp_options is invoked to expand that chain into the concrete option contracts for the next three expirations (remember get_next_3_exp_options merges in market data for those contracts). The returned list of option records is then normalized into a pandas DataFrame so the sample can be filtered and formatted: the code converts strike and last trade price to numeric values and keeps only contracts whose strike is within two dollars of the underlying’s last trade price (a near-the-money slice useful for quick inspection or validation). A column-mapping is applied to produce shorter, user-friendly column names (for example expiration_date becomes exp, strike_price becomes strike, adjusted_mark_price becomes mark, and implied_volatility becomes impl_vol), the rows are sorted by expiration and strike, and only the mapped display columns are selected. Finally, the script prints the formatted DataFrame to demonstrate the shape and content of the option data that this thin client can provide to the framework’s download/update and validation pipelines; in other words, this block is a smoke-test/example showing how the Robinhood adapter's login, quote, and option-chain methods feed into downstream data-normalization steps used elsewhere in the project."
  },
  {
    "create-folder": "data/russell"
  },
  {
    "create-file": "data/russell/russell.py"
  },
  {
    "open-file": "data/russell/russell.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 4,
        "character": 0
      }
    },
    "code": "import os\nimport pandas as pd\nimport numpy as np\nfrom ..info import info as info_tool",
    "narration": "The file imports the operating-system utilities so it can inspect and build paths and interact with cached constituency files on disk, brings in pandas to represent and manipulate the Russell constituent list as tabular data (reading/writing CSVs or DataFrames that the data layer will hand to the engine), and pulls in numpy to perform efficient, vectorized numeric and boolean operations when filtering or transforming ticker arrays during universe construction. It also does a relative import of the shared info helper and renames it to info_tool so the Russell helpers can call the project's common metadata and caching routines (the same central info utilities other data helpers use) when delegating to the shared load routine that returns the small-cap constituents. This mirrors other modules that use os and pandas for IO and tabular work and follows the same pattern of reusing the central info functionality, with the only notable difference being the explicit use of numpy here for array-level operations and the upward relative import that aliases info to info_tool for clarity."
  },
  {
    "open-file": "data/russell/russell.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 42,
        "character": 18
      }
    },
    "code": "def small_cap():\n    ticker_csv_path = os.path.join(os.path.dirname(__file__), './small_cap.csv')\n    df = pd.read_csv(ticker_csv_path).set_index('Ticker')\n    info = info_tool.load('./small_cap.p')\n    column_csv_path = os.path.join(os.path.dirname(__file__), '../info/columns.csv')\n    info_columns = pd.read_csv(column_csv_path, header=None, index_col=0)[1].rename('info_columns')\n    info_df = pd.DataFrame.from_dict(info, orient='index')[list(info_columns)]\n    info_df['turnover'] = info_df['averageDailyVolume3Month'] / info_df['sharesOutstanding']\n    info_df['debtToEquity'] = (pd.to_numeric(info_df['Total Liabilities']) /\n                               pd.to_numeric(info_df[\"Total stockholders' equity\"]))\n    info_df['returnOnEquity'] = (pd.to_numeric(info_df['Net Income']) /\n                                 pd.to_numeric(info_df[\"Total stockholders' equity\"]))\n    info_df['priceToRevenue'] = (pd.to_numeric(info_df['marketCap']) /\n                                 pd.to_numeric(info_df['Total Revenue']))\n    df_filter = (\n        info_df['marketCap'] > 100000\n    ) & (\n        info_df['averageDailyVolume3Month'] > info_df['averageDailyVolume3Month'].quantile(0.05)\n    ) & (\n        info_df['turnover'] < info_df['turnover'].quantile(.95)\n    ) & (\n        info_df['trailingAnnualDividendYield'] > 0\n    ) & (\n        info_df['trailingPE'] <= 20\n    ) & (\n        info_df['forwardPE'] <= 25\n    ) & (\n        info_df['epsForward'] > 0\n    ) & (\n        info_df['priceToBook'] <= 3\n    ) & (\n        info_df['debtToEquity'] <= 2\n    ) & (\n        info_df['returnOnEquity'] >= .1\n    )\n    info_df = info_df[df_filter]\n    info_df.to_csv(os.path.join(os.path.dirname(__file__), 'small_cap_filtered.csv'))\n    return info_df",
    "narration": "small_cap is the entry routine the data layer exposes to produce a Russell small-cap universe enriched with fundamentals for backtests: it starts by loading the raw ticker list from a CSV into df and then delegates to the shared deserialization helper (the load routine you already examined) via info_tool.load to load the pickled per-ticker info dictionary, which it then turns into a pandas DataFrame called info_df using a curated set of columns sourced from an external columns CSV. After assembling info_df, small_cap computes several derived metrics — turnover (averageDailyVolume3Month divided by sharesOutstanding), debtToEquity, returnOnEquity, and priceToRevenue — converting text fields to numeric as needed so arithmetic works reliably. It then builds a composite boolean filter (df_filter) that keeps only names meeting a sequence of liquidity and fundamental screens: a minimum marketCap threshold, averageDailyVolume3Month above the 5th percentile, turnover below the 95th percentile, positive trailing annual dividend yield, trailingPE no greater than 20, forwardPE no greater than 25, positive forward EPS, price-to-book at or below 3, debt-to-equity at or below 2, and return on equity at least 10%. The routine applies that filter to info_df, writes the filtered universe out to a CSV for auditing, and returns the filtered DataFrame for use by the backtesting engine. Compared to the SP500/SP100/SP500_TECH universe constructors that simply read a ticker CSV and apply an exclude list, small_cap differs by ingesting detailed per-ticker metadata, computing financial ratios, and performing multi-criteria fundamental and liquidity filtering before returning the universe; it relies on file I/O throughout (CSV reads, pickle load, and writing the filtered CSV)."
  },
  {
    "open-file": "data/russell/russell.py",
    "range": {
      "start": {
        "line": 43,
        "character": 0
      },
      "end": {
        "line": 44,
        "character": 15
      }
    },
    "code": "if __name__ == '__main__':\n    small_cap()\n",
    "narration": "When this module is executed directly as a script rather than imported by other parts of the system, the module triggers a run of the small_cap helper so the Russell small-cap universe is built or refreshed immediately; when used as an imported helper inside the data layer nothing is executed automatically. In the context of the project's data pipeline, that invocation kicks off the local-data workflow that small_cap implements: it reads the stored ticker list and cached info, computes the filtering and derived columns, writes out the filtered universe CSV for downstream backtests, and returns the DataFrame for any ad-hoc inspection. This mirrors the runnable-script pattern used elsewhere in the repository (for example, the end-to-end Robinhood exercise that performs login and a live quote fetch), but it differs in scope and side effects — the Robinhood script exercises live network calls and authentication, whereas this entry-point simply regenerates a local universe artifact from existing files and caches."
  },
  {
    "create-file": "data/info/test_info.py"
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 5,
        "character": 0
      }
    },
    "code": "import unittest\nfrom datetime import datetime\nimport pandas as pd\nfrom .info import load_info, balance, cashflow, income, all_info",
    "narration": "The module imports the unit-test harness, a date constructor, the pandas library, and the set of financial-statement loader routines it is validating. unittest provides the TestCase scaffolding and test discovery/runner that TestInfo uses to declare and execute the individual assertions. The datetime constructor is used to create a specific snapshot date to exercise load_info’s time-slicing behavior so the test can verify that a single-date request returns a Series rather than a DataFrame. pandas is imported because the loaders return pandas DataFrame/Series objects and the tests assert on those types and inspect column names. The local info functions — load_info, balance, cashflow, income, and all_info — are the actual targets of the tests: load_info is the general loader that accepts statement type and optional date, balance/cashflow/income are the per-statement accessors the tests call directly, and all_info aggregates across statements; the test methods referenced earlier invoke these to confirm the data layer produces correctly structured, typed financial-statement payloads for the backtesting pipeline. Unlike the Robinhood client routines you’ve already seen, these imports operate against the repo’s historical/fundamentals data layer and return prestructured pandas objects consumed by the engine."
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 7,
        "character": 0
      },
      "end": {
        "line": 55,
        "character": 37
      }
    },
    "code": "class TestInfo(unittest.TestCase):\n\n    def test_load_info(self):\n        ...\n\n    def test_balance(self):\n        ...\n\n    def test_cashflow(self):\n        ...\n\n    def test_income(self):\n        ...\n\n    def test_all_info(self):\n        ...",
    "narration": "TestInfo is a unittest.TestCase that validates the data layer's financial-statement loaders so backtests receive clean, consistent fundamentals; its five methods exercise load_info, balance, cashflow, income, and all_info and assert that the right fields and types come back for a sample ticker. test_load_info calls load_info for the BALANCE, CASHFLOW, and INCOME families for a symbol like AAPL and checks that expected column names such as Total Assets, Net Change in Cash, and Net Income appear and that the return type is a pandas DataFrame in the multi-period case but becomes a pandas Series when a single datetime is requested. test_balance, test_cashflow, and test_income each exercise their respective helper (balance, cashflow, income) to ensure each statement loader returns the expected structure and key fields that downstream strategy code will consume. test_all_info exercises all_info to verify the aggregation of the individual statements into a unified payload the engine can use. These tests are leaf-level, do not perform network authentication themselves, and sit under the data.info suite where suite wires them into the test runner and prints the data.info header; remember that higher-level Robinhood helpers like Robinhood.login and the quote/option adapters we reviewed earlier produce market data for live runs, whereas TestInfo protects the offline fundamentals path so the backtester gets reliable statement data."
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 9,
        "character": 4
      },
      "end": {
        "line": 21,
        "character": 45
      }
    },
    "code": "    def test_load_info(self):\n        df = load_info('BALANCE', 'AAPL')\n        self.assertEqual(list(df.columns)[18], 'Total Assets')\n        self.assertEqual(type(df), pd.DataFrame)\n\n        df = load_info('CASHFLOW', 'AAPL')\n        self.assertEqual(list(df.columns)[-1], 'Net Change in Cash')\n\n        df = load_info('INCOME', 'AAPL')\n        self.assertEqual(list(df.columns)[-2], 'Net Income')\n\n        df = load_info('BALANCE', 'AAPL', datetime(2016, 1, 1))\n        self.assertEqual(type(df), pd.Series)",
    "narration": "TestInfo.test_load_info exercises the CSV-backed info loaders to ensure the financial-statement retrieval layer returns the expected schema and row/shape behavior so downstream backtests get consistent fundamentals. The test first asks load_info for BALANCE data for AAPL and asserts that the column at position 18 is named Total Assets and that the result is a pandas DataFrame, confirming a full table is returned when no date is supplied. It then requests CASHFLOW and checks the final column is Net Change in Cash, and requests INCOME and checks the penultimate column is Net Income, validating that each statement type maps to the expected column layout from the underlying CSV. Finally the test calls load_info for BALANCE with a cutoff date and asserts the returned object is a pandas Series, verifying the branch that selects a single report row when a date filter is provided. Under the hood load_info uses load_file to read a semicolon-delimited CSV from the PATHS mapping into a DataFrame, filters rows by Ticker, and if a date is given finds the first Report Date after that date and returns that row; the test thereby checks both the no-date (table) and date-filtered (single-row) control paths. Unlike the live market fetch routines we reviewed earlier (the Robinhood quote/option helpers), this test targets static, repo-hosted fundamentals so the framework can rely on consistent historical statement shapes for reporting and strategy logic; the balance, cashflow, and income convenience functions simply delegate to load_info, so passing these assertions guarantees those wrappers behave the same."
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 23,
        "character": 4
      },
      "end": {
        "line": 30,
        "character": 57
      }
    },
    "code": "    def test_balance(self):\n        df = balance('AAPL')\n        self.assertEqual(df.iloc[0]['Report Date'], '2000-06-30')\n        self.assertEqual(df.iloc[0]['Total Assets'], 6803000000)\n\n        s = balance('AAPL', datetime(2016, 1, 1))\n        self.assertEqual(s['Report Date'], '2016-03-31')\n        self.assertEqual(s['Total Assets'], 305277000000)",
    "narration": "TestInfo.test_balance exercises the BALANCE loader end-to-end to confirm both the unfiltered and date-filtered retrieval paths return the expected records and numeric values so downstream backtests can rely on consistent balance-sheet fields. It first calls the balance helper for the AAPL universe without a date cutoff; balance delegates to load_info for the BALANCE dataset, which loads the BALANCE file and filters rows by ticker, and the test asserts that the earliest returned report row corresponds to the historical mid‑2000 quarter and that the Total Assets column contains roughly 6.803 billion, validating that the loader returns a full DataFrame with intact column semantics and numeric values. The test then calls balance again but supplies a cutoff of January 1, 2016; load_info converts report dates to timestamps, finds the first report strictly after the cutoff, and returns that single row as a Series, and the test verifies that this selected row is the March 31, 2016 report with Total Assets around 305.277 billion. Together these checks ensure the BALANCE access pattern used by"
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 32,
        "character": 4
      },
      "end": {
        "line": 39,
        "character": 61
      }
    },
    "code": "    def test_cashflow(self):\n        df = cashflow('AAPL')\n        self.assertEqual(df.iloc[0]['Report Date'], '2000-06-30')\n        self.assertEqual(df.iloc[0]['Net Change in Cash'], -318000000)\n\n        s = cashflow('AAPL', datetime(2016, 1, 1))\n        self.assertEqual(s['Report Date'], '2016-03-31')\n        self.assertEqual(s['Net Change in Cash'], 4825000000)",
    "narration": "TestInfo.test_cashflow verifies that the cashflow loader returns both the full set of cashflow statements for a given ticker and the single statement that immediately follows a supplied cutoff date, which is important because the backtester needs accurate historical financials and the ability to pick the next report after any simulation date. The test first invokes cashflow for AAPL and asserts that the first returned row has the expected report date and Net Change in Cash value, exercising the path where cashflow delegates to load_info, load_info asks load_file for the CASHFLOW dataset, and then filters that dataset down to rows matching the ticker. The test then calls cashflow again with a cutoff datetime and checks that the returned record corresponds to the first report whose Report Date occurs after that cutoff, exercising the branch inside load_info that converts Report Date to datetimes, finds the index of the first date greater than the given cutoff, and returns that single report. By checking both the DataFrame-returning and date-filtering behaviors, the test ensures the cashflow loader reliably supplies raw series for aggregation and the correct \"next report\" selection logic that all_info and the rest of the pipeline rely on. This pattern mirrors TestInfo.test_income, which validates the same two access modes for the income loader, and complements all_info’s aggregation behavior that concatenates balance, cashflow, and income datasets and removes duplicated columns before returning consolidated information."
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 41,
        "character": 4
      },
      "end": {
        "line": 48,
        "character": 54
      }
    },
    "code": "    def test_income(self):\n        df = income('AAPL')\n        self.assertEqual(df.iloc[0]['Report Date'], '2000-06-30')\n        self.assertEqual(df.iloc[0]['Net Income'], 200000000)\n\n        s = income('AAPL', datetime(2016, 1, 1))\n        self.assertEqual(s['Report Date'], '2016-03-31')\n        self.assertEqual(s['Net Income'], 10516000000)",
    "narration": "TestInfo.test_income validates the income-statement side of the data layer to make sure the fundamentals loader returns the expected rows and numeric values that downstream backtests rely on. It does this by calling income, which delegates to load_info to read the INCOME dataset via load_file, filter that table down to the requested ticker, and then either return the whole ticker-specific table or, when given a cutoff date, locate and return the first report whose report date is after that cutoff. The test exercises both paths: first it requests income for AAPL with no date and asserts that the first row in the returned table has the expected early report date and Net Income value, and then it requests income for AAPL with a 2016-01-01 cutoff and asserts that the single row returned corresponds to the 2016-03-31 report with the expected Net Income amount. By checking both the multi-row retrieval and the date-filtered single-row retrieval, the test ensures the loader preserves report dates and numeric parsing and that the control flow selecting either the DataFrame or a single row behaves as intended; this mirrors the pattern used in TestInfo.test_balance and feeds into all_info, which concatenates balance, cashflow, and income to provide a combined fundamentals view for the rest of the backtester."
  },
  {
    "open-file": "data/info/test_info.py",
    "range": {
      "start": {
        "line": 50,
        "character": 4
      },
      "end": {
        "line": 55,
        "character": 37
      }
    },
    "code": "    def test_all_info(self):\n        df = all_info()\n        self.assertEqual(len(df.Ticker.unique()), 2080)\n\n        df = all_info('AAPL')\n        self.assertEqual(len(df), 78)",
    "narration": "TestInfo.test_all_info verifies that the file-backed fundamentals loaders produce a correctly aggregated dataset both at the universe level and for a single ticker so downstream backtests get consistent input. When called with no ticker, all_info assembles the three CSV-backed statement tables by asking load_file for each of BALANCE, CASHFLOW, and INCOME, concatenating them side-by-side and then dropping any duplicate columns; the test asserts that the resulting DataFrame contains 2,080 distinct ticker identifiers, validating the full-universe aggregation and the PATHS/CSV ingestion pipeline (load_file reads the bundled semicolon-delimited CSV files). When called with a specific ticker symbol, all_info takes the per-ticker route: it calls balance, cashflow, and income (each of which delegates to load_info) and concatenates those per-ticker results into a single table with duplicate columns removed; the test asserts that the combined AAPL payload has 78 rows, validating the per-symbol merging behavior. In short, the test checks both the no-ticker aggregation path and the ticker-specific composition path so the data layer reliably supplies unified fundamental matrices for the backtesting engine."
  },
  {
    "create-folder": "test"
  },
  {
    "create-file": "test/test.py"
  },
  {
    "open-file": "test/test.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import unittest\nfrom data.info.test_info import TestInfo",
    "narration": "The first import brings in Python’s unittest harness so this module can construct a TestSuite, add individual TestCase instances, and be run by whatever test runner the project uses; the file uses that API to assemble and execute a minimal set of checks rather than invoking the data loaders directly. The second import pulls the TestInfo TestCase class from the data.info.test_info module so the suite can reference the concrete test methods we already reviewed (test_load_info, test_balance, test_cashflow, test_income, test_all_info) and include the additional gap_L12_14 scenario when the suite is built. Compared with other files that import the actual info loaders or use package-relative imports, this file only imports the test class itself and the unittest framework—its role is orchestration of existing tests rather than exercising or wiring the data-loading code paths directly."
  },
  {
    "open-file": "test/test.py",
    "range": {
      "start": {
        "line": 3,
        "character": 0
      },
      "end": {
        "line": 11,
        "character": 21
      }
    },
    "code": "def suite():\n    test_suite = unittest.TestSuite()\n    print('==> data.info')\n    test_suite.addTest(TestInfo('test_load_info'))\n    test_suite.addTest(TestInfo('test_balance'))\n    test_suite.addTest(TestInfo('test_cashflow'))\n    test_suite.addTest(TestInfo('test_income'))    \n    test_suite.addTest(TestInfo('test_all_info'))    \n    return test_suite",
    "narration": "The suite function constructs a unittest test suite and serves as the file's entry point that wires up the framework's info-focused tests for execution; it emits a short console marker to indicate the data.info phase, then adds the TestInfo test methods test_load_info, test_balance, test_cashflow, test_income, and test_all_info into the aggregate suite and returns that suite to the test runner. By collecting those previously explained TestInfo checks into a single runnable bundle, suite implements the common unittest aggregation pattern so the test harness can exercise the CSV-backed fundamentals loaders and their behavior (including gap handling and basic info reporting) as one logical group; its only side effect is the console output that signals the start of the info tests."
  },
  {
    "open-file": "test/test.py",
    "range": {
      "start": {
        "line": 12,
        "character": 0
      },
      "end": {
        "line": 14,
        "character": 23
      }
    },
    "code": "if __name__ == '__main__':\n    runner = unittest.TextTestRunner(verbosity=2)\n    runner.run(suite())\n",
    "narration": "The top-level guard ensures that when this module is executed as a script it launches the unit-test harness, but it does nothing when the module is merely imported elsewhere so suite() and the test cases can be reused programmatically. The test harness is a TextTestRunner configured for verbose output so you get per-test progress and detailed pass/fail lines on stdout rather than a silent result object. When the runner is invoked it calls suite(), which assembles the TestSuite that includes the TestInfo tests we already reviewed and the gap_L12_14 case that specifically exercises the framework’s handling of missing rows in the data feed and basic info-reporting behavior. In practice this main entry point is the convenient developer-facing way to run the CSV-backed info loader checks and the gap validation together and see human-readable results immediately."
  },
  {
    "create-file": "__init__.py"
  },
  {
    "open-file": "__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import finance, data, api, tools, algos, test",
    "narration": "The package initializer imports the finance, data, api, tools, algos, and test subpackages so that the top-level package presents a consolidated public API and any module-level registration or wiring those subpackages perform runs on import. finance brings the framework’s fundamentals and finance-related helpers that back the CSV-backed info loaders exercised by the TestInfo tests you already reviewed; data is the lightweight data layer that wraps market data providers and downloader/updater tooling so the engine can consume consistent historical prices; api exposes broker-style runtime interfaces and endpoints that higher-level code and the backtester call into; tools contains universes, VIX term-structure helpers, and reporting utilities used by orchestration and post-run reporting; algos contains strategy bases and reusable algorithmic building blocks that strategies extend; and test exposes test utilities and fixtures used by the project’s test suite. This pattern follows the same package-aggregation approach used elsewhere in the codebase—where other initializers pull together related submodules like algos and util or log and fin_calc—so the top-level import surface is a facade that both re-exports functionality for consumers and triggers necessary side effects (provider/strategy registrations, fixtures, etc.) when the package is imported."
  },
  {
    "create-file": "api/__init__.py"
  },
  {
    "open-file": "api/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import yahoo",
    "narration": "Importing the yahoo module brings the Yahoo-backed provider into the api package namespace so consumers can reach the Yahoo client/provider through the package facade rather than importing the submodule directly. This matches the same re-export pattern used for modules like spy, info, and russell and keeps Yahoo as a parallel market-data provider that feeds into the same data layer tested by the TestInfo.* loaders."
  },
  {
    "create-file": "backtest/__init__.py"
  },
  {
    "open-file": "backtest/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import algos, util",
    "narration": "These import statements pull the algos and util submodules into the package namespace so users can refer to algorithm-building blocks and shared helper functions directly from the top-level backtest package instead of digging into subpackages. algos is the collection of small, composable algorithm primitives and strategy helpers that strategy implementations and the run-time runner compose to express trading behavior, while util contains the common utilities used across the data acquisition, strategy, and reporting layers (helpers that support normalization, parsing, small transformations and other cross-cutting tasks that the file-backed fundamentals loaders exercised in the TestInfo.* tests rely on). This follows the same façade pattern used elsewhere in the package where specific strategy classes like BaseStrategy, BuyAndHold and CrossOver are exposed at the package level or where broader modules such as finance, data, api, tools, algos, and test are re-exported; the difference here is that the package is exposing whole submodules rather than individual classes. The import of the external backtrader dependency elsewhere is an example of how some modules bring in third-party APIs while these two imports simply surface internal modules for a cleaner top-level API."
  },
  {
    "create-file": "backtest/algos/__init__.py"
  },
  {
    "open-file": "backtest/algos/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import BaseStrategy, BuyAndHold, CrossOver, MeanReversion, LeveragedEtfPair, WeightedHold, NCAV",
    "narration": "The package initializer pulls a set of strategy classes and the shared BaseStrategy abstraction into the algos package namespace so callers can import algorithm implementations directly from backtest.algos without touching submodules. BaseStrategy is the common abstract ancestor that defines the interface and shared behaviour the concrete strategies rely on, while BuyAndHold, CrossOver, MeanReversion, LeveragedEtfPair, WeightedHold, and NCAV are the concrete strategy implementations that implement different trading rules (simple buy-and-hold, signal crossover, mean-reversion, a paired leveraged-ETF approach, weighted-basket holding, and a net-current-asset-value value screen, respectively). Exposing these names here is purely about creating a clean public API for the strategy layer and fits the project’s separation of concerns where the data acquisition and validation layer (exercised by the TestInfo suite you already looked at) provides consistent inputs that these strategies consume. This pattern differs from other files that import BaseStrategy under an alias or combine it with framework imports; instead of aliasing a single symbol, the initializer aggregates and re-exports multiple strategy classes to simplify downstream imports."
  },
  {
    "create-file": "backtest/util/__init__.py"
  },
  {
    "open-file": "backtest/util/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import commission, observers, analyzers, universe",
    "narration": "The import brings the four sibling modules commission, observers, analyzers, and universe into the util package namespace so other parts of the system can import utility helpers from backtest.util rather than drilling into deeper module paths; commission contains fee and execution-cost models used by the engine, observers provides runtime observer classes that capture metrics during a run, analyzers implements post-run analysis and reporting pieces the reporting layer consumes, and universe exposes predefined universes and the VIX term-structure helper that the data pipeline and strategy selection use for market-regime context. There is no runtime logic here beyond making those modules available under the util package, which creates a small, stable facade for utilities; this is the same re-exporting pattern you saw elsewhere where modules import utilities directly from util or alias universe as universe_util, and it contrasts with the third‑party imports like backtrader which pull external libraries into consumers rather than shaping an internal package API. This file therefore plays an organizational role: it wires the utility modules into a concise import surface that the rest of the backtesting framework—separate from the CSV-backed fundamentals tests you inspected earlier—relies on for commissioning, observing, analyzing, and defining universes."
  },
  {
    "create-file": "backtest/util/commission.py"
  },
  {
    "open-file": "backtest/util/commission.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "import backtrader as bt",
    "narration": "The file brings the Backtrader library into scope using the conventional bt alias so the commission model can directly extend and interact with Backtrader’s engine-level abstractions. By importing backtrader, the code can subclass CommInfoBase and implement the _getcommission hook that the broker/engine calls when it computes trade costs, allowing the IBCommision class to plug realistic per-share, minimum, and tiered rules into the backtesting lifecycle. This differs from the project’s relative imports that pull in local modules such as commission, observers, analyzers, and universe; those wire internal plumbing and helpers, whereas the Backtrader import exposes the external framework types and runtime hooks necessary for the commission model to be invoked by strategies and the broker during simulation. Remember the TestInfo tests we already covered that validate data loaders; importing Backtrader here is what lets the commission logic consume the trades produced by those upstream data and strategy components and have those costs flow into reporting and P&L."
  },
  {
    "open-file": "backtest/util/commission.py",
    "range": {
      "start": {
        "line": 2,
        "character": 0
      },
      "end": {
        "line": 16,
        "character": 25
      }
    },
    "code": "class IBCommision(bt.CommInfoBase):\n    params = (\n        ('per_share', 0.005),\n        ('min_per_order', 1.0),\n        ('max_per_order_abs_pct', 0.005),\n    )\n    def _getcommission(self, size, price, pseudoexec):\n        ...",
    "narration": "IBCommision is a subclass of backtrader.CommInfoBase that implements an Interactive Brokers–style commission schedule so the backtester charges realistic transaction fees when executions occur; Backtrader calls its _getcommission method with the executed size and price (the pseudoexec argument is accepted but not used in the calculation) and expects a numeric cost back. The class exposes three parameters to describe the fee schedule: a per-share fee, a floor that enforces a minimum charge per order, and a cap expressed as the maximum absolute percentage of the order value. At runtime _getcommission computes the raw per-share cost by multiplying the absolute trade size by the per-share fee, derives the order notional by multiplying price by absolute size, and then enforces two guard rules in sequence: if the raw commission is below the configured minimum it is bumped up to that minimum, otherwise if the commission represents more than the configured percentage of the order value it is capped to that percentage of the order notional. The result is returned to Backtrader so the engine can fold realistic fees into P&L and performance metrics. The parameter defaults and the calculation flow mirror the similar IB-style implementation elsewhere in the repo, keeping a consistent pattern for per-share, minimum, and percentage-cap logic used across commission models. This class is standalone in the codebase and sits in the data/transaction modeling layer that the strategy and reporting components rely on for accurate cost modeling; remember the TestInfo.* tests we reviewed earlier ensure the historical data feeding the engine is consistent, and IBCommision consumes the execution price and size produced from that pipeline to compute trade costs."
  },
  {
    "open-file": "backtest/util/commission.py",
    "range": {
      "start": {
        "line": 3,
        "character": 4
      },
      "end": {
        "line": 7,
        "character": 5
      }
    },
    "code": "    params = (\n        ('per_share', 0.005),\n        ('min_per_order', 1.0),\n        ('max_per_order_abs_pct', 0.005),\n    )",
    "narration": "IBCommision declares a params tuple that establishes three configurable defaults that Backtrader exposes on the class instance: a per-share fee (set to five tenths of a cent by default), a minimum fee per order (one dollar by default), and a maximum fee expressed as an absolute percentage of the order value (set to half a percent by default). Backtrader makes these available via the class parameter namespace so IBCommision._getcommission reads them at runtime to compute the raw per-share cost, enforce the per-order floor, and cap the charge relative to the total trade value—recreating the practical tier/minimum rules of an Interactive Brokers schedule. Because they use Backtrader’s params mechanism, callers can override the defaults when wiring the commission model into a run, enabling reuse across strategies and scenarios; the pattern mirrors other modules in the codebase that expose behavior through params to keep runtime configuration separate from implementation."
  },
  {
    "open-file": "backtest/util/commission.py",
    "range": {
      "start": {
        "line": 8,
        "character": 4
      },
      "end": {
        "line": 16,
        "character": 25
      }
    },
    "code": "    def _getcommission(self, size, price, pseudoexec):\n        commission = abs(size) * self.p.per_share\n        order_price = price * abs(size)\n        commission_as_percentage_of_order_price = commission / order_price\n        if commission < self.p.min_per_order:\n            commission = self.p.min_per_order\n        elif commission_as_percentage_of_order_price > self.p.max_per_order_abs_pct:\n            commission = order_price * self.p.max_per_order_abs_pct\n        return commission",
    "narration": "As noted earlier, IBCommision is a subclass of backtrader.CommInfoBase and Backtrader calls its _getcommission with the executed size and price when it needs to charge transaction costs; _getcommission implements the IB-style schedule by computing a per-trade numeric cost and returning it to the engine. The method first treats size as unsigned by taking its absolute value so buys and sells are charged the same way, and multiplies that absolute size by the per_share parameter to produce the raw per-share commission. It also computes the total order notional by multiplying price by the absolute size so it can express the raw commission as a fraction of order value. The control flow then enforces two business rules: if the raw commission is smaller than the configured min_per_order, the commission is overridden to that minimum; otherwise, if the raw commission expressed as a percentage of the order value exceeds max_per_order_abs_pct, the commission is capped to that percentage of the order notional. The pseudoexec argument is accepted for Backtrader compatibility but is not used in the calculation. Conceptually this implements a template-method style override of the engine’s commission hook to encapsulate a per-share fee with an order-level floor and an absolute-percent cap so backtests reflect realistic Interactive Brokers–style fee behavior."
  },
  {
    "create-file": "backtest/util/observers.py"
  },
  {
    "open-file": "backtest/util/observers.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "import backtrader as bt",
    "narration": "The file brings the Backtrader library into scope using the conventional bt alias so the Value observer implementation can reference Backtrader’s engine types and hooks (for example the observer base class, the broker interface, and the runtime callback semantics) when it records portfolio equity each bar. This mirrors the pattern used elsewhere in the codebase—such as the IBCommision implementation and the BaseStrategy-containing modules—that also import Backtrader as bt so all modules use a consistent name for engine-level types. Because the Value observer runs inside Backtrader’s event loop, having Backtrader available under the bt alias lets the observer interact directly with the running broker/engine to extract and emit the equity-curve data the reporting layer consumes."
  },
  {
    "open-file": "backtest/util/observers.py",
    "range": {
      "start": {
        "line": 4,
        "character": 0
      },
      "end": {
        "line": 11,
        "character": 59
      }
    },
    "code": "class Value(bt.Observer):\n    alias = ('Value',)\n    lines = ('value',)\n\n    plotinfo = dict(plot=True, subplot=True)\n\n    def next(self):\n        ...",
    "narration": "Value is a simple Backtrader observer subclass that wires the engine’s live broker portfolio worth into the reporting pipeline on every bar. By inheriting from bt.Observer it registers itself with Backtrader so that Backtrader will call its next method each time the engine advances a bar; the class declares a single output line named value and an alias so the observer is discoverable by that name, and the plotinfo requests that the line be drawn in its own subplot. In next it pulls the current portfolio/equity by asking the observer owner’s broker for its value and writes that number into the observer’s value line for the current bar; that time series becomes the equity curve the analyzers and reporting layer consume after or during the run. Functionally it’s the straightforward observer counterpart to the engine’s transaction and strategy layers: market data and strategies drive broker state, and Value samples broker.getvalue each bar to produce the series used for performance metrics and plotting. The implementation matches the common pattern elsewhere in the codebase for a single-line observer that simply forwards broker value into an observable line."
  },
  {
    "open-file": "backtest/util/observers.py",
    "range": {
      "start": {
        "line": 5,
        "character": 4
      },
      "end": {
        "line": 5,
        "character": 22
      }
    },
    "code": "    alias = ('Value',)",
    "narration": "The alias attribute declares the public name Backtrader will use to register and display this observer at runtime; by assigning a single-entry tuple containing the name Value, the observer is exposed to the engine, plotter, and any external reporting/serialization under that human-friendly identifier. This plays a supportive role in the reporting pipeline: the observer class Value both defines the numeric series it records via its lines attribute and populates that series in its next method (which reads the broker’s current getvalue); alias is orthogonal to lines — lines defines the data produced, while alias defines how that observer is referenced and labeled by Backtrader and the rest of the reporting layer. The pattern mirrors other observers in the project that pair a lines declaration with an alias to let the backtest framework find and display runtime metrics consistently."
  },
  {
    "open-file": "backtest/util/observers.py",
    "range": {
      "start": {
        "line": 6,
        "character": 4
      },
      "end": {
        "line": 6,
        "character": 22
      }
    },
    "code": "    lines = ('value',)",
    "narration": "The lines declaration defines a single named output series called value for the Value observer so Backtrader will allocate and manage a time-series buffer that the observer can write into each bar; Value.next then writes the broker’s current portfolio value into that series, and the reporting layer and analyzers consume that series as the equity curve. This pattern — declaring lines at the class level so Backtrader can create LineBuffers for plotting and analysis — mirrors the alias and plotinfo declarations on the Value observer and matches how other observers in the util.observers module expose named metrics for downstream plotting and analyzers."
  },
  {
    "open-file": "backtest/util/observers.py",
    "range": {
      "start": {
        "line": 8,
        "character": 4
      },
      "end": {
        "line": 8,
        "character": 44
      }
    },
    "code": "    plotinfo = dict(plot=True, subplot=True)",
    "narration": "The Value observer sets a plotinfo attribute so Backtrader’s plotting subsystem knows how to render the equity series produced by the Value observer: it enables plotting and requests that the observer’s single line be drawn on its own subplot rather than overlaid on the price panel. In the context of the Value observer, this makes the broker/portfolio value appear as a distinct equity-curve pane during interactive or saved Backtrader plots, which supports the reporting layer’s need for a clear visual of performance; this is a presentation hint for Backtrader’s renderer and does not change the runtime collection of the value line itself. This follows the same pattern other observers/indicators use to control Backtrader plotting, and differs from the project’s standalone plot helper that manually draws series with matplotlib because plotinfo integrates the observer into Backtrader’s native plot pipeline."
  },
  {
    "open-file": "backtest/util/observers.py",
    "range": {
      "start": {
        "line": 10,
        "character": 4
      },
      "end": {
        "line": 11,
        "character": 59
      }
    },
    "code": "    def next(self):\n        self.lines.value[0] = self._owner.broker.getvalue()",
    "narration": "The next method samples the live portfolio snapshot each time Backtrader advances a bar: it asks the observer’s owner for the broker’s current total value and stores that number into the observer’s value line for the current timeframe. Because Backtrader calls next on observers on every bar, this produces a time series of equity values that the reporting layer and analyzers consume to build the equity curve and related metrics. Conceptually this is the Observer pattern in action—the observer nondestructively polls engine state (the broker) and records it—so unlike IBCommision which integrates with execution cost calculations, Value simply captures and records portfolio-level state once per bar with no branching or additional logic."
  },
  {
    "create-file": "data/__init__.py"
  },
  {
    "open-file": "data/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import spy, info, russell",
    "narration": "As with the earlier package initializers that surfaced algos and util submodules into their parent namespaces, this initializer pulls the spy, info, and russell modules into the data package namespace so consumers can import those provider and metadata modules directly from data rather than digging into submodules. spy supplies the SPY-related provider or dataset helpers used by downstream loaders, russell exposes the Russell-universe loader and related universe definitions, and info contains the supporting metadata and utility functions that loaders, updaters, and validators rely on. This follows the same re-export pattern used elsewhere in the codebase where single modules or small groups of related modules are exposed at the package level (for example the places that expose russell alone, info alongside test_info, or tickers); the goal is a stable, convenient public API for the data layer so callers don’t reach into internal module paths."
  },
  {
    "create-file": "data/info/__init__.py"
  },
  {
    "open-file": "data/info/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import info, test_info",
    "narration": "The initializer pulls the info and test_info modules into the data.info package namespace so callers can import those modules directly from data.info instead of reaching into deeper paths; info provides the reference-data, universe and regime-helper utilities the data pipeline and strategy selection rely on, and test_info exposes the testing harness and helpers (including the TestInfo test class) used to validate those metadata utilities. This mirrors the same packaging pattern used elsewhere in the codebase where sibling modules like spy and russell are re-exported at their package level to present a concise public API, and contrasts with places that import a specific class from a submodule when only that symbol is needed. By binding info and test_info at package import time, the package initializer shapes the surface the rest of the system depends on for metadata and its tests without requiring callers to know the internal module layout."
  },
  {
    "create-file": "data/russell/__init__.py"
  },
  {
    "open-file": "data/russell/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import russell",
    "narration": "The initializer pulls the russell module into the package namespace so callers can access Russell-specific symbol lists and loader utilities directly from the data package rather than importing from a deeper submodule path; in the context of the data layer’s predefined universes, russell supplies the Russell index universes the pipeline uses when building target baskets and resolving symbol metadata. This follows the same package-level re-export pattern used elsewhere in the codebase where modules like spy and info were surfaced at the parent namespace to make provider and metadata utilities easy to import, so the data package acts as a single entry point for universe helpers used by the rest of the backtesting framework."
  },
  {
    "create-folder": "data/spy"
  },
  {
    "create-file": "data/spy/__init__.py"
  },
  {
    "open-file": "data/spy/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import tickers",
    "narration": "The initializer pulls the tickers submodule into the data.spy package namespace so callers can access SPY-specific ticker utilities directly through the package rather than drilling into a nested module. This follows the same surface-the-submodule pattern you saw earlier for spy, info, and russell and keeps the SPY public API centralized. The tickers module contains the SpyTickers provider whose constructor invokes download to build and store the constituent list (the tickers attribute) that the data acquisition and universe-selection code will request when assembling SPY symbol lists. Exposing the module object, rather than only importing the SpyTickers class, lets downstream code instantiate SpyTickers or use any additional helpers or constants defined alongside it without extra import paths."
  },
  {
    "create-file": "data/spy/spy.py"
  },
  {
    "open-file": "data/spy/spy.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from data.spy.tickers import SpyTickers",
    "narration": "The import brings the SpyTickers helper into the spy utilities so the SPY-specific features—like the gap_L2_3 signal—can rely on a ready-made universe of tickers. Remember earlier we discussed how package initializers surface submodules into the data namespace; this line differs only in that it imports a concrete class rather than an entire module, making the spy utilities able to instantiate a SpyTickers object directly. Instantiating SpyTickers runs its initializer, which populates its tickers list by calling its download helper, so downstream code in the spy module gets an immediately usable list of SPY-related symbols to feed the gap labeling logic and other thin data-layer wrappers. This mirrors the pattern used elsewhere where ticker-related functionality is centralized, but here the file imports the specific class to obtain the populated universe rather than importing the module as a whole."
  },
  {
    "open-file": "data/spy/spy.py",
    "range": {
      "start": {
        "line": 2,
        "character": 0
      },
      "end": {
        "line": 3,
        "character": 29
      }
    },
    "code": "if __name__ == '__main__':\n    spyTickers = SpyTickers()\n",
    "narration": "When the module is executed as a standalone script, the module-level guard causes a SpyTickers instance to be created. That instantiation produces the helper object that the SPY-specific utilities — including the gap_L2_3 helper — rely on for ticker metadata and any download or cache-seeding behavior; if the SpyTickers constructor performs side effects (like loading a local cache or kicking off a download) those will happen at that point, otherwise the instance simply gives you an immediately usable API for manual inspection or ad-hoc calls (for example to invoke its download method). This follows the same pattern used elsewhere in the project where provider classes are imported into the package namespace and modules make a simple instance available when run interactively so developers can exercise or bootstrap the data layer without running the full backtest."
  },
  {
    "create-file": "data/spy/tickers.py"
  },
  {
    "open-file": "data/spy/tickers.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 5,
        "character": 0
      }
    },
    "code": "import os\nimport pandas as pd\nimport pandas_datareader.data as web\nfrom datetime import datetime",
    "narration": "The file pulls in a few standard and third-party utilities that SpyTickers uses to fetch, shape, and persist the SPY constituent list: os provides filesystem primitives used by SpyTickers to locate or create a local cache directory and check for existing cached files; pandas is the DataFrame-centric toolkit the class uses to validate, clean, and serialize the tabular list of tickers once downloaded; pandas_datareader.data supplies the web-facing data reader that the download method calls to retrieve the live constituent information from an external provider; and datetime.datetime is used to construct timestamps and date bounds for the fetch or for naming/invalidating cached snapshots. Remember the earlier package-level imports that exposed provider modules into the data namespace; those were about wiring internal modules for consumers, whereas these imports bring in the runtime dependencies SpyTickers needs to perform network I/O, date handling, and tabular processing so it can return a reusable, validated SPY universe to the data layer."
  },
  {
    "open-file": "data/spy/tickers.py",
    "range": {
      "start": {
        "line": 8,
        "character": 0
      },
      "end": {
        "line": 26,
        "character": 22
      }
    },
    "code": "class SpyTickers:\n    def __init__(self):\n        ...\n\n    def download(self):\n        ...",
    "narration": "SpyTickers is the small provider object that gives the rest of the data pipeline a ready-to-use S&P 500 (SPY) constituent universe. When you create a SpyTickers instance, its __init__ populates the instance attribute tickers by invoking download immediately, so any consumer that imports and instantiates SpyTickers gets a validated list synchronously. The download method implements a simple persistent-cache pattern: it first attempts to load a local tickers.csv file next to the module and return the saved list if found; if the file is missing, it falls back to downloading the canonical S&P 500 table from a public web source, extracts the symbol column, normalizes symbols (for example converting dots to the dash format used by our data providers), sorts the list, writes it back to tickers.csv for future runs, and then returns the processed collection. Control flow is straightforward: the primary path reads the cached CSV, the fallback path scrapes, normalizes, saves, and returns. SpyTickers follows the same provider-and-cache pattern used by other universe helpers in the data package and is reachable via the package-level imports you already reviewed, so the data layer and strategy selection code can import SpyTickers and obtain a consistent SPY universe for backtests and live usage."
  },
  {
    "open-file": "data/spy/tickers.py",
    "range": {
      "start": {
        "line": 9,
        "character": 4
      },
      "end": {
        "line": 10,
        "character": 38
      }
    },
    "code": "    def __init__(self):\n        self.tickers = self.download()",
    "narration": "SpyTickers.__init__ constructs a SpyTickers instance and immediately populates its tickers attribute by invoking the download routine so that a ready-to-use SPY universe is available as soon as the object is created. This eager initialization is deliberate because SpyTickers serves as an entry point: callers that instantiate SpyTickers (or import the module that exposes it) expect a validated list of S&P 500 constituents to be present for the data layer and strategies to consume. The download call has observable side effects beyond returning the ticker list — it prints status messages and sets additional instance attributes such as the path used for the cached tickers file — and it implements the two main control paths we rely on (use a local tickers.csv when present, or scrape the Wikipedia table otherwise). The pattern here mirrors other small provider classes in the project that fetch or load reference data during construction so downstream components can depend on an initialized, consistent universe immediately."
  },
  {
    "open-file": "data/spy/tickers.py",
    "range": {
      "start": {
        "line": 12,
        "character": 4
      },
      "end": {
        "line": 26,
        "character": 22
      }
    },
    "code": "    def download(self):\n        print('Downloading S&P 500 members...')\n        self.ticker_csv_path = os.path.join(os.path.dirname(__file__), 'tickers.csv')\n        try:\n            tickers = pd.read_csv(self.ticker_csv_path, header=None)[1]\n            print('tickers.csv found. Nothing downloaded.')\n        except FileNotFoundError:\n            print('No tickers.csv file...')\n            data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n            table = data[0]\n            tickers = table.iloc[1:, 0].tolist()\n            tickers = pd.Series([t.replace('.', '-') for t in tickers]).sort_values(ignore_index=True)\n            tickers.to_csv(self.ticker_csv_path, header=False)\n            print(\"Tickers downloaded and saved.\")\n        return tickers",
    "narration": "SpyTickers.download is the small utility that ensures the project has a local, reusable list of S&P 500 symbols for the data layer: when called (SpyTickers.__init__ assigns its return to self.tickers) it first constructs a path to a local tickers.csv file placed alongside the module and stores that path on the instance. It then attempts to read that CSV with pandas (expecting no header) and, if successful, returns the tickers while printing a brief status message indicating the local cache was used. If the CSV is missing, the method catches the FileNotFoundError and falls back to fetching the current constituents by parsing the S&P 500 Wikipedia page with pandas.read_html, selecting the table’s ticker column while skipping the header row, normalizing ticker strings by replacing dots with hyphens, sorting the list, saving the series back to tickers.csv without a header, and printing a message that the download and save occurred. The method therefore implements a simple cache-then-fetch flow that writes the cached file as a side effect, returns a pandas Series (or similar iterable) of cleaned tickers for the rest of the data pipeline, and provides console-level status reporting so backtests and strategies get a consistent SPY universe without repeated web fetches."
  },
  {
    "create-file": "tools/__init__.py"
  },
  {
    "open-file": "tools/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import log, fin_calc",
    "narration": "The initializer brings the log and fin_calc modules into the tools package namespace so callers can access the package-scoped logging utilities and the shared financial-calculation helpers directly from tools rather than importing deeper submodules. In the context of the data-maintenance role of tools — downloading, updating, and validating historical prices and fundamentals — log provides a consistent logging surface for those workflows and fin_calc encapsulates the common numeric routines (the returns/adjustments/aggregation helpers used across validation and transformation steps). This follows the same exposure pattern used elsewhere in the project where provider and metadata modules like spy, info, and russell were surfaced at their package level; it’s essentially a namespace-aggregation/facade approach that simplifies imports for downstream scripts. Compared to the similar one-off import that only surfaced log in another place, this line groups both logging and calculation utilities together, and it mirrors the broader pattern where packages surface multiple related modules (as seen in the places that pull in finance, data, api, tools, and algos)."
  },
  {
    "create-file": "tools/log/__init__.py"
  },
  {
    "open-file": "tools/log/__init__.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 1,
        "character": 0
      }
    },
    "code": "from . import log",
    "narration": "The initializer performs a relative import of the log submodule into the package namespace so that the package exposes logging utilities directly as tools.log; when the package is imported, Python will load and execute the log module so its setup, configuration helpers, and adapter objects become immediately available to consumers across data, strategy, and reporting. This follows the same packaging pattern used elsewhere where submodules like spy, info, and russell were surfaced at the package level to simplify imports; a closely related variant in the codebase also re-exports an additional fin_calc module when needed, whereas here only the logging surface is exposed. Because some modules compute runtime settings such as a LOG_FILE_PATH using os and datetime, importing log at package initialization ensures any module-level logging configuration is evaluated up front, providing a single, consistent entry point (a lightweight facade) for the project’s logging functionality."
  },
  {
    "create-file": "tools/markowitz.py"
  },
  {
    "open-file": "tools/markowitz.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 7,
        "character": 0
      }
    },
    "code": "import os\nimport argparse\nimport pandas as pd\nfrom pypfopt import expected_returns\nfrom pypfopt import risk_models\nfrom pypfopt.efficient_frontier import EfficientFrontier\nfrom pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices",
    "narration": "This module imports a small set of utilities needed to turn the data pipeline’s local price histories into Markowitz-style portfolio targets: os provides filesystem operations so the optimizer can locate the local CSV price files that the data layer produces; argparse lets the module be invoked as a standalone CLI with selectable tickers, cash, or constraint flags; pandas is used to load and shape time series into a DataFrame that represents the historical price matrix the optimizer consumes. The pypfopt pieces supply the mean-variance engine: expected_returns supplies the mean-return estimators used to turn price series into mu, risk_models supplies covariance estimators used to produce the S matrix, EfficientFrontier is the core optimizer that constructs and solves the Markowitz problem under bounds and constraints, and DiscreteAllocation together with get_latest_prices convert the continuous weight solution into integer share counts against the latest prices and a total cash budget. In terms of data flow, the imports reflect the typical pipeline where pandas reads local prices, expected_returns and risk_models compute mu and S, EfficientFrontier produces raw and cleaned weights, and get_latest_prices plus DiscreteAllocation translate those weights into a tradeable allocation; this mirrors the sequence used in the optimize utility elsewhere in the project, whereas other modules tend to import API wrappers or statistical libraries when they perform data acquisition or heavier time-series testing."
  },
  {
    "open-file": "tools/markowitz.py",
    "range": {
      "start": {
        "line": 8,
        "character": 0
      },
      "end": {
        "line": 27,
        "character": 42
      }
    },
    "code": "def optimize(tickers, cash=1000, longshort=False):\n    print(f'Cash: ${cash}')\n    date_start = 20 * 6\n    df = pd.DataFrame()\n    for t in tickers:\n        path = os.path.join(os.path.dirname(__file__), f'../data/price/{t}.csv')\n        price = pd.read_csv(path, parse_dates=True, index_col='Date')['Adj Close'].rename(t)\n        df[t] = price[-date_start:]\n    mu = expected_returns.mean_historical_return(df)\n    S = risk_models.sample_cov(df)\n    ef = EfficientFrontier(mu, S, weight_bounds=((-1, 1) if longshort else (0, 1)))\n    raw_weights = ef.max_sharpe()\n    clean_weights = ef.clean_weights()\n    latest_prices = get_latest_prices(df)\n    da = DiscreteAllocation(raw_weights, latest_prices, total_portfolio_value=cash)\n    allocation, leftover = da.lp_portfolio()\n    print('\\nWeights:', clean_weights)\n    print('\\nShares:', allocation)\n    print(f'\\n${leftover:.2f} leftover')\n    ef.portfolio_performance(verbose=True)",
    "narration": "As a Markowitz utility, optimize turns historical price series for a given list of tickers into a concrete, tradeable portfolio recommendation: it reads local CSV price files for each ticker and builds a price DataFrame by taking the most recent 20 times 6 rows (roughly 120 observations, i.e., about six months) per asset, then computes expected returns using expected_returns.mean_historical_return and the sample covariance matrix using risk_models.sample_cov. It instantiates an EfficientFrontier with weight bounds that permit short positions only when the longshort flag is true, then finds the maximum-Sharpe continuous portfolio via EfficientFrontier.max_sharpe and produces a cleaned, interpretable weight mapping via EfficientFrontier.clean_weights. To turn continuous weights into integer share counts it asks get_latest_prices for current prices and hands the weights and prices to DiscreteAllocation, which solves an LP-based discrete-allocation and returns a per-ticker share allocation plus leftover cash. Throughout it prints the input cash, the cleaned weights, the discrete share counts, the leftover cash, and calls EfficientFrontier.portfolio_performance to output performance metrics; it therefore performs file I/O (reading the local price CSVs) and console output but does not return a value. The function expects the same locally cached price dataset produced by the project’s data layer (for example, the SpyTickers universe you examined earlier would be a typical input)."
  },
  {
    "open-file": "tools/markowitz.py",
    "range": {
      "start": {
        "line": 28,
        "character": 0
      },
      "end": {
        "line": 39,
        "character": 50
      }
    },
    "code": "if __name__ == '__main__':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument('-t', '--ticker', nargs='+')\n    PARSER.add_argument('--cash', nargs=1, type=int)\n    PARSER.add_argument('-ls', '--longshort', action=\"store_true\")\n    ARGS = PARSER.parse_args()\n    CASH = ARGS.cash or [1000]\n    if ARGS.ticker:\n        optimize(ARGS.ticker, CASH[0], ARGS.longshort)\n    else:\n        TICKERS = ['TLT', 'FB', 'AAPL', 'AMZN', 'NFLX', 'GOOG']\n        optimize(TICKERS, CASH[0], ARGS.longshort)\n",
    "narration": "The module-level guard wires up a small command-line interface so the Markowitz optimizer can be run interactively for quick experiments. It uses argparse to accept a list of ticker symbols via the ticker option, a single integer cash amount via the cash option (nargs=1 so argparse returns a one-element list), and a longshort boolean flag to allow negative weights. After parsing, the code normalizes the cash input by falling back to a default of 1000 when no cash argument is provided, and then it decides which universe to pass into optimize: if the user supplied tickers those are forwarded, otherwise a hard-coded example universe made of TLT, FB, AAPL, AMZN, NFLX, and GOOG is used. The chosen tickers, the unpacked cash value, and the longshort flag are then handed to optimize, which (as seen in the similar optimize function) builds price series, computes mean returns and the covariance, runs the efficient frontier, and produces discrete allocations. Conceptually, this entry point provides a lightweight, standalone way to turn historical returns and covariances into target weights and share allocations outside the Backtrader engine, mirroring the pattern used by run_strategy for CLI-driven workflows but focused solely on portfolio optimization and allocation output."
  },
  {
    "create-file": "tools/validate_data.py"
  },
  {
    "open-file": "tools/validate_data.py",
    "range": {
      "start": {
        "line": 1,
        "character": 0
      },
      "end": {
        "line": 2,
        "character": 0
      }
    },
    "code": "import os\nimport pandas as pd",
    "narration": "The module brings in the os library to handle filesystem-level work and pandas (aliased as pd) to do tabular data handling. In the context of validate_data, os is used to locate and introspect the cached price and fundamental files the validator will open — composing paths, checking for existence, listing or walking directories and generally mapping from the project’s storage layout to the validator’s inputs (remember how SpyTickers and other downloader routines persist files to known local paths). Pandas is used to read those files into DataFrame objects and drive the actual validation logic: verifying required columns, detecting missing or duplicate rows, checking date continuity and dtypes, and aggregating errors that get reported through the module’s print_err mechanism. This pairing mirrors other utilities in the codebase that also combine filesystem access with DataFrame operations; the nearby variants you’ve seen add pickle, numpy, or the info_tool import when they additionally need serialization, low-level numeric array operations, or project metadata, but validate_data only requires os and pandas because its responsibility is focused on file discovery and tabular validation rather than serialization or heavy numeric calculation."
  },
  {
    "open-file": "tools/validate_data.py",
    "range": {
      "start": {
        "line": 5,
        "character": 0
      },
      "end": {
        "line": 9,
        "character": 29
      }
    },
    "code": "def print_err(message, note=None):\n    if note:\n        print('!!!', message, note)\n    else:\n        print('!!!', message)",
    "narration": "print_err is the tiny, focused reporting helper the validator uses to surface dataset problems to the console before a backtest runs. It accepts a primary message and an optional note; when called from validate the primary message is typically a ticker symbol and the optional note describes the problem (for example an empty file or a bad dtype). If a note is supplied it emits both the message and the note together with a prominent attention marker so the operator can spot the issue; if no note is supplied it emits just the message with the same attention marker. It has no return value and exists solely for side-effectful console output, centralizing how data-validation warnings and errors are presented across the validate checks. Compared with Strategy.log, which conditionally prints verbose, timestamped runtime messages, print_err always prints a concise, high-visibility error line; compared with the last helper that reads and prints the final log line, print_err is simpler and purpose-built for validation-time error reporting."
  },
  {
    "open-file": "tools/validate_data.py",
    "range": {
      "start": {
        "line": 12,
        "character": 0
      },
      "end": {
        "line": 41,
        "character": 18
      }
    },
    "code": "def validate(tickers):\n    for ticker in tickers:\n        try:\n            d = pd.read_csv(DATA_PATH + ticker + '.csv', index_col=0, parse_dates=True)\n            write_data = False\n            if len(d) < 2:\n                print_err(ticker, '(empty)')\n\n            if (d.dtypes == object).any():\n                print_err(ticker, '(bad dtype)')\n\n            if (abs(d['Adj Close']) <= 1E-8).any():\n                print_err(ticker, '(0 Adj Close)')\n                zero_values = abs(d['Adj Close']) <= 1E-8\n                print(d.loc[zero_values])\n                d.loc[zero_values, 'Adj Close'] = d.loc[zero_values, 'Close']\n                write_data = True\n\n            if d.isnull().any(axis=1).any():\n                print_err(ticker, '(null)')\n                d = d.interpolate(method='time')\n                write_data = True\n\n            if write_data:\n                print_err('writing...')\n                d.to_csv(DATA_PATH + ticker + '.csv')\n\n        except Exception as e:\n            print_err(e)\n    print('done.')",
    "narration": "validate is the entry-point utility that walks the local cached CSVs produced by the data layer and sanity-checks each ticker before a backtest. For every ticker it attempts to load the CSV into a pandas DataFrame with the first column as the datetime index, then runs a sequence of validations and lightweight repairs: it flags very small or empty files by reporting an empty dataset, checks for any columns with object dtype and reports them as bad dtypes, looks for adjusted-close values that are effectively zero (using a 1e-8 tolerance) and reports those while printing the offending rows and replacing zero adjusted-close with the raw close price, and detects any rows with nulls and reports them before filling gaps by interpolating along the time index. A write_data boolean tracks whether any in-place fixes were made; if so the function reports that it is writing and persists the corrected DataFrame back to the same CSV so the downstream engine sees repaired input. All of this is wrapped in a try/except so any read or processing exception is funneled through the centralized print_err reporting, and when the loop finishes it prints a done message. validate therefore focuses on per-file data integrity and minimal automatic recovery (zero adj-close replacement and time interpolation), which complements clean_tickers that instead filters tickers by date coverage and load_data that expects clean Adj Close series to build paired datasets."
  },
  {
    "open-file": "tools/validate_data.py",
    "range": {
      "start": {
        "line": 44,
        "character": 0
      },
      "end": {
        "line": 48,
        "character": 87
      }
    },
    "code": "if __name__ == '__main__':\n    print('loading files...')\n    DATA_PATH = os.path.join(os.path.dirname(__file__), '../data/price/')\n    FILE_LIST = os.listdir(DATA_PATH)\n    TICKERS = [f[:-4] for f in FILE_LIST if os.path.isfile(os.path.join(DATA_PATH, f))]\n",
    "narration": "The main-module guard makes this file runnable as a standalone sanity-check tool; when executed it first prints a small status message to indicate it is scanning the cached price data. It constructs DATA_PATH by anchoring a relative ../data/price location to the module file's directory so the lookup is independent of the current working directory, then enumerates the directory entries into FILE_LIST. TICKERS is produced by filtering FILE_LIST down to actual files under DATA_PATH and converting each filename to the expected ticker identifier by removing the final four characters (i.e., dropping the file extension such as .csv). The result is a ready-to-use list of ticker names and a data-directory constant that the rest of this utility (for example the validate routine) and other helpers like load_data expect to consume when iterating through and opening the cached CSVs."
  },
  {
    "open-file": "tools/validate_data.py",
    "range": {
      "start": {
        "line": 50,
        "character": 0
      },
      "end": {
        "line": 52,
        "character": 21
      }
    },
    "code": "    print('loaded.')\n    print('validating data...')\n    validate(TICKERS)\n",
    "narration": "The two status prints report that the local ticker list has been loaded and that the script is about to run its integrity checks, and then the code hands the TICKERS list to validate so the per-symbol sanity routine runs. validate iterates each ticker, reads the corresponding CSV from the project's price data store, and performs a sequence of checks that match the file's role in the data pipeline: it flags trivially empty files, detects wrong dtypes, locates near-zero adjusted close values and replaces them with the close price, and finds null rows which it attempts to repair via time-based interpolation. When a repair is made the routine rewrites the CSV back to the DATA_PATH so the local cache is corrected for subsequent backtests; any exceptions encountered for a ticker are reported through print_err and the loop continues. In the broader architecture this call is the gating step that ensures the data layer hands the backtester clean, continuous price series; it complements routines like clean_tickers, which filter universes by date coverage, by focusing on file-level data quality and automated, centralized error reporting."
  }
]